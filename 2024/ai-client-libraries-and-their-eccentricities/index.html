<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>AI Client libraries and their eccentricities &#183; Marc Dougherty</title>
<meta name=title content="AI Client libraries and their eccentricities &#183; Marc Dougherty"><link rel=canonical href=https://www.marcdougherty.com/2024/ai-client-libraries-and-their-eccentricities/><link type=text/css rel=stylesheet href=/css/main.bundle.min.1d8b820a57a192ce1bb9342d9dd1d66b0ffb9953b2faa5f7d0af952364b54857f002bd95c41f483b7beda5debdf1572876ee24b166e43a54d970d2079df6c8bb.css integrity="sha512-HYuCClehks4buTQtndHWaw/7mVOy+qX30K+VI2S1SFfwAr2VxB9IO3vtpd698Vcodu4ksWbkOlTZcNIHnfbIuw=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.d6c35110b5d7df8014403fbb81cd21a9d89daf9eb586c936e585e9d04adc204dce554aa737f81e8c902b9dea9e67bd63a977b77dedefb8a39430581ebc519207.js integrity="sha512-1sNRELXX34AUQD+7gc0hqdidr561hsk25YXp0ErcIE3OVUqnN/gejJArneqeZ71jqXe3fe3vuKOUMFgevFGSBw==" data-copy data-copied></script><script src=/js/zoom.min.js></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://www.marcdougherty.com/2024/ai-client-libraries-and-their-eccentricities/"><meta property="og:site_name" content="Marc Dougherty"><meta property="og:title" content="AI Client libraries and their eccentricities"><meta property="og:description" content="As discussed earlier in this series, there are several factors that affect how client libraries communicate with your AI Model service.
In this article, I’ll be demonstrating using some of the popular client libraries to connect to models served in 3 different ways, to demonstrate the usability and flexibility of these client libraries. For this comparison, I’ll be using python since it is the most common choice in LLM user communities."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-15T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-15T12:02:20-07:00"><meta property="og:see_also" content="https://www.marcdougherty.com/2024/ai-model-apis/"><meta property="og:see_also" content="https://www.marcdougherty.com/2024/ai-model-serving-layers/"><meta name=twitter:card content="summary"><meta name=twitter:title content="AI Client libraries and their eccentricities"><meta name=twitter:description content="As discussed earlier in this series, there are several factors that affect how client libraries communicate with your AI Model service.
In this article, I’ll be demonstrating using some of the popular client libraries to connect to models served in 3 different ways, to demonstrate the usability and flexibility of these client libraries. For this comparison, I’ll be using python since it is the most common choice in LLM user communities."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"AI Client libraries and their eccentricities","headline":"AI Client libraries and their eccentricities","abstract":"As discussed earlier in this series, there are several factors that affect how client libraries communicate with your AI Model service.\nIn this article, I\u0026rsquo;ll be demonstrating using some of the popular client libraries to connect to models served in 3 different ways, to demonstrate the usability and flexibility of these client libraries. For this comparison, I\u0026rsquo;ll be using python since it is the most common choice in LLM user communities.","inLanguage":"en","url":"https:\/\/www.marcdougherty.com\/2024\/ai-client-libraries-and-their-eccentricities\/","author":{"@type":"Person","name":"Marc Dougherty"},"copyrightYear":"2024","dateCreated":"2024-08-15T00:00:00\u002b00:00","datePublished":"2024-08-15T00:00:00\u002b00:00","dateModified":"2024-08-15T12:02:20-07:00","mainEntityOfPage":"true","wordCount":"1872"}]</script><meta name=author content="Marc Dougherty"><link href=https://github.com/muncus rel=me><link href=https://twitter.com/muncus rel=me><link href=https://dev.to/muncus rel=me><link href=https://medium.com/@muncus rel=me><script src=/lib/jquery/jquery.slim.min.js integrity></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-31G86DYMCL"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-31G86DYMCL")</script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Marc Dougherty</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Blog</p></a><a href=/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=/archive/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button for=menu-controller class=block><input type=checkbox id=menu-controller class=hidden><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Blog</p></a></li><li class=mt-1><a href=/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=/archive/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li></ul></div></label></div></div><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">AI Client libraries and their eccentricities</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2024-08-15 00:00:00 +0000 UTC">2024-08-15</time><span class="px-2 text-primary-500">&#183;</span><span>1872 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">9 mins</span></div><div class="flex flex-row flex-wrap items-center"></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10"><details open class="toc-right mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#criteria>Criteria</a></li><li><a href=#model-services>Model services</a></li><li><a href=#libraries>Libraries</a><ul><li><a href=#openai-client>OpenAI Client</a></li><li><a href=#vertex-ai>Vertex AI</a></li><li><a href=#langchain>Langchain</a></li></ul></li><li><a href=#conclusions>Conclusions</a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#criteria>Criteria</a></li><li><a href=#model-services>Model services</a></li><li><a href=#libraries>Libraries</a><ul><li><a href=#openai-client>OpenAI Client</a></li><li><a href=#vertex-ai>Vertex AI</a></li><li><a href=#langchain>Langchain</a></li></ul></li><li><a href=#conclusions>Conclusions</a></li></ul></nav></div></details></div></div><div class="min-w-0 min-h-0 max-w-fit"><details style=margin-left:0 class="mt-2 mb-5 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5"><summary class="py-1 text-lg font-semibold cursor-pointer bg-primary-200 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-primary-800 dark:text-neutral-100">AI productionization - This article is part of a series.</summary><div class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">Part : This Article</div><div class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><a href=https://www.marcdougherty.com/2024/ai-model-apis/>Part : AI Model APIs</a></div><div class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><a href=https://www.marcdougherty.com/2024/ai-model-serving-layers/>Part : AI model serving layers</a></div></details><div class="article-content max-w-prose mb-20"><p>As discussed earlier in this series, there are several factors that affect how
client libraries communicate with your AI Model service.</p><p>In this article, I&rsquo;ll be demonstrating using some of the popular client
libraries to connect to models served in 3 different ways, to demonstrate the
usability and flexibility of these client libraries. For this comparison, I&rsquo;ll
be using python since it is the most common choice in LLM user communities.</p><p>This is by no means a complete accounting of client libraries, but a quick
overview to get you started on the right path.</p><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M234.7 42.7 197 56.8c-3 1.1-5 4-5 7.2s2 6.1 5 7.2l37.7 14.1L248.8 123c1.1 3 4 5 7.2 5s6.1-2 7.2-5l14.1-37.7L315 71.2c3-1.1 5-4 5-7.2s-2-6.1-5-7.2L277.3 42.7 263.2 5c-1.1-3-4-5-7.2-5s-6.1 2-7.2 5L234.7 42.7zM46.1 395.4c-18.7 18.7-18.7 49.1.0 67.9l34.6 34.6c18.7 18.7 49.1 18.7 67.9.0L529.9 116.5c18.7-18.7 18.7-49.1.0-67.9L495.3 14.1c-18.7-18.7-49.1-18.7-67.9.0L46.1 395.4zM484.6 82.6l-105 105-23.3-23.3 105-105 23.3 23.3zM7.5 117.2C3 118.9.0 123.2.0 128s3 9.1 7.5 10.8L64 160l21.2 56.5c1.7 4.5 6 7.5 10.8 7.5s9.1-3 10.8-7.5L128 160l56.5-21.2c4.5-1.7 7.5-6 7.5-10.8s-3-9.1-7.5-10.8L128 96 106.8 39.5C105.1 35 100.8 32 96 32s-9.1 3-10.8 7.5L64 96 7.5 117.2zm352 256c-4.5 1.7-7.5 6-7.5 10.8s3 9.1 7.5 10.8L416 416l21.2 56.5c1.7 4.5 6 7.5 10.8 7.5s9.1-3 10.8-7.5L480 416l56.5-21.2c4.5-1.7 7.5-6 7.5-10.8s-3-9.1-7.5-10.8L480 352l-21.2-56.5c-1.7-4.5-6-7.5-10.8-7.5s-9.1 3-10.8 7.5L416 352l-56.5 21.2z"/></svg>
</span></span><span class=dark:text-neutral-300>TL;DR: I suggest Langchain for maximum flexibility. OpenAI client is a great
choice (though less great if you&rsquo;re hosting the model on Vertex AI as
configuration and debugging are difficult.)</span></div><h2 class="relative group">Criteria<div id=criteria class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#criteria aria-label=Anchor>#</a></span></h2><p>The library evaluation will focus on the following criteria.</p><dl><dt>API usability</dt><dd>a subjective measure of the libraries usablility. Does it provide a good API</dd><dd>surface? Does it produce useful errors when things go wrong? etc.</dd><dt>Model Flexibility</dt><dd>Can the library be used to access Models using both the Chat and Completions</dd><dd>API? Does it work for Instruction-tuned models? How much change is needed</dd><dd>to use a different model?</dd></dl><h2 class="relative group">Model services<div id=model-services class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#model-services aria-label=Anchor>#</a></span></h2><p>To evaluate flexibilty, we&rsquo;ll be using 3 different model services:</p><dl><dt>Gemini 1.5 Flash</dt><dd>Google&rsquo;s hosted offering.</dd><dt>Gemma 2 on Vertex AI</dt><dd>Google&rsquo;s Gemma is an Open Model, which can be deployed to their Vertex AI</dd><dd>platform through the AI Model Garden. This service is deployed on GPUs, and</dd><dd>served with <a href=https://huggingface.co/docs/text-generation-inference/en/index target=_blank>Huggingface&rsquo;s TGI</a>.</dd><dt>Gemma 2 on GKE</dt><dd>Gemma models can also be hosted on Kubernetes. This service is also deployed</dd><dd>on GPUs, with TGI.</dd></dl><h2 class="relative group">Libraries<div id=libraries class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#libraries aria-label=Anchor>#</a></span></h2><p>Each of the code snippets below has been validated to work at the time of this
writing - but AI is a rapidly evolving space, so some changes may be necessary.</p><h3 class="relative group">OpenAI Client<div id=openai-client class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#openai-client aria-label=Anchor>#</a></span></h3><p><a href=https://github.com/openai/openai-python target=_blank>OpenAI&rsquo;s Client libraries</a> are
probably the first thought of anyone who&rsquo;s already working with AI client
libraries. They are something of the industry default, since OpenAI&rsquo;s APIs are
widely copied in other AI model serving tools like vLLM and TGI.</p><p>For long-running processes, it is necessary to refresh the google credentials
that the OpenAI client uses - it is not done automatically. Use this <a href=https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-vertex-using-openai-library#refresh_your_credentials target=_blank>sample on
credential
refreshing</a> to implement this ability.</p><h4 class="relative group">OpenAI and Gemini<div id=openai-and-gemini class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#openai-and-gemini aria-label=Anchor>#</a></span></h4><pre tabindex=0><code>import google.auth
import google.auth.transport
import google.auth.transport.requests
from openai import OpenAI
import vertexai

creds, project = google.auth.default(scopes=[&#34;https://www.googleapis.com/auth/cloud-platform&#34;])
auth_req = google.auth.transport.requests.Request()
creds.refresh(auth_req)

LOCATION=&#34;us-central1&#34;
PROJECT_ID=project
vertexai.init(project=project, location=LOCATION)
client = OpenAI(
    base_url = f&#39;https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/openapi&#39;,
    api_key = creds.token)

r = client.chat.completions.create(
    model=&#34;google/gemini-1.5-flash&#34;,
    messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;tell me a frog fact&#34;}],
)
print(r.choices[0].message.content)
</code></pre><h4 class="relative group">OpenAI and Vertex AI<div id=openai-and-vertex-ai class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#openai-and-vertex-ai aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M159.3 5.4c7.8-7.3 19.9-7.2 27.7.1 27.6 25.9 53.5 53.8 77.7 84 11-14.4 23.5-30.1 37-42.9 7.9-7.4 20.1-7.4 28 .1 34.6 33 63.9 76.6 84.5 118 20.3 40.8 33.8 82.5 33.8 111.9C448 404.2 348.2 512 224 512 98.4 512 0 404.1.0 276.5c0-38.4 17.8-85.3 45.4-131.7C73.3 97.7 112.7 48.6 159.3 5.4zM225.7 416c25.3.0 47.7-7 68.8-21 42.1-29.4 53.4-88.2 28.1-134.4-2.8-5.6-5.6-11.2-9.8-16.8l-50.6 58.8S180.8 199 175.1 192c-42 51.8-63.1 81.2-63.1 114.8C112 375.4 162.6 416 225.7 416z"/></svg>
</span></span><span class=dark:text-neutral-300><p>I was not able to get this working with either OpenAI or cURL with <a href=https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-vertex-using-openai-library#call_a_self-deployed_model_with_the_chat_completions_api target=_blank>Google&rsquo;s
OpenAI Client documentation</a></p><p>I consistenly received 400 errors, with either &lsquo;PRECONDITION_FAILED&rsquo; or
&lsquo;INVALID_REQUEST&rsquo;. no other information was available.</p></span></div><pre tabindex=0><code>    import google.auth
    import google.auth.transport
    import google.auth.transport.requests
    from openai import OpenAI
    import vertexai
    from openai.types.chat import ChatCompletion

    creds, project = google.auth.default()
    auth_req = google.auth.transport.requests.Request()
    creds.refresh(auth_req)

    LOCATION=&#34;us-central1&#34;
    PROJECT_ID=project
    ENDPOINT=&#34;my-endpoint-number&#34;
    vertexai.init(project=project, location=LOCATION)

    client = OpenAI(
        base_url = f&#39;https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT}/&#39;,
        api_key = creds.token)

    r = client.chat.completions.create(
        model=&#34;tgi&#34;,
        messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;tell me a frog fact&#34;}],
    )
    print(r)
</code></pre><h4 class="relative group">OpenAI and GKE<div id=openai-and-gke class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#openai-and-gke aria-label=Anchor>#</a></span></h4><p>This looks a lot like the Gemini sample, but notably simpler because we are not
doing any authentication and have a simpler base URL.</p><pre tabindex=0><code>from openai import OpenAI

client = OpenAI(
    base_url = &#39;http://localhost:8080/v1&#39;,
    api_key = &#34;unused&#34;)

r = client.chat.completions.create(
    model=&#34;unused&#34;,
    messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;tell me a frog fact&#34;}],
)
print(r.choices[0].message.content)
</code></pre><h4 class="relative group">Results<div id=results class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#results aria-label=Anchor>#</a></span></h4><ul><li><p>Usability: &#x1f7e0;</p><p>The OpenAI client is flexible, and can be used to talk to any
OpenAI-compatible model server (which is nearly all of them!).</p><p>However, using this library with Google&rsquo;s offerings does not seem like a top
priority for either party - there are clearly some sharp edges.</p></li><li><p>Flexibility: &#x1f534;</p><p>This is another case where I want to grade the library and the service
separately. The client is basically just a well-wrapped HTTP client, and is
adequately flexible. (especially when enabling debug logs, <code>httpx</code> provides
solid debugging info.)</p><p>Google&rsquo;s Vertex AI service provides terse, generic errors with insufficient
information to understand what the problem is. I found that there were often
no server-side log messages to aid my debugging either. &#x1f622;</p></li><li><p>Overall: &#x1f534;</p><p>I was hoping for better compatability in Google&rsquo;s services, given the
popularity of the OpenAI APIs in all major model serving tools. The layer of
Vertex AI appears to be creating more problems than it is solving here.</p></li></ul><h3 class="relative group">Vertex AI<div id=vertex-ai class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#vertex-ai aria-label=Anchor>#</a></span></h3><p>The Vertex AI client library is the Google-published SDK for communicating with
Google&rsquo;s hosted Gemini models, and user-deployed models that are hosted on the
Vertex AI platform.</p><p>As discussed in this <a href=https://medium.com/@muncus/which-gemini-ai-is-right-for-you-b03e625eff0b target=_blank>prior article about
Gemini</a>,
Vertex AI client libraries actually have 2 different pieces - I&rsquo;ll be referring
to them this way:</p><ul><li><code>aiplatform</code>: the <code>google.cloud.aiplatform</code> python package. This
auto-generated library uses a resource-based approach to call the underlying
API.</li><li><code>vertexai</code>: the <code>vertexai</code> python package, which is a handwritten SDK built on
top of the <code>aiplatform</code> package, providing an improved developer experience
but lacking some features.</li></ul><h4 class="relative group">Vertex and Gemini<div id=vertex-and-gemini class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#vertex-and-gemini aria-label=Anchor>#</a></span></h4><p>This is the flagship case for this library, and the one that <code>vertexai</code>
was created for. The code is quite straightforward and requires minimal
configuration.</p><pre tabindex=0><code>from vertexai.generative_models import GenerativeModel
llm = GenerativeModel(&#34;gemini-1.5-flash&#34;)
r = llm.generate_content(&#34;tell me a fact about frogs&#34;)
print(r.text)
</code></pre><h4 class="relative group">Vertex and Vertex (Vertex Squared)<div id=vertex-and-vertex-vertex-squared class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#vertex-and-vertex-vertex-squared aria-label=Anchor>#</a></span></h4><p>Here&rsquo;s where things get awkward - the <code>GenerativeModel</code> classes that work with
Gemini do not work for user-deployed models in Vertex AI. For these, we&rsquo;ll need
to use the <code>aiplatform</code> library.</p><pre tabindex=0><code>project=&#34;MY_PROJECT&#34;
location=&#34;us-central1&#34;
# use `gcloud ai endpoints list --region us-central1` to see endpoint ids
endpoint_id=&#34;NNNNNNNN&#34;

import google.cloud.aiplatform_v1beta1 as aipb
client = aipb.PredictionServiceClient(client_options={
      &#39;api_endpoint&#39;: location + &#34;-aiplatform.googlapis.com&#34; })

endpoint = str.format(&#34;projects/{project}/locations/{location}/endpoints/{endpoint}&#34;,
        project=project,
        location=location,
        endpoint=endpoint_id)

r = client.predict(
      endpoint=endpoint,
      instances=[{&#39;inputs&#39;: prompt}])

print(r.predictions)
</code></pre><p>Certainly not as tidy as the Gemini version, but not too bad once you understand
the use of Endpoint resources, and the PredictionClient.</p><p>The <code>instances</code> parameter is a bit tricky here, and varies based on how your
model was deployed. The instances key (<code>'inputs'</code> above) must be set differently
for models served by Huggingface&rsquo;s TGI vs OpenAI-compatible model serving like
vLLM. TGI uses &lsquo;inputs&rsquo;, but OpenAI-compatible servers will use &lsquo;prompt&rsquo;.</p><p>This choice is poorly exposed in the Model Garden, so you&rsquo;ll likely need to
inspect the Model (<strong>NOT</strong> the Endpoint, but the model behind it) using <code>gcloud ai models describe $DEPLOYED_MODEL_NAME --region $REGION</code>. The <code>imageUri</code> field
in the output is the serving container your model is using.</p><h4 class="relative group">Vertex and GKE<div id=vertex-and-gke class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#vertex-and-gke aria-label=Anchor>#</a></span></h4><p>The Vertex AI client libraries are only useful for talking to Models hosted by
Google&rsquo;s Vertex AI service, so they&rsquo;re not usable for models hosted in your own
Kubernetes cluster. :sad:</p><h4 class="relative group">Results<div id=results-1 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#results-1 aria-label=Anchor>#</a></span></h4><ul><li><p>Usability: &#x1f7e0;</p><p><code>vertexai</code> deserves a &#x1f7e2;, but the need to <em>also</em> understand
<code>aiplatform</code> API surface downgrades this to orange.</p></li><li><p>Flexibility: &#x1f534;</p><p>Two different APIs for talking to google-hosted vs user-hosted models is
awkward and would require a full rewrite to switch, or building one&rsquo;s own
abstraction layer.</p></li><li><p>Overall: &#x1f534;</p><p>These APIs appear to prioritize google&rsquo;s own models, with little
consideration for user-hosted use cases. If the <code>vertexai</code> experience worked
for user-hosted models, this would be <strong>much</strong> better.</p></li></ul><h3 class="relative group">Langchain<div id=langchain class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#langchain aria-label=Anchor>#</a></span></h3><p><a href=http://langchain.com target=_blank>Langchain</a> is a framework that lets the same client API
be used to talk to multiple AI models on a collection of platforms. The goal of
langchain is to allow for a similar developer experience across all leading AI
models and platforms.</p><p>Finding the correct model class object can be a bit tricky, but once you have
that, the usage of those objects is consistent across the framework (as you&rsquo;ll
see below).</p><h4 class="relative group">Langchain and Gemini<div id=langchain-and-gemini class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#langchain-and-gemini aria-label=Anchor>#</a></span></h4><pre tabindex=0><code>from langchain_google_vertexai import ChatVertexAI
llm = ChatVertexAI(model_name=&#34;gemini-1.5-flash&#34;)
r = llm.invoke(&#34;tell me a frog fact&#34;)
print(r.content)
</code></pre><p>The above is an example of basic Gemini usage, and is a good place to start.
More flexiblility can be achieved using Prompt templates and chains:</p><pre tabindex=0><code>from langchain_google_vertexai import ChatVertexAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
llm = ChatVertexAI(model_name=&#34;gemini-1.5-flash&#34;)
pt = ChatPromptTemplate.from_messages([
    (&#34;system&#34;, &#34;you are a helpful assistant who likes amphibians&#34;),
    (&#34;human&#34;, &#34;{input}&#34;)
])
chain = pt | llm
r = chain.invoke(&#34;tell me a frog fact&#34;)
print(r.content)
</code></pre><p>This example shows the use of Chat Prompt Templates to add some system-level
instruction for how the model should behave. The argument passed to <code>invoke()</code>
is inserted in the Prompt Template placeholder <code>{input}</code> since there is only
one. When using multiple placeholders, <code>invoke()</code> requires a mapping.</p><p>Prompt templates can also be used to reformat input for Instruction-tuned models
like Gemma.</p><p>The use of chains and prompt templates applies to all langchain examples, though
for brevity I will only demonstrate it here.</p><h4 class="relative group">Langchain and Vertex AI<div id=langchain-and-vertex-ai class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#langchain-and-vertex-ai aria-label=Anchor>#</a></span></h4><pre tabindex=0><code>llm = VertexAIModelGarden(project=projectid,
                          location = location,
                          endpoint_id=endpointid,
                          prompt_arg=&#34;inputs&#34;)
r = llm.invoke(prompt)
</code></pre><p>As with other Endpoint usage, the <code>endpointid</code> above is the integer identifier
of the endpoint (not the name). IDs can be seen with the <code>gcloud ai endpoints list</code> command.</p><p>Chat Prompt template usage is indentical to the previous example.</p><h4 class="relative group">Langchain and GKE<div id=langchain-and-gke class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#langchain-and-gke aria-label=Anchor>#</a></span></h4><p>The most challenging part of this was finding the proper LLM class to use for a
&ldquo;generic&rdquo; LLM endpoint. Since I&rsquo;m using TGI to serve my model, and I know that
TGI is OpenAI-compatible, I used the OpenAI module with a custom base URL.</p><p>This example uses a localhost URL because I was using kubernetes port forwarding
to access the service. Production use cases should use a different approach
(like cluster-level DNS).</p><pre tabindex=0><code>from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage

llm = ChatOpenAI(
    openai_api_key=&#34;unused&#34;, # this key is required.
    base_url=&#34;http://localhost:8080/v1/&#34;
)
r = llm.invoke(&#34;tell me a frog fact&#34;)
print(r.content)
</code></pre><h4 class="relative group">Results<div id=results-2 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#results-2 aria-label=Anchor>#</a></span></h4><ul><li><p>Usability: &#x1f7e0;</p><p>The advanced langchain concepts (e.g. Prompt Templates, chains) take some
significant learning to understand. The errors from prompts and chains can
be difficult to debug - things like &ldquo;expected str&rdquo;, but the stacktrace is
deep in the langchain code, and its not clear how the user would fix it.</p></li><li><p>Flexibility: &#x1f7e2;</p><p>Langchain delivers on the goal to keep the query experience pretty uniform
across models and providers. There are still some bumps in the road around
the exact shape of arguments to <code>invoke()</code>, especially with
instruction-tuned models, but those are industry-wide issues,
not specific to langchain.</p></li><li><p>Overall: &#x1f7e0;</p><p>The cryptic error stacktraces are the biggest contributor to an orange
rating here. The cognitive load of learning about prompt templates and
output chains are also a factor, though relatively minor.</p></li></ul><h2 class="relative group">Conclusions<div id=conclusions class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#conclusions aria-label=Anchor>#</a></span></h2><p>As I went through these evaluations, I tried to separate commentary on the model
hosting platform from the client library. Both aspects have an effect on the
developer experience, and evaluating them separately was not always possible.</p><p>At this point, I would choose Langchain over other client libraries, as it
provides the most insulation from the rapid change in the underlying
technologies. The next time there is a major shift in AI technology, I would
expect a fairly simple transition as a langchain user - other client
libraries will likely have more work to do.</p><p>If I was planning to use providers that were comitted to OpenAI-compatability,
the OpenAI client library would be a solid choice. Google&rsquo;s compatability here
is OK, but the debugging experience is pretty opaque.</p><p>I hope this helps you pick the right client library for your AI-calling needs!</p></div><details style=margin-left:0 class="mt-2 mb-5 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5"><summary class="py-1 text-lg font-semibold cursor-pointer bg-primary-200 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-primary-800 dark:text-neutral-100">AI productionization - This article is part of a series.</summary><div class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">Part : This Article</div><div class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><a href=https://www.marcdougherty.com/2024/ai-model-apis/>Part : AI Model APIs</a></div><div class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><a href=https://www.marcdougherty.com/2024/ai-model-serving-layers/>Part : AI model serving layers</a></div></details></div><script>var oid="views_posts/ai-clients.md",oid_likes="likes_posts/ai-clients.md"</script><script type=text/javascript src=/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
Marc Dougherty</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://www.marcdougherty.com/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>