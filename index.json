[{"content":"","date":"2024-04-09","externalUrl":null,"permalink":"/","section":"Marc Dougherty","summary":"","title":"Marc Dougherty","type":"page"},{"content":"","date":"2024-04-09","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"Terraform and Kubernetes are both declarative systems, but there can be some rough edges when these two systems interact. Kubernetes - specifically Google Kubernetes Engine (GKE) makes extensive use of annotations to store additional information about GKE resources.\nI encountered this when working on an article about traffic drains, and you can see it for yourself on Service objects. By default, GKE clusters contain several services that show this - for example, you can inspect the annotations on the built-in default-http-backend service:\nkubectl get service -n kube-system default-http-backend -o yaml\nUnder metadata you\u0026rsquo;ll see a block like this:\nannotations: cloud.google.com/neg: \u0026#39;{\u0026#34;ingress\u0026#34;:true}\u0026#39; components.gke.io/component-name: l7-lb-controller-combined components.gke.io/component-version: 1.23.5-gke.0 components.gke.io/layer: addon While not usually an issue for built-in resources, annotations for Terraform-managed resources can create a couple different types of issue. Continuous Delivery systems may delete the unexpected annotations, causing the GKE infrastructure to re-create them, resulting in a lot of busywork for the automation \u0026#x1f916;.\nAnother type of annotation-related problem occurs with GKE Autopilot, which uses annotations to store scaling and load information. Constantly deleting this information reduces Autopilot\u0026rsquo;s ability to assess your long-term needs, and may result in suboptimal scaling behavior.\nPart 1: Ignoring annotations / labels # Terraform does have a couple of ways to ignore these troublesome annotations: one that works per-object and uses the lifecycle stanza, and the second works for all objects managed by the Kubernetes provider. The Kubernetes provider docs cover both solutions, but this article only covers the provider-level approach.\nThe ignore_annotations and ignore_labels options allow for the global ignoring of any annotation or label that matches one of the regular expressions provided. For example, to ignore all of the GKE Autopilot annotations, you could declare your provider like this:\nprovider \u0026#34;kubernetes\u0026#34; { ignore_annotations = [ \u0026#34;^autopilot\\\\.gke\\\\.io\\\\/.*\u0026#34;, ] } This works great when writing new terraform, and don\u0026rsquo;t have an existing terraform state file. If you add this to existing Terraform, you\u0026rsquo;ll see that it still reports diffs \u0026#x1f926;. This configuration prevents these labels from being part of the stored terraform state - if you already have offending annotations in your terraform state, you\u0026rsquo;ll need to continue to part 2.\nPart 2: Selective editing of TF state # If you\u0026rsquo;ve already got annotations in your terraform state, you\u0026rsquo;ll need to \u0026ldquo;forget\u0026rdquo; those resources, and re-import them with the ignores in place. This solution was initially shared on a TF issue about these ignore options.\nBefore getting started, make sure you have a backup of your current TF state file. If anything goes wrong, you can use this backup to recover.\nEnsure there are no active diffs besides the annotations, with terraform plan. List known objects from the terraform state: terraform state list to find the relevant resource names for deletion. Delete each resource from the state file with terraform state rm ${TF_RESOURCE} Re-import the resource with terraform import ${TF_RESOURCE} ${K8S_OBJECT} Finally, re-run terraform plan to verify that the resource imported correctly and does not show diffs for the ignored annotations. That\u0026rsquo;s it! You\u0026rsquo;ve successfully removed those pesky annotations from your Terraform state file! \u0026#x1f389;\nRecommended ignores # In addition to the autopilot example above, I\u0026rsquo;ve also run into problems with a few other Google-specific annotations. I\u0026rsquo;ll be configuring my GKE Kubernetes providers with the following set of ignores from now on:\nignore_annotations = [ \u0026#34;cloud\\\\.google\\\\.com\\\\/neg\u0026#34;, \u0026#34;cloud\\\\.google\\\\.com\\\\/neg-status\u0026#34;, \u0026#34;^autopilot\\\\.gke\\\\.io\\\\/.*\u0026#34;, ] Hopefully this will prevent any poor interactions between GKE and Terraform. \u0026#x1f91e;\n","date":"2024-04-09","externalUrl":null,"permalink":"/2024/terraform-and-gke-annotations/","section":"Posts","summary":"Terraform and Kubernetes are both declarative systems, but there can be some rough edges when these two systems interact. Kubernetes - specifically Google Kubernetes Engine (GKE) makes extensive use of annotations to store additional information about GKE resources.\nI encountered this when working on an article about traffic drains, and you can see it for yourself on Service objects. By default, GKE clusters contain several services that show this - for example, you can inspect the annotations on the built-in default-http-backend service:","title":"Terraform and GKE Annotations","type":"posts"},{"content":"Distributed systems are capable of fast change and adaptation, and highly tolerant of constrained failures. This is often achieved by building systems that can exclude failing components from the larger system, but this capability is not automatic. Many large systems use load balancers to \u0026ldquo;route around a problem\u0026rdquo; by removing failed components. This process is often called \u0026ldquo;draining\u0026rdquo;.\nDrains are a generic mitigation, which means you can use them even if you don\u0026rsquo;t understand the cause of the problem (yet)!\nBut to take advantage of drains, your services must be architected to support them. The details will vary depending on the service, but common requirements include:\nServing locations in separate failure domains Often achieved by using multiple zones/regions from your cloud provider, this ensures outages in one location do not affect others. Requests may be served from any available location If a whole region is unavailable, the requests may go to a neighboring region. Any data needed to serve the request should be present in multiple regions. A frontend load balancer with configurable backends To perform drains, we need to change the available backends in the load balancer. Most load balancers support this, but some managed load balancers may not allow you to customize the set of backends. Example Architecture # There are many ways to achieve a drainable service, and this article will use the following architecture.\nflowchart TB lb(Global Load Balancer) subgraph RegionX [\"Region X\"] subgraph Cl-A [\"GKE Cluster A\"] direction TB svcA(\"k8s Service\") --\u003e depA(\"k8s Deployment\") --\u003e podA(\"k8s Pod\") end end subgraph RegionY [\"Region Y\"] subgraph Cl-B [\"GKE Cluster B\"] direction TB svcB(\"k8s Service\") --\u003e depB(\"k8s Deployment\") --\u003e podB(\"k8s Pod\") end end lb --\u003e Cl-A lb --\u003e Cl-B Components:\nGlobal frontend load balancer 2 Regional GKE clusters The whereami example service from GoogleCloudPlatform/kubernetes-engine-samples (using google\u0026rsquo;s publicly available container) This example is modeled in Terraform in the drain-demo github repository in three steps. If you\u0026rsquo;re not yet familiar with Terraform, you can take a look at some of the gcp terraform tutorials.\nPrepare your clusters # The first step is to create the 2 regional GKE clusters to host our backend service. This is done as a separate step to prevent terraform errors when using the kubernetes terraform provider on non-existant clusters.\nTo create the clusters, run the following commands from the 00-setup-clusters directory:\nterraform init terraform apply --var project=${your_project_id} This will create 2 GKE clusters, called drain-demo-1-a and drain-demo-1-b. These names are important in subsequent steps.\nDeploy your workload # Next, we deploy our backend service to both clusters. We\u0026rsquo;re using terraform for this step as well, so these commands will look familiar. This time from the 01-deploy-workload directory:\nterraform init terraform apply --var project=${your_project_id} These steps create a kubernetes Deployment of our whereami service, as well as associated Service and Ingress objects.\nAt this point, we have two separate, independent deployments of our whereami service, one in each cluster.\nTODO: identify if we need the ingress objects.\nCreate your load balancer # The last piece of the puzzle is to create our load balancer. Load balancers in Google Cloud are made up of several related objects - the most interesting for our case is the Backend Service (terraform calls this a google_compute_backend_service).\nFor this example, we populate the Backend Service with the Network Endpoint Groups that are automatically created from the kubernetes Service objects. This information is available as annotations on the kubernetes objects. You can view these annotations directly with kubectl get service whereami - they are encoded as a json object under the key cloud.google.com/neg-status.\nThe provided terraform parses out the relevant information from the Service objects in both clusters, and populates a single Backend Service using both Network Endpoint Groups.\nTo deploy the load balancer, run the following commands from the 02-loadbalancer directory:\nterraform init terraform apply --var project=${your_project_id} We now have a global load balancer pointing to both of our independent deployments. The loadbalancer address can be found in the terraform output: terraform output loadbalancer_url.\nTo verify the expected behavior, you can use a web browser to view your loadbalancer. Repeated requests should show some results serviced from each of our GKE clusters.\nIf you have the curl and jq tools installed, you could also run a command like the following to show which cluster served each request:\nwhile true; do curl --silent ${loadbalancer_url} | jq .cluster_name ; sleep 0.2 ; done Performing a traffic drain # Our shiny new global load balancer is working great! Until late one night, when we get paged because the site is serving errors! \u0026#x26a0;\u0026#xfe0f; \u0026#x1f4df;\nA quick look at our monitoring dashboards show errors are only coming from Cluster A. We could spend our time investigating exactly what makes Cluster A different, but with complex distributed systems that can take a lot of investigation - meanwhile our users are getting errors. To restore service as quickly as possible, we can drain Cluster A, go back to sleep, and debug in the morning once we\u0026rsquo;ve had coffee \u0026#x2615;.\nTo perform a drain, we\u0026rsquo;ll need to edit the backend stanzas in our load balancer\u0026rsquo;s Backend Service object\n// NOTE: zero is not a valid max_rate. You must remove the block to drain. backend { group = data.google_compute_network_endpoint_group.neg-A.self_link balancing_mode = \u0026#34;RATE\u0026#34; max_rate_per_endpoint = 100 } backend { group = data.google_compute_network_endpoint_group.neg-B.self_link balancing_mode = \u0026#34;RATE\u0026#34; max_rate_per_endpoint = 100 } With these stanzas, we can control the balance of traffic between our clusters, and even drain all traffic by removing (or commenting out) the backend stanza for that cluster.\nTry commenting out the first block, and re-applying the loadbalancer terraform. Once the terraform apply has completed, you will see that cluster B is serving all incoming requests! \u0026#x1f389;\nConclusions # This example illustrated how traffic drains can be used to eliminate the user impact of a problem, without needing to solve the problem first.\nWhen used in a production incident, drains can quickly eliminate the user-facing impact of an incident, while preserving the misbehaving service for further investigation.\nThis example uses the Network Endpoint Groups (NEGs) that Google Cloud creates automatically for GKE Service objects, to route traffic to the correct kubernetes pods.\nHappy Draining!\n","date":"2024-04-05","externalUrl":null,"permalink":"/2024/architecting-for-traffic-drains/","section":"Posts","summary":"Distributed systems are capable of fast change and adaptation, and highly tolerant of constrained failures. This is often achieved by building systems that can exclude failing components from the larger system, but this capability is not automatic. Many large systems use load balancers to \u0026ldquo;route around a problem\u0026rdquo; by removing failed components. This process is often called \u0026ldquo;draining\u0026rdquo;.\nDrains are a generic mitigation, which means you can use them even if you don\u0026rsquo;t understand the cause of the problem (yet)!","title":"Architecting for Traffic Drains","type":"posts"},{"content":"I get a bit nervous whenever I touch renovate.json files, because I did not know how to test the effects of my changes. Well, this week I spent some quality time with Renovate and while the results are not perfect, they\u0026rsquo;re a lot better than what I had before!\nIf you\u0026rsquo;re not familiar with Renovate, it is a convenient way to keep your dependencies up to date. It understands dependencies in many language ecosystems, and supports a variety of ways to run and configure Renovate.\nTL;DR # Testing is still a bit tedious, because Renovate has a lot of functionality, and the output does not conveniently sum up what renovate would do. But the information is there, you just have to know where to look.\nFirst, the command I\u0026rsquo;m using:\nLOG_LEVEL=debug \\ RENOVATE_CONFIG_FILE=.github/renovate.json \\ npx renovate --platform=local \\ --require-config=ignored LOG_LEVEL=debug ensures we get plenty of output about decisions renovate is making. we need this flag to see what packages would be updated. RENOVATE_CONFIG_FILE=... - specifies a local path to a renovate configuration to use for this run. note: if you omit --require-config=ignored, this config will be merged with the one from this repository, which can cause unexpected results. npx renovate - uses npx to run renovate. will prompt you to install renovate if not already present. --platform=local - Renovate\u0026rsquo;s local platform works on the current directory only, and does not require a remote git repo --require-config=ignored - tells renovate to ignore any configuration found in the repository. How I test # In a git checkout of a repository, check out a point in time where I know I expect some renovate changes - for example, the commit just before your latest renovate PR was merged.\nFrom here, I modify the renovate.json file to reflect my changes.\nTo ensure the config is well-formed, you can run npx -p renovate renovate-config-validator, which does some syntactic validation.\nThen I run renovate, and interpret the results\nLOG_LEVEL=debug RENOVATE_CONFIG_FILE=.github/renovate.json npx renovate --platform=local --require-config=ignored Interpreting renovate\u0026rsquo;s debug output # The output from renovate\u0026rsquo;s debug logs is verbose. There are a few things I look for in the logs.\n\u0026ldquo;flattened updates found\u0026rdquo; # This line summarizes the packages renovate has found updates for. This helps you ensure your datasources are configure correctly, and that updated packages are found. As of this writing, the line looks like this:\nDEBUG: 5 flattened updates found: opentelemetry-sdk, opentelemetry-instrumentation-flask, opentelemetry-instrumentation-jinja2, opentelemetry-instrumentation-requests (repository=local) This tells me that my datasources are configured correctly, because these are the updates I am expecting. The specific issue this helped me identify is that the opentelemetry-instrumentation packages were considered \u0026lsquo;unstable\u0026rsquo; because they were versioned as 0.41b0, which is pre-release by pep440 versioning rules. Adding a renovate packageRule with \u0026quot;ignoreUnstable\u0026quot;: false made the instrumentation packages appear in this list.\n\u0026ldquo;packageFiles with updates\u0026rdquo; # Following this line is a large json object containing information about the files renovate would edit, and the updates that were found. This object is large, so I\u0026rsquo;ve omitted most of it, and only included the bits I look for:\n\u0026#34;config\u0026#34;: { \u0026#34;pip-compile\u0026#34;: [ { \u0026#34;deps\u0026#34;: [ { \u0026#34;depName\u0026#34;: \u0026#34;opentelemetry-sdk\u0026#34;, \u0026#34;datasource\u0026#34;: \u0026#34;pypi\u0026#34;, \u0026#34;currentVersion\u0026#34;: \u0026#34;1.18.0\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;bucket\u0026#34;: \u0026#34;non-major\u0026#34;, \u0026#34;newVersion\u0026#34;: \u0026#34;1.20.0\u0026#34;, \u0026#34;releaseTimestamp\u0026#34;: \u0026#34;2023-09-04T19:01:22.000Z\u0026#34;, \u0026#34;updateType\u0026#34;: \u0026#34;minor\u0026#34;, \u0026#34;branchName\u0026#34;: \u0026#34;renovate/opentelemetry\u0026#34; } ], // more fields omitted. }, // More dependencies omitted ], \u0026#34;packageFile\u0026#34;: \u0026#34;requirements.in\u0026#34; } ] } packageFile: the file in the repository that declares the dependencies. This is the file renovate would edit. The rest of these fields will appear for each depencency renovate finds:\ndepName: what renovate calls this package. currentVersion: the version currently listed in the packageFile updates[0].newVersion: the version renovate will upgrade you to. updates[0].branchName: the branch that this change will go into. This helps you identify if your package grouping rules are working as expected. This is not the most concise summary of renovate\u0026rsquo;s intended changes, but it has certainly helped me feel more confident in making renovate.json changes.\nFor a more complete test, it is recommended that you fork an existing repo, and let renovate run on that fork. For me, this would have required downgrading some packages so that renovate would have updates to do. That seemed a little tedious for my current use case, but maybe there will be a part 2 😅.\n","date":"2023-09-21","externalUrl":null,"permalink":"/2023/testing-changes-to-renovate-configs/","section":"Posts","summary":"I get a bit nervous whenever I touch renovate.json files, because I did not know how to test the effects of my changes. Well, this week I spent some quality time with Renovate and while the results are not perfect, they\u0026rsquo;re a lot better than what I had before!\nIf you\u0026rsquo;re not familiar with Renovate, it is a convenient way to keep your dependencies up to date. It understands dependencies in many language ecosystems, and supports a variety of ways to run and configure Renovate.","title":"Testing changes to Renovate configs","type":"posts"},{"content":"I recently spent some time with external application loadbalancers in GCP, and I found the data model pretty difficult to work with. What follows is an attempt to better explain these concepts, practicing some of the advice from Docs for Developers, which I\u0026rsquo;ve been reading.\nOverview # External Application Loadbalancers are represented in the API by a series of related configuration objects. There is no single \u0026ldquo;Loadbalancer\u0026rdquo; object, so it is important to ensure the relevant objects all reference each other.\nDescribed below are some of the objects involved in a Global External Application Loadbalancer, as I encountered them. The GCP docs describe many other types of LB configuration that may better fit your use case.\nURL Map - ( 💡, \u0026#x1f4d6;) responsible for routing incoming HTTP requests to the correct backend based on host and url. Backend Service - ( \u0026#x1f4d6;): tells the loadbalancer how to connect to your backend, but does not contain the list of backends! Network Endpoint Group (NEG) ( \u0026#x1f4d6;: contains the list of backends. Note that there are several types, for different types of backends (e.g. zonal VMs, serverless, etc) Backend Bucket ( \u0026#x1f4d6;): similar to a NEG, but used when serving from a Cloud Storage bucket. URL Map # URL Maps map incoming HTTP urls to Backend Services, through hostRules, pathMatchers and pathRules. They also contain the default backend service, to which requests will be send if they do not match any rules (or if no rules exist).\nHost Rules match only on the Hostname of incoming HTTP requests. They control which Path Matcher the request is sent to next.\nPath Matchers contain Path Rules, which map url \u0026ldquo;globs\u0026rdquo;, to a specific backend service.\nbackend service # Backend services contain a bunch of configuration for how the Load Balancer should connect to the service that is actually serving the request.\nThe backend service object contains a list of backends. the group field of a backed refers to either a Compute Instance Group (not discussed here), or a Network Endpoint Group. These references use urls that start with https://googleapis.com - these URLs can be used directly with gcloud commands, so there\u0026rsquo;s no need to parse them for their individual path components.\nnetwork endpoint group (NEG) # Important object with a terrible name \u0026#x1f926;\nThese list your actual backends. There is support for several different types, from fully-managed Serverless NEGs, to Internet NEGs (which are just host:port or ip:port endpoints).\nThe format varies depending on what kind of NEG you need, so be sure to check the Backend docs.\nBackend Bucket # Sort of a special kind of NEG, this serves static content from a Cloud Storage bucket. The \u0026ldquo;backend Bucket\u0026rdquo; object, like the Backend Service, contains meta-information about how to serve your content, and contains only a link to the actual gcs bucket (in the bucketName field).\nRecap # This probably deserves a diagram (with links to the relevant docs):\nclassDiagram direction LR class urlmap[\"URL Map\"]{ pathMatcher.defaultService pathmatcher.pathRules[].service } class backendservice { backends[].group } class NEG { various types } urlmap --\u003e backendservice backendservice --\u003e NEG link urlmap \"https://cloud.google.com/compute/docs/reference/rest/v1/urlMaps\" link backendservice \"https://cloud.google.com/compute/docs/reference/rest/v1/backendServices\" link NEG \"https://cloud.google.com/load-balancing/docs/negs/\" I hope this little tour through the jungle of LB products has been helpful. I hope I find this article again the next time i have to touch one of these!\n","date":"2023-08-07","externalUrl":null,"permalink":"/2023/understanding-gcps-loadbalancer-models/","section":"Posts","summary":"I recently spent some time with external application loadbalancers in GCP, and I found the data model pretty difficult to work with. What follows is an attempt to better explain these concepts, practicing some of the advice from Docs for Developers, which I\u0026rsquo;ve been reading.\nOverview # External Application Loadbalancers are represented in the API by a series of related configuration objects. There is no single \u0026ldquo;Loadbalancer\u0026rdquo; object, so it is important to ensure the relevant objects all reference each other.","title":"Understanding GCP's Loadbalancer models","type":"posts"},{"content":"This post assumes you\u0026rsquo;re already familiar with OpenTelemetry, and are already collecting some observability data.\nWhether you\u0026rsquo;ve chosen automatic instrumentation, or manual, you\u0026rsquo;re now collecting telemetry data from your code. Congratulations \u0026#x1f389;\nBut what about all the other code you\u0026rsquo;re using? When your service makes a database query, or fetches weather data, you\u0026rsquo;re using someone else\u0026rsquo;s code. These other services may have their own production problems - can you separate issues in your code from issues in a dependency with your current observability signals?\nIf you\u0026rsquo;re not sure, read on! We\u0026rsquo;ll cover creating metrics and traces around your existing calls to other services over HTTP or GRPC. The samples below are in Go, but similar tactics should work in most languages.\nBefore you start instrumenting these calls yourself, consider searching the OTel Registry for existing instrumentation libraries. For example, Postgres database users could adopt the pgotel library, which will auto-magically provide instrumentation for existing go-pg code.\nWrapping HTTP clients # Most of the APIs you\u0026rsquo;re calling are likely HTTP-based. Some of these services may provide a client library, some users may choose to create their own client library, and still others will choose to use a simple HTTP client. No matter which category you\u0026rsquo;re in, this approach can help you get better telemetry (provided your language supports interfaces or something equivalent).\nLet\u0026rsquo;s assume you\u0026rsquo;re using a client library to fetch pictures of cats, called a CatClient. You can create an instrumented version of this library using Go\u0026rsquo;s embedding.\nTo begin, we\u0026rsquo;ll define a type for our OTelCatClient:\ntype OTelCatClient struct { CatClient } Now we\u0026rsquo;ll need to \u0026ldquo;wrap\u0026rdquo; the CatClient method calls to include our instrumentation. For a method like CatClient.GetRandomCat, we can add a trace span as described in the OTel guide to Manual Instrumentation:\nfunc (c *OTelCatClient) GetRandomCat(c context.Context) Cat { ctx, span := c.tracer.Start(c, \u0026#34;get-random-cat\u0026#34;) defer span.End() return c.CatClient.GetRandomCat(ctx) } The same can be done to add Metrics as desired, to track the number of calls, or errors.\nWe can now use OTelCatClient the same way we would use a regular CatClient, and the instrumented client will produce a trace span for any calls to GetRandomCat.\nIf you produce your own client libraries, you can add instrumentation directly to your libraries with the OpenTelemetry API. By default, OpenTelemetry libraries use a no-op implementation which has a minimal effect on performance and does not record any data. When the OpenTelemetry SDK is configured by the consumer of your client libraries, all your beautiful telemetry will be available, sent to the destination of their choosing.\nGRPC Interceptors # For calls made over GRPC (which includes most of Google\u0026rsquo;s Client Libraries), you can get telemetry by using one of the GRPC Interceptors provided by the otelgrpc instrumentation library.\nGRPC Interceptors provide \u0026ldquo;hooks\u0026rdquo; in the GRPC handling process, as a way to implement logging, authorization, and other types of \u0026ldquo;middleware\u0026rdquo; tasks. The Interceptor concept is present in all supported GRPC languages, though I find it is not well described. This guide to gRPC and Interceptors is a nice summary of the concept.\nTo make use of the interceptor, it must be plumbed down into the GRPC Dial() call as an option. If you\u0026rsquo;re creating GRPC connections yourself, this is straightforward. For Google API Clients, it looks a bit like this:\nimport ( iam \u0026#34;cloud.google.com/go/iam/apiv2\u0026#34; \u0026#34;go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc\u0026#34; \u0026#34;google.golang.org/api/option\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; ) policyclient, _ := iam.NewPoliciesClient(r.Context(), option.WithGRPCDialOption( grpc.WithUnaryInterceptor( otelgrpc.UnaryClientInterceptor()))) Note that for Streaming APIs, there\u0026rsquo;s also an otelgrpc.StreamingClientInterceptor.\nThis Policy Client will now record OpenTelemetry spans for each of its GRPC calls, and report them to whichever backend OTel has been configured to use. These spans include labels such as the method it called, and what the returned status code. With this telemetry at your fingertips, it becomes easier to identify when your dependent services are experiencing latency or instability.\nRecap # We\u0026rsquo;ve discussed a few ways to add instrumentation when calling another service via custom clients, or GRPC.\nIf you create your own libraries, you can add native instrumentation (so your customers get better telemetry!) using the OTel guide to Instrumenting libraries.\nHappy Instrumenting! \u0026#x1f52d;\n","date":"2023-03-31","externalUrl":null,"permalink":"/2023/day-2-observability-calls-to-other-services/","section":"Posts","summary":"This post assumes you\u0026rsquo;re already familiar with OpenTelemetry, and are already collecting some observability data.\nWhether you\u0026rsquo;ve chosen automatic instrumentation, or manual, you\u0026rsquo;re now collecting telemetry data from your code. Congratulations \u0026#x1f389;\nBut what about all the other code you\u0026rsquo;re using? When your service makes a database query, or fetches weather data, you\u0026rsquo;re using someone else\u0026rsquo;s code. These other services may have their own production problems - can you separate issues in your code from issues in a dependency with your current observability signals?","title":"Day 2 Observability - calls to other services","type":"posts"},{"content":"","date":"2023-03-31","externalUrl":null,"permalink":"/tags/o11y/","section":"Tags","summary":"","title":"O11y","type":"tags"},{"content":"","date":"2023-03-31","externalUrl":null,"permalink":"/tags/observability/","section":"Tags","summary":"","title":"Observability","type":"tags"},{"content":"","date":"2023-03-31","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2023-02-24","externalUrl":null,"permalink":"/tags/cli/","section":"Tags","summary":"","title":"Cli","type":"tags"},{"content":"","date":"2023-02-24","externalUrl":null,"permalink":"/tags/gh/","section":"Tags","summary":"","title":"Gh","type":"tags"},{"content":"Last year, I started working in a number of public Github repositories, and learned to use the gh Github CLI. I wrote an article about using the github cli with multiple repos, but given how much my workflow has changed, I think its time for an update.\nPicking a random teammate # Sometimes, I need to pick a human to be responsible for something (usually a PR review). When there is no obvious choice (for example, someone who already knows the context of the PR), I caught myself relying on the same team members repeatedly. While this is not a big issue, I wanted a more \u0026ldquo;fair\u0026rdquo; way to pick a random assignee.\nTo do that, I first needed a quick way to list the members in a github team. Teams are typically referred to with a @my-org-name/my-team-name syntax - but the API call to list team members needs the individual parts, rather than the whole string. Bash string manipulation to the rescue! The following alias uses the %% and ## operators to remove the substring from the back or front of a string, respectively. This allows us to get either the org name or team name from the full string. We also use jq to print just the login of each member.\nThe alias should be added to the aliases section of your gh config file. On linux machines, that is ~/.config/gh/config.yml\nmembers: \u0026gt; ! gh api orgs/${1%%/*}/teams/${1##*/}/members | jq -r \u0026#34;.[].login\u0026#34; With this alias, I can run gh members my-org/my-team | shuf -n 1 to pick a random member of this team.\nCoping with long-running status checks # Some PRs run exhaustive tests as PR Checks, and these can take a while. As either an author or reviewer, I want to know when the Checks are done, so I can properly review the change.\nTo achieve this, I use the following gh alias:\n# pop up a notification when the checks are complete for a given PR. lmk: \u0026gt; ! ( gh pr checks $1 --watch \u0026gt; /dev/null ; notify-send \u0026#34;GH PR Checks done\u0026#34; \\ $(gh pr view $1 --json url --jq \u0026#34;.url\u0026#34;) ) \u0026amp; For the Mac users, you\u0026rsquo;ll need to replace the notify-send command with something like the following, after installing terminal-notifier:\nterminal-notifier -title \u0026#34;PR Ready: $1\u0026#34; -message \u0026#34;PR checks done\u0026#34; \\ -contentImage https://github.githubassets.com/images/modules/logos_page/Octocat.png \\ -open $(gh pr view $1 --json url --jq \u0026#34;.url\u0026#34;) In either case, this will produce a little desktop notification when the PR checks are completed. To use, just give it a PR number, branch, or full URL (just like with gh pr view).\nMaintainer / Reviewer SLOs # One of the responsibilities of a repository maintainer (or reviewer) is ensuring contributors get timely follow-up on their contributions. I have a few different reviewer roles, and each of them has different review expectations. To help me keep them straight, I\u0026rsquo;ve written a small gh extension:\nmuncus/gh-slocheck Go 0 0 In brief, the extension allows me to search open PRs, and sorts them with the oldest ones first, highlighting any that are older than a specified age. Most of this can be done without an extension command, but I wanted to also include status indicators for Status Checks, Review status, and Mergeability. Github provides this information through the API, and the tool just wraps them up in convenient output.\nWith this tool, I can now define aliases for reviews assigned to me, and also for each of my reviewer and maintainer roles.\nReviews I\u0026rsquo;m actively involved in: gh slocheck -s \u0026quot;involves:@me is:open review:required\u0026quot; --limit 20 Reviews where I\u0026rsquo;m explicitly requested: gh slocheck -s \u0026quot;user-review-requested:@me is:open\u0026quot; Reviews that are looking for a Golang-samples reviewer: gh slocheck -s \u0026quot;team-review-requested:googlecloudplatform/go-samples-reviewers is:open draft:false status:success\u0026quot; --limit 20 -w 36h This explicitly checks that the PR is ready for review, with passing status checks, and non-draft status. It also has a shorter warning period than the rest, since this repo has more strict review expectations. I like to prefix all these aliases with slo:, so I can see everything that needs my input with a few short commands.\nBenefits # This gives me a full view of my github review responsibilities with a few short commands, and helps me stay on top of PRs that need my attention.\nMoving to a gh-extension simplifies my aliases considerably, and gives me more flexibility in the output format and sort order.\n","date":"2023-02-24","externalUrl":null,"permalink":"/2023/more-github-cli-tips/","section":"Posts","summary":"Last year, I started working in a number of public Github repositories, and learned to use the gh Github CLI. I wrote an article about using the github cli with multiple repos, but given how much my workflow has changed, I think its time for an update.\nPicking a random teammate # Sometimes, I need to pick a human to be responsible for something (usually a PR review). When there is no obvious choice (for example, someone who already knows the context of the PR), I caught myself relying on the same team members repeatedly.","title":"More Github CLI Tips","type":"posts"},{"content":"(This content was originally published at https://dev.to/muncus/handy-yaml-tricks-415p)\nIn the past few years, YAML ( http://yaml.org) has become an essential part of software, particularly for infrastructure-as-code tools. Yaml at the heart of kubernetes configuration, kubernetes-inspired APIs like Google\u0026rsquo;s config connector, and a number of workflow systems like Google Cloud Workflows and Github Actions.\nIn its simplest forms, Yaml is quite human-readable, but over time many of these configurations become more complex, and the documentation of these formats is not always as complete or searchable as we might like. Included below are some small tips that may help you make the most of your Yaml configurations.\nIDE assistance with JSON Schemas! # Do you struggle to remember the names of fields in your Yaml objects? I sure do!\nMost modern editors and IDEs support the Language Server Protocol, which powers the code completion, validation, and tooltip features. Combined with a Yaml Language Server, we can get rich completion for Yaml files!\nInstallation varies depending on the editor, but I found installation with VSCode to be pretty straightforward.\nFor many common yaml files, the correct schema can be inferred from the file name, and looked up automatically with SchemaStore. SchemaStore hosts a wide collection of JSON Schemas, which can be used to validate Yaml. SchemaStore is backed by the SchemaStore Github Repository, and contributions of additional JSON Schemas are welcome!\nSchema detection can also be configured manually in your editor if auto-detection is inaccurate.\nSetting up the Yaml Language Server takes a bit of work, but if you regularly work with complex Yaml objects, you\u0026rsquo;ll be glad you took the time to set it up!\nBetter Multiline Strings with \u0026gt; and | # Sometimes, with workflow configuration like cloudbuild.yaml and Github Actions, we write long commandlines that are not very readable in an IDE.\nThe Yaml spec has what they call literal style (|) and folded style (\u0026gt;) to help with this.\nLiteral Style preserves newlines in the string, so it can be used to run multiple commands as a single step. For example, installing python dependencies and running the relevant tests can be done like this:\n- name: Run tests run: | pip install -r requirements.txt pytest -v . Note: In some situations, this may mask failures in the earlier commands if the last command exits successfully.\nFolding Style \u0026ldquo;condenses\u0026rdquo; whitespace in the string, replacing spaces and/or newlines with a single space as describe in the spec section on line folding. In short, it lets us insert newlines in a string to make it more readable.\nConsider this very long line from a github actions workflow that calls the github API with curl:\n- name: Get current repo settings run: curl -o repository.json https://api.github.com/repos/${{ github.repository}} -H \u0026#34;Authorization: Bearer ${{ github.token }}\u0026#34; Rewritten with Folding Style, it looks like this instead:\n- name: Get current repo settings run: \u0026gt; curl -o repository.json https://api.github.com/repos/${{ github.repository}} -H \u0026#34;Authorization: Bearer ${{ github.token }}\u0026#34; For long lines, this can make them easier to read, and easier to edit and review.\nThese tips have helped me write more maintainable Yaml files, and I hope they help you, too!\n","date":"2023-02-08","externalUrl":null,"permalink":"/2023/handy-yaml-tricks/","section":"Posts","summary":"(This content was originally published at https://dev.to/muncus/handy-yaml-tricks-415p)\nIn the past few years, YAML ( http://yaml.org) has become an essential part of software, particularly for infrastructure-as-code tools. Yaml at the heart of kubernetes configuration, kubernetes-inspired APIs like Google\u0026rsquo;s config connector, and a number of workflow systems like Google Cloud Workflows and Github Actions.\nIn its simplest forms, Yaml is quite human-readable, but over time many of these configurations become more complex, and the documentation of these formats is not always as complete or searchable as we might like.","title":"Handy Yaml Tricks!","type":"posts"},{"content":"","date":"2023-02-08","externalUrl":null,"permalink":"/tags/yaml/","section":"Tags","summary":"","title":"Yaml","type":"tags"},{"content":"","date":"2018-07-17","externalUrl":null,"permalink":"/archive/","section":"Archives","summary":"","title":"Archives","type":"archive"},{"content":"I\u0026rsquo;ve been a debian/ubuntu user for years, but had always been intimidated by the packaging process (because i tried to read the New Maintainer\u0026rsquo;s Guide). I decided to give it a shot recently, because I was building experimental software for the raspberry pi, and grew tired for scp and rsync.\nNote: The advice below does not abide by the debian packaging policy.\nVincent Bernat\u0026rsquo;s Pragmatic Debian Packaging is a great place to get started. Unfortunately, I only found it near the end of my efforts.\nSo\u0026hellip; why? # I had been tinkering with several different raspberry-pi-based projects, and I found copying around whole directories from my computer (where I do most of the coding) to the pi was becoming tedious. I was also looking for a clean and simple way to cleanly remove a project if I later decided it was no longer needed.\nI had looked into a few other \u0026lsquo;installation methods\u0026rsquo;, like shell scripts, and tarballs, etc. But none of them seemed to be quite simple enough to set up.\n\u0026ldquo;Simple?\u0026rdquo; you say, \u0026ldquo;you picked debian packages because you wanted simple?!\u0026rdquo;\nWell, no. It was not simple, but it does teach me more about debian packages, which is sure to be beneficial down the line.\nBefore we start # There is a great deal of tooling built around debian packaging, but if we start with the robots, there will be cleanup work to do later.\nset the environment variables DEBEMAIL to your email address, and DEBFULLNAME to your full (first + last) name. These are used in the autogenerated steps to come, and its easier to set them now than to fix the generated output.\n(Optional) put the following in ~/.devscripts, which is read by many debian tools:\n# Do not require directories to conform to packagename-version standard. DEVSCRIPTS_CHECK_DIRNAME_LEVEL=0 # dont rename the directory when the version number changes. DEBCHANGE_PRESERVE=no # Dont sign packages. I just build them for me, so signing is pointless. DEBUILD_DPKG_BUILDPACKAGE_OPTS=\u0026#34;-uc -us\u0026#34; Generate boilerplate: dh_make # I started by creating a basic debian package with dh_make --native. This gives me a \u0026ldquo;native\u0026rdquo; debian package, meaning that there is no separate source tarball to worry about. For my use case, i felt this was best, as it avoids the intermediate step of tarring up my files. This approach can be found in an appendix of the debian maintainer docs\ndh_make --native --packagename=$pkgname_$version\nWhere $pkgname is the name of the package you\u0026rsquo;re building, and $version is some arbitrary version number.\nSee docs on source/format for more information about native vs non-native packages.\nThis command will generate a debian/ directory, with lots of files in it. Anything that ends in .ex is an example, and can safely be deleted if you wish.\nCustomize the boilerplate for your needs # The most important two files generated are debian/control and debian/rules.\ndebian/control # The control file is metadata about your package. there are several placeholders in there, like Description and Section which should be filled in.\nBuild-Deps is a bit harder, and should include packages needed to build the package. Depends should list packages required for your package to function. I often guess at the right packages, and test by running a package build.\ndebian/rules # This is actually a Makefile, with rules for building your package. Thanks to debhelper (dh and friends), this file is typically short in my experience. There\u0026rsquo;s a variety of helper utils for specific purposes (for example, dh-golang helps build packages written in Go, and is discussed below).\nTrials and Errors: debuild # debuild is the main tool for building packages. now that we\u0026rsquo;ve customized some of the debian build files, its time to give it a shot. It probably wont work, but it will tell us what needs to be fixed. The lintian tool makes helpful suggestions that are probably worth fixing, and points out placeholders that need to be filled in.\nThe build process assumes that there\u0026rsquo;s a toplevel Makefile, though if you are packaging a simple collection of files, this is not necessary. See the section below on Install and Configuration.\nEventually, you should end up with a functional debian package in the parent directory of your package. If that\u0026rsquo;s all you needed, you\u0026rsquo;re done!\nHelpers, and other considerations # Install and Conffile # For packages that do not require compilation, or are platform-agnostic (e.g. python scripts), a Makefile feels a bit overwhelming. There\u0026rsquo;s a simpler approach: debian/install. this file describes how files from the package build directory should be installed by the package.\nFor example, a directory with files a, b, and c might choose to install into different directories like this:\na /usr/bin/ b /usr/sbin/ c /etc/ That file c looks like a configuration file, and we dont want to overwrite with the package version when we upgrade. debian/conffiles provides a mechanism for this. Just include the path to the installed configuration file (i.e. etc/c) in here, and dpkg will treat it as a configuration file.\nHelper: dh-golang # The debian go team has a golang packaging guide for full details, but here\u0026rsquo;s the short version:\ninstall dh-golang, which is a deb helper for making go packages. in debian/rules set DH_GOPKG to the name of your go package (what you\u0026rsquo;d usually go get) in debian/rules, add the following options to the dh invocation: --buildsystem=golang --with=golang run debuild again, and hopefully get a go package! For binary-only packages, set DH_GOLANG_BUILDPKG in rules, pointing to only the packages that are binaries (which are all under cmd, right?)\nHelper: dh-systemd # If you are creating services to run at startup, say on a raspberry pi, you\u0026rsquo;ll likely want to include some systemd configuration. There are some docs for dh-systemd.\nFor debhelper versions \u0026gt;= 10, systemd is activated by default, according to the above.\nadd a systemd file as debian/packagename.systemd, and it will be included in the deb package.\n","date":"2018-07-17","externalUrl":null,"permalink":"/archive/debian-packaging/","section":"Archives","summary":"I\u0026rsquo;ve been a debian/ubuntu user for years, but had always been intimidated by the packaging process (because i tried to read the New Maintainer\u0026rsquo;s Guide). I decided to give it a shot recently, because I was building experimental software for the raspberry pi, and grew tired for scp and rsync.\nNote: The advice below does not abide by the debian packaging policy.\nVincent Bernat\u0026rsquo;s Pragmatic Debian Packaging is a great place to get started.","title":"Building Debian packages for my own use.","type":"archive"},{"content":" TL;DR - I made a vim compiler plugin for arduino that parses errors into the quickfix list, and learned a bunch about the quickfix list and compiler plugins.\nI\u0026rsquo;ve been working on several small arduino projects lately (mostly the series of bluetooth projects posted earlier). While I find the arduino toolchain easy to use, I\u0026rsquo;ve never enjoyed using the editor, or having to use the mouse to click the Verify and Upload buttons.\nExisting vim-arduino integration # A bit of googling brought to my attention that the arduino tool can also be used as a commandline tool as of version 1.15, and has a man page. Much of the desired vim integration exists already, in the github/stevearc/vim-arduino plugin, but the one thing I was still missing was the quickfix list of errors.\nA small discussion of vim variables # This sent me down the rabbithole of reading vim\u0026rsquo;s help about quickfix, and several other related topics (setqflist, QuickfixCmdPost, errorformat, efm-entries, etc). The first lesson is one of maintainability, as there are many errorformat strings on github, but most of them are just there as one long string, with commas, like the default one in this vim session:\nerrorformat=%*[^\u0026#34;]\u0026#34;%f\u0026#34;%*\\D%l: %m,\u0026#34;%f\u0026#34;%*\\D%l: %m,%-G%f:%l: (Each undeclared identifier is reported only once,%-G%f:%l: for each function it appears in.),%-GIn file included from %f:%l:%c:,%-GIn file included from %f:%l:%c\\,,%-GIn file included from %f:%l:%c,%-GIn file included from %f:%l,%-G%*[ ]from %f:%l:%c,%-G%*[ ]from %f:%l:,%-G%*[ ]from %f:%l\\,,%-G%*[ ]from %f:%l,%f:%l:%c:%m,%f(%l):%m,%f:%l:%m,\u0026#34;%f\u0026#34;\\, line %l%*\\D%c%*[^ ]%m,%D%*\\a[%*\\d]: Entering directory%*[`\u0026#39;]%f\u0026#39;,%X%*\\a[%*\\d]: Leaving directory %*[`\u0026#39;]%f\u0026#39;,%D%*\\a: Entering directory%*[`\u0026#39;]%f\u0026#39;,%X%*\\a: Leaving directory %*[`\u0026#39;]%f\u0026#39;,%DMaking %*\\a in %f,%f|%l| %m\u0026#39; The first little vim trick is that the option errorformat (and all other options, actually), can be set as variables by prepending \u0026amp;. The following two lines do the same thing:\nfiletype=markdown let \u0026amp;filetype=markdown Why would we choose the second syntax? I learned from the vim help for efm-entries that there are multiple ways to change the value of long strings like errorformat. We can build them up from smaller strings by appending, by using .= instead of =. According to efm-entries, += and -= can be used to add and remove entries in errorformat.\nSetting errorformat # The format of errors is tricky here because the arduino toolchain actually pre-processes the arduino sketch files into a standard C++ file before building, so the resulting error messages do not always contain a filename. Sample output:\nPicked up JAVA_TOOL_OPTIONS: Loading configuration... Initializing packages... Preparing boards... Verifying... sketch_file:32: error: \u0026#39;asdf\u0026#39; does not name a type asdf ^ /path/to/sketch_file.ino: In function \u0026#39;void setupCandleService()\u0026#39;: sketch_file:59: error: \u0026#39;candleService\u0026#39; was not declared in this scope candleService.begin(); ^ exit status 1 Ignoring the \u0026ldquo;standard\u0026rdquo; output lines is easy enough with the errorformat specifier %-G, which ignores the lines that match it. (e.g. %-GInitializing packages...).\nThe next tricky bit was that the two errors above only show one full file path, and it is for the second error. Here I considered fetching the existing quickfix list in a QuickfixCmdPost, and \u0026ldquo;fixing\u0026rdquo; the files to be opened, but after spending some time on this solution, it occurred to me that the most likely outcome was that the error was in the current buffer.\nPutting it all together # To get the right compiler options set, I created my own vim compiler plugin, for arduino. Since the existing plugin from stevearc/vim-arduino sets up make, I used that. The only real settings here are for the errorformat parsing (the top bit is all boilerplate for vim compiler plugins, and copied from the help docs). Here is the custom arduino compiler plugin, and to make it work automatically, I set the compiler in the filetype plugin for arduino.\nNow when I open an arduino sketch file in vim, I can verify the build, and get decent quickfix integration as well!\n","date":"2018-01-16","externalUrl":null,"permalink":"/archive/vim-arduino/","section":"Archives","summary":"TL;DR - I made a vim compiler plugin for arduino that parses errors into the quickfix list, and learned a bunch about the quickfix list and compiler plugins.\nI\u0026rsquo;ve been working on several small arduino projects lately (mostly the series of bluetooth projects posted earlier). While I find the arduino toolchain easy to use, I\u0026rsquo;ve never enjoyed using the editor, or having to use the mouse to click the Verify and Upload buttons.","title":"Vim and Arduino","type":"archive"},{"content":"I\u0026rsquo;ve recently become interested in bluetooth, specifically web-bluetooth, which is newly enabled in most chrome/firefox/opera browsers. It allows a web app (javascript) to interact with bluetooth devices near the web browser. Combine this with eddystone beacons and the bluetooth device is advertising a URL that links a user directly to an app to interact with the device.\nHardware and software # To play with bluetooth, the first thing I needed was a bluetooth radio. Some simple beacon stuff can be done with the beacon toy app, I wanted to use a microcontroller, suitable for embedding into projects, as this was my eventual goal. There are plenty of good options on this front:\nmicro:bit Adafruit nRF51 Adafruit nRF52 Sparkfun ESP32 Thing many more. I went for the Adafruit nRF52, only because that was the first one that caught my eye. the micro:bit has several other onboard sensors, and is cheaper, so I may switch to that platform eventually.\nTo program the nRF52, i\u0026rsquo;m using the arduino toolchain, but since i\u0026rsquo;m a vim user, i\u0026rsquo;m editing primarily with vim, and only using the arduino IDE parts for the compilation. (I later switched to using the vim-arduino plugin, but i\u0026rsquo;ll talk about that setup separately).\nThere are also some mobile apps that are helpful here, notably nRF Connect, which is produced by Nordic Semiconductor, a major manufacturer of bluetooth chips. It has a variety of modes, and companion apps that i found indispensible for debugging and testing these examples.\nFirst steps: Eddystone Beacon # My first goal was to just get an Eddystone beacon broadcasting, to direct an interested user to a web page. The code is nearly verbatim from the adafruit nRF52 example. it is important to note that the Eddystone protocol only allocates 17 bytes for the encoded URL. No error message is emitted when using a url that is too long. The best practice for avoiding problems here is to use a url shortener like goo.gl. This has the added benefit of letting you change the beacon\u0026rsquo;s destination without having to update the beacon device.\nNext: Playbulb candle emulation # At this point, i\u0026rsquo;m more familiar with arduino/C++ than with javascript, so I opted to create a device that an existing webapp could interact with. I stumbled on the playbulb candle codelab, which interacts with a fairly simple custom bluetooth device.\nBased on the javascript code, I discovered the protocol used to set the name, color and light effect on the playbulb candle devices. These are handled by individual Characteristics inside of a Bluetooth GATT service. Armed with this knowledge, i created a sketch that implemented enough of the service to interact with the app.\nThe webapp has some rough edges, notably that if web-bluetooth is not enabled, there is no error message displayed (though there is one printed to the javascript console if you open chrome\u0026rsquo;s developer tools). The same is true if you access the page over http, rather than https. (https is required for all web-bluetooth functionality, in accordance with the spec).\nThe big takeaway from this example is that i\u0026rsquo;ve now implemented entirely custom BLE services on the device, with read-only and read-write characteristics. These can serve as the basis for any custom services I build later.\nNordic Uart Service: not-exactly-standard, but close enough. # Chronologically, this project was the second one i built, not the third. but logically it makes more sense here.\nIn my research about bluetooth LE services, i kept seeing references to the Nordic Uart Service (NUS). This is a service common to many of the chips from Nordic Semiconductor, that emulates a standard bluetooth UART connection over BLE.\nWhile the nRF52 i\u0026rsquo;m using is made by Nordic, it does not have built-in support for this service, so I decided to build a simple Echo service on the NUS protocol. A later addition can interpret commands delivered over this link, to perform actions. See the terminal echo sketch for the details here. I was now able to use nRF Connect mobile app to connect to the device over NUS, and send/receive text.\nWondering how to apply this to a web-bluetooth app, i came across a web bluetooth terminal app that had a similar behavior, but was built using only a single service and characteristic, where NUS uses two characteristics in the same service (one for Transmit, and one for Receive). With some refactoring of the main javascript file, I was eventually able to produce a terminal app that\u0026rsquo;s compatible with NUS, and could use it to connect and interact with my bluetooth echo device, which would dutifully reply with whatever it was sent. (as long as it was in chunks smaller than 20 bytes, which is the max write size for a ble characteristic). The original terminal had some buffering to work around this, but with my limited javascript experience, i decided to remove it, as it complicated the parsing of ble packets.\nWhat\u0026rsquo;s next? # Well, now that i\u0026rsquo;ve mastered beacons, created custom services and characteristics, and exchanged simple text commands over BLE, its time to build something bigger! Maybe Zork over ble, or multi-player bluetooth hungry-hungry-hippos. I\u0026rsquo;m not sure exactly what\u0026rsquo;s next, but stay tuned to find out!\n","date":"2017-11-25","externalUrl":null,"permalink":"/archive/bluetooth-pt1/","section":"Archives","summary":"I\u0026rsquo;ve recently become interested in bluetooth, specifically web-bluetooth, which is newly enabled in most chrome/firefox/opera browsers. It allows a web app (javascript) to interact with bluetooth devices near the web browser. Combine this with eddystone beacons and the bluetooth device is advertising a URL that links a user directly to an app to interact with the device.\nHardware and software # To play with bluetooth, the first thing I needed was a bluetooth radio.","title":"Bluetooth Part 1","type":"archive"},{"content":"I did not really need a thermal receipt printer, but i bought one anyway. Then i tried to make it useful.\nCode on github\nThe Basics # First, i followed the excellent tutorial from Adafruit(where i bought the printer). It covers all the initial setup stuff very well, and got me to a working printer that outputs simple text, with some basic text styling features (and other stuff like barcodes i did not expect to use).\nPrinter Drivers # A printer is fun, but I don\u0026rsquo;t typically have a monitor connected to my raspberry pi, so printing things was infrequent, as I had to ssh into the pi, and run a script.\nI found another excellent adafruit tutorial that described the setup process for a network-connected thermal printer.\nThis let me sent more complex content to the printer, and print from other computers. This setup is great for printing out a grocery list before heading off to the grocery store. It uses the thermal printer just like any other printer.\nI ran into some interesting paper sizing troubles here, where pages were printed in the aspect ratio of a 8.5x11in page, but only 2in wide.\nPrint the list, on demand. # Being able to print the grocery list, or my todo list, is good, but still involves using a regular computer to print out the list. The next idea was to set up physical buttons that would cause the pi itself to fetch the list, and print it.\nI keep a few different lists on todoist, which has a decent api, and a python client library. Python is perfect, since the Pi also has a python-based library for reading the GPIO pins (where we can attach the buttons). With some small refactoring of the gpio_listener.py script from the above example, it will listen for multiple buttons, and launch the todoist script to print any number of pre-configured lists.\nBe aware that the search strings \u0026ldquo;supported\u0026rdquo; by the todoist api are not the same as the searches done through the search box. Many search features are for paying customers only. There is a todoist help article on Filters that explains available filters, but only some of them appear to work correctly for me (not a paying customer). Notably, the no date, today, and overdue filters dont appear to work. \u0026#x1f626;\nI was still able to build a reasonable set priority searches that work well enough for me.\nAside: packaging # My process so far has been to develop on my laptop, and copy files over to the Pi for testing. This workflow gets a little tedious, and means i\u0026rsquo;m constantly overwriting files with new copies, from a machine which cannot run most of this code (the pi\u0026rsquo;s GPIO libraries are not available for my laptop).\nTo ease this tedium, and allow for easier rollbacks, I decided to put the todo-printer files into a quick-and-dirty debian package, so i\u0026rsquo;d only need to copy a single file over to the pi, and could install/uninstall as needed. My process for doing that will be covered in a separate post about quick-and-dirty debian packaging.\nEnclosure # The last step of the project is to build a suitably attractive case for the pi and printer. I settled on a cube ~4.5\u0026quot; in all dimensions. This size allows plenty of room for the printer, and a set of 4 small pushbuttons on the top surface.\n","date":"2017-06-30","externalUrl":null,"permalink":"/archive/thermal-printer/","section":"Archives","summary":"I did not really need a thermal receipt printer, but i bought one anyway. Then i tried to make it useful.\nCode on github\nThe Basics # First, i followed the excellent tutorial from Adafruit(where i bought the printer). It covers all the initial setup stuff very well, and got me to a working printer that outputs simple text, with some basic text styling features (and other stuff like barcodes i did not expect to use).","title":"Thermal Receipt printer experiments","type":"archive"},{"content":"The Internet can be a very distracting place sometimes. There are so many sites (particularly social media) that demand our attention, and let us mindlessly scroll through photos and status updates, when we really want to be doing something else.\nIn an effort to aid my own concentration, I build a proof-of-concept Internet Volume Control that helps filter out some of the distraction.\nAll my work is up on github.com/muncus/internet-volume and the background / origin story is below.\nInspiration and Design # While i was thinking about this, i considered several possible approaches to filtering out \u0026ldquo;noisy\u0026rdquo; internet content. A \u0026ldquo;captive portal\u0026rdquo; like public wifi, that would have specific rules. A \u0026ldquo;transparent proxy\u0026rdquo; that would rewrite content (sort of like an ad-blocker). These all had different drawbacks, particularly when it comes to HTTPS content.\nThen I realized that there\u0026rsquo;s a service under the rest of the internet\u0026rsquo;s noisy content: DNS! Before an app or a web browser can do anything, it needs to talk to DNS to find the \u0026ldquo;address\u0026rdquo; for the noisy service (e.g. facebook.com). By intercepting these calls, it is relatively easy to prevent users from accessing these sites. It is not foolproof (anyone willing to edit their DHCP-received nameservers can get around it). But it seemed good enough for my purposes.\nSo, using some handy Ruby libraries, I put together a simple DNS server that checks the \u0026ldquo;internet volume\u0026rdquo; (more on that later) and either returns the upstream response (from Google\u0026rsquo;s public DNS server), or returns an error of NXDOMAIN, which indicates the name cannot be resolved.\nInternet Volume # Now that i\u0026rsquo;d built the guts of the volume control, I needed a way to set the volume. Obviously, this was going to be a big dial of some kind, but how would it connect to the rest of the system?\nI\u0026rsquo;d built a few standalone internet-connected things before, and this is just one more. The wifi-connected dial (well, potentiometer plus wifi-connected microcontroller) reports the current \u0026ldquo;position\u0026rdquo; as a number from 0 - 10. This number is sent to adafruit.io, but could use any other IoT pubsub service.\nSome updates to the DNS server to read (and cache!) this value, and i\u0026rsquo;m ready for distraction-free wifi browsing!\nFuture work # I\u0026rsquo;d like to add a proper captive portal \u0026ldquo;login\u0026rdquo; page which describes the project, so users are not caught by surprise when i dial down the volume.\n","date":"2017-06-22","externalUrl":null,"permalink":"/archive/internet-volume/","section":"Archives","summary":"The Internet can be a very distracting place sometimes. There are so many sites (particularly social media) that demand our attention, and let us mindlessly scroll through photos and status updates, when we really want to be doing something else.\nIn an effort to aid my own concentration, I build a proof-of-concept Internet Volume Control that helps filter out some of the distraction.\nAll my work is up on github.com/muncus/internet-volume and the background / origin story is below.","title":"Internet Volume Control","type":"archive"},{"content":"Inspired by the Amazon Dash-style esp8266 IoT button, I built a button my distant family could use to request a refill of my home-made jam (though it could be used for anything). To keep with the theme, i built the whole thing to fit in a standard 8oz jam jar.\nUnlike the inspiration, this button should be able to live for months in the pantry without power, so it needed a power switch. After designing a P-channel mosfet circuit to handle this, i came across the adafruit powerswitch which is pretty much exactly what i needed. Throw a micro-lipo battery charge circuit in there and BOOM - button that turns on the ESP chip, and allow it to turn itself off.\nThe powerswitch means that there is no longer a \u0026ldquo;button press\u0026rdquo; from the ESP perspective, but the IFTTT event should fire as soon as it gets connected to the wifi network. Minor code changes were needed to make that happen, and then assert the pin connected to the powerswitch\u0026rsquo;s p-fet gate (to power off the device). All code is available on github/muncus/jambutton.\nAfter configuring a couple of IFTTT recipes, a press of the button results in ~20 seconds of powered-on time for the device, and a pushbullet notification being sent to my phone.\nI\u0026rsquo;m currently playing around with a small thermal printer to print out a physical receipt for the jam order, so they are harder to ignore. :)\n","date":"2015-12-26","externalUrl":null,"permalink":"/archive/jamomatic/","section":"Archives","summary":"Inspired by the Amazon Dash-style esp8266 IoT button, I built a button my distant family could use to request a refill of my home-made jam (though it could be used for anything). To keep with the theme, i built the whole thing to fit in a standard 8oz jam jar.\nUnlike the inspiration, this button should be able to live for months in the pantry without power, so it needed a power switch.","title":"Jam-O-Matic","type":"archive"},{"content":"San Francisco\u0026rsquo;s Muni trains are not well known for their ability to be on time. Recent sfmta data shows a ~60% on-time rate (where \u0026ldquo;on-time\u0026rdquo; includes arriving anywhere between 1 minute before to 4 minutes after the intended time). Fortunately, they provide real-time arrival estimates with the Next Bus service. While the QuickMuni app does a great job of displaying this on my phone, I wanted to build a more \u0026ldquo;ambient\u0026rdquo; display, to help me decide when to leave in the morning.\nRevision 1 # My first attempt at a muni display was an Arduino with 3 big LEDs, laid out as a stoplight. A python script parsed train arrival times, and sent them to the arduino, which then lit the corresponding lights: Green for \u0026gt;10 minutes, Yellow for 7-10, Red for 6-7. (These times were chosen based on how long it took for me to walk to the train).\nThe most obvious drawback here was the need for a computer to run the python script. I was eventually able to get the script running on a wireless access point with DD-WRT, but the package was still rather awkward.\nThe first working prototype Around this time, Quick Muni came out, so I had a quick phone-based way to check trains, and I lost interest for a while.\nRevision 2 # Some time later, I bought a Particle Photon (formerly known as the Spark Core). Small, and wifi-enabled, it was perfect for a tiny train display.\nInstead of a stoplight, I found a small servo, and got the Photon using it very quickly thanks to their builtin Servo library. Original Proof-of-concept pushed the train times to the device through the Particle Cloud api, but I wanted the device to be a bit more self-sufficient.\nWiring photo of muni 2.0 To that end, I put together a web server with some configuration that maps the Particle device name (which the device can fetch from the Particle cloud api) to a set of nextbus query parameters. The device requests /times/scrapple_ferret (for instance), and the server returns the number of minutes until the next estimated arrival at the stop at which scrapple_ferret is configured.\nThe servo, however, makes a slight noise when it moves, and I found that having it run all night was not ideal. I added a button to activate the device for ~30m, and a pair of LED headlights that turn on when the device activates, and gradually dim as the device turns off.\nAll that remained was to paint up a display face, vaguely resembling the from of an SF Muni train, and hang it on the wall!\nFinished enclosure, on wall The code for this project is available from github.com/muncus/muni-display.\n","date":"2015-08-24","externalUrl":null,"permalink":"/archive/muni-displays/","section":"Archives","summary":"San Francisco\u0026rsquo;s Muni trains are not well known for their ability to be on time. Recent sfmta data shows a ~60% on-time rate (where \u0026ldquo;on-time\u0026rdquo; includes arriving anywhere between 1 minute before to 4 minutes after the intended time). Fortunately, they provide real-time arrival estimates with the Next Bus service. While the QuickMuni app does a great job of displaying this on my phone, I wanted to build a more \u0026ldquo;ambient\u0026rdquo; display, to help me decide when to leave in the morning.","title":"Muni Displays","type":"archive"},{"content":"Emoji keyboards on mobile devices have been in widespread use for a while, but I sometimes find myself using a computer, and have a hard time remembering the unicode identifiers for a nice cup of tea (1f375 - 🍵 ) or a suitable warning character (2620 - ☠ ). So I decided to make a supplemental Unicode keyboard.\nCode # The code can be found on Github: http://github.com/muncus/unicode-keyboard. User-servicable parts are in config.h.\nSince the key sequence for entering a unicode character varies by operating system, the current implementation only supports Linux, though there are notes in the code for how support for other OSes could be implemented.\nBuild # I had a Teensy 2.0 handy, for which there are several good examples of emulating a USB keyboard. Any old button will do, but I wanted that good old keyboard feel, so i bought a Cherry MX switch sampler from WASD keyboards and some spare switches, just in case.\nWired one side of the switches all to ground, the other side to the first 6 data pins on the Teensy.\nOnce that was done, all that was left was to add the symbols to the keys. Knowing i\u0026rsquo;d probably want to change these frequently, i chose to print out the symbols, and just tape them to the keycaps with clear scotch tape. Done!\n","date":"2015-04-21","externalUrl":null,"permalink":"/archive/unicode-keyboard/","section":"Archives","summary":"Emoji keyboards on mobile devices have been in widespread use for a while, but I sometimes find myself using a computer, and have a hard time remembering the unicode identifiers for a nice cup of tea (1f375 - 🍵 ) or a suitable warning character (2620 - ☠ ). So I decided to make a supplemental Unicode keyboard.\nCode # The code can be found on Github: http://github.com/muncus/unicode-keyboard. User-servicable parts are in config.","title":"Unicode (emoji) keyboard","type":"archive"},{"content":"I was working on a particularly challenging service turndown, which involved handholding some user migrations. During the waiting periods, it occurred to me that turning off the service by simply typing the right command lacked a certain gravity.\nSo I grabbed a Bluefruit EZ-key (12-key programmable bluetooth keyboard), a big red arcade button, and a 9-volt battery. Using the adafruit intro docs, it was simple to get the button working. But it still wasnt quite right.\nIt wasn\u0026rsquo;t until i found a fancy cardboard box at the local variety store that things really came together. A few tasteful stamps invited the pushing of the big red button.\nI also wrote a quick script for pairing, since the pairing instructions for linux were a little cumbersome. (unfortunately, there\u0026rsquo;s a lot of sudo in there\u0026hellip;).\nNow i\u0026rsquo;m fully prepared for the next dramatic launch (or turndown)!\n","date":"2015-01-01","externalUrl":null,"permalink":"/archive/big-red-button/","section":"Archives","summary":"I was working on a particularly challenging service turndown, which involved handholding some user migrations. During the waiting periods, it occurred to me that turning off the service by simply typing the right command lacked a certain gravity.\nSo I grabbed a Bluefruit EZ-key (12-key programmable bluetooth keyboard), a big red arcade button, and a 9-volt battery. Using the adafruit intro docs, it was simple to get the button working. But it still wasnt quite right.","title":"Big Red Button","type":"archive"},{"content":"","date":"0001-01-01","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"}]