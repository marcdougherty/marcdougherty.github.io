[{"content":"Maintaining up-to-date dependencies in a large Go codebase is relatively simple most of the time - there are still a few situations that I find challenging. Notably, when a package does not comply with semantic versioning and makes API changes in minor versions.\nThanks to the Go communities focus on compatability, this does not usually affect too many dependencies. Aside from the occasional mistake, two widely used projects routinely disobey semantic versioning conventions - Kubernetes and OpenTelemetry.\nI understand that these projects have bigger coordination challenges than most, and I do respect the choices they have made. But I have my own perspective as a maintainer. \u0026#x1f604;\nUpdates and API differences # Go\u0026rsquo;s built-in tooling for identifying updates is pretty great: go list -m -u will show you all the dependencies that have a new version.\nBut this list, for lax maintainers like myself, can sometimes get surprisingly large. I\u0026rsquo;m looking for a way to separate the \u0026ldquo;low risk\u0026rdquo; updates from the higher risk updates.\nThat\u0026rsquo;s where the golang.org/x/tools/apidiff package comes in. In essence, this package analyses the exported symbols in two packages and determines if they are \u0026ldquo;compatible\u0026rdquo; (see apidiff\u0026rsquo;s definition of compatible for details). While this may not be a perfect heuristic, it is certainly more information than I had previously!\nI can now think about this problem as a relatively simple process:\nLoad the current and \u0026ldquo;new\u0026rdquo; version of a dependency (load from the modcache, or populate it if needed) use apidiff to determine the differences in exported symbols Check the report for Incompatible differences no incompatible differences means its a low risk update any incompatible differences makes it a high risk update The prototype # With the above process in mind, I set out to build a prototype tool that would do this for me. I would not consider this a \u0026ldquo;production grade\u0026rdquo; solution, or even a \u0026ldquo;supported\u0026rdquo; solution - its more of a proof-of-concept. (it does some awkward things like rummaging around in your $GOMODCACHE and running go download to fetch updated dependencies.\nmuncus/go-depdiffs Go 0 0 Now I can run go-depdiffs --risk low to see all the easy updates, or go-depdiffs --risk high -v to see the API differences in the higher risk updates.\nSee the README in the repo for more examples of output and usage.\nNext steps and future work # Loading # Loading these modules from the modcache mostly works, but could certainly be improved. For example, package.Load() from a file path works, but the error handling is awkward because the package driver gives me errors back as a string. :gasp:\nFalse positives # There is a high degree of false positives with this approach - notably, sometimes a symbol changes that my code does not use. One example of this is the constant google.golang.org/grpc.Version, which changes on each release.\nIn the future, dependency diffs could analyze the calling codebase, to see if the incompatible symbols are used. This mirrors the approach used in the govulncheck tool, that checks if your code calls any of the exploitable functions for a known vulnerability.\nThis information could then be used to lower the assessed risk of a particular update if none of the incompatible symbols are referenced.\nPluggable risk assessment # With some significant refactoring of the prototype, I could see a few different risk assessment styles, selectable from the commandline.\nConclusions # Overall, this prototype has been a good learning exercise. I\u0026rsquo;m also willing to use this tool to take a \u0026ldquo;first pass\u0026rdquo; to break down massive, unwieldy pull requests from dependabot.\n","date":"7 November 2025","externalUrl":null,"permalink":"/2025/go-dependencies-and-api-diffs/","section":"Posts","summary":"\u003cp\u003eMaintaining up-to-date dependencies in a large Go codebase is relatively simple\n\u003cstrong\u003emost\u003c/strong\u003e of the time - there are still a few situations that I find challenging.\nNotably, when a package does not comply with \u003ca\n  href=\"http://semver.org\"\n    target=\"_blank\"\n  \u003esemantic\nversioning\u003c/a\u003e and makes API changes in minor versions.\u003c/p\u003e\n\u003cp\u003eThanks to the Go communities focus on compatability, this does not usually\naffect too many dependencies. Aside from the occasional mistake, two widely used\nprojects routinely disobey semantic versioning conventions - Kubernetes and\nOpenTelemetry.\u003c/p\u003e","title":"Go dependencies and API diffs","type":"posts"},{"content":"","date":"7 November 2025","externalUrl":null,"permalink":"/","section":"Marc Dougherty","summary":"","title":"Marc Dougherty","type":"page"},{"content":"","date":"7 November 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"As discussed earlier in this series, there are several factors that affect how client libraries communicate with your AI Model service.\nIn this article, I\u0026rsquo;ll be demonstrating using some of the popular client libraries to connect to models served in 3 different ways, to demonstrate the usability and flexibility of these client libraries. For this comparison, I\u0026rsquo;ll be using python since it is the most common choice in LLM user communities.\nThis is by no means a complete accounting of client libraries, but a quick overview to get you started on the right path.\nTL;DR: I suggest Langchain for maximum flexibility. OpenAI client is a great choice (though less great if you\u0026rsquo;re hosting the model on Vertex AI as configuration and debugging are difficult.) Criteria # The library evaluation will focus on the following criteria.\nAPI usability a subjective measure of the libraries usablility. Does it provide a good API surface? Does it produce useful errors when things go wrong? etc. Model Flexibility Can the library be used to access Models using both the Chat and Completions API? Does it work for Instruction-tuned models? How much change is needed to use a different model? Model services # To evaluate flexibilty, we\u0026rsquo;ll be using 3 different model services:\nGemini 1.5 Flash Google\u0026rsquo;s hosted offering. Gemma 2 on Vertex AI Google\u0026rsquo;s Gemma is an Open Model, which can be deployed to their Vertex AI platform through the AI Model Garden. This service is deployed on GPUs, and served with Huggingface\u0026rsquo;s TGI. Gemma 2 on GKE Gemma models can also be hosted on Kubernetes. This service is also deployed on GPUs, with TGI. Libraries # Each of the code snippets below has been validated to work at the time of this writing - but AI is a rapidly evolving space, so some changes may be necessary.\nOpenAI Client # OpenAI\u0026rsquo;s Client libraries are probably the first thought of anyone who\u0026rsquo;s already working with AI client libraries. They are something of the industry default, since OpenAI\u0026rsquo;s APIs are widely copied in other AI model serving tools like vLLM and TGI.\nFor long-running processes, it is necessary to refresh the google credentials that the OpenAI client uses - it is not done automatically. Use this sample on credential refreshing to implement this ability.\nOpenAI and Gemini # import google.auth import google.auth.transport import google.auth.transport.requests from openai import OpenAI import vertexai creds, project = google.auth.default(scopes=[\u0026#34;https://www.googleapis.com/auth/cloud-platform\u0026#34;]) auth_req = google.auth.transport.requests.Request() creds.refresh(auth_req) LOCATION=\u0026#34;us-central1\u0026#34; PROJECT_ID=project vertexai.init(project=project, location=LOCATION) client = OpenAI( base_url = f\u0026#39;https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/openapi\u0026#39;, api_key = creds.token) r = client.chat.completions.create( model=\u0026#34;google/gemini-1.5-flash\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;tell me a frog fact\u0026#34;}], ) print(r.choices[0].message.content) OpenAI and Vertex AI # I was not able to get this working with either OpenAI or cURL with Google\u0026rsquo;s OpenAI Client documentation\nI consistenly received 400 errors, with either \u0026lsquo;PRECONDITION_FAILED\u0026rsquo; or \u0026lsquo;INVALID_REQUEST\u0026rsquo;. no other information was available.\nimport google.auth import google.auth.transport import google.auth.transport.requests from openai import OpenAI import vertexai from openai.types.chat import ChatCompletion creds, project = google.auth.default() auth_req = google.auth.transport.requests.Request() creds.refresh(auth_req) LOCATION=\u0026#34;us-central1\u0026#34; PROJECT_ID=project ENDPOINT=\u0026#34;my-endpoint-number\u0026#34; vertexai.init(project=project, location=LOCATION) client = OpenAI( base_url = f\u0026#39;https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT}/\u0026#39;, api_key = creds.token) r = client.chat.completions.create( model=\u0026#34;tgi\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;tell me a frog fact\u0026#34;}], ) print(r) OpenAI and GKE # This looks a lot like the Gemini sample, but notably simpler because we are not doing any authentication and have a simpler base URL.\nfrom openai import OpenAI client = OpenAI( base_url = \u0026#39;http://localhost:8080/v1\u0026#39;, api_key = \u0026#34;unused\u0026#34;) r = client.chat.completions.create( model=\u0026#34;unused\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;tell me a frog fact\u0026#34;}], ) print(r.choices[0].message.content) Results # Usability: \u0026#x1f7e0;\nThe OpenAI client is flexible, and can be used to talk to any OpenAI-compatible model server (which is nearly all of them!).\nHowever, using this library with Google\u0026rsquo;s offerings does not seem like a top priority for either party - there are clearly some sharp edges.\nFlexibility: \u0026#x1f534;\nThis is another case where I want to grade the library and the service separately. The client is basically just a well-wrapped HTTP client, and is adequately flexible. (especially when enabling debug logs, httpx provides solid debugging info.)\nGoogle\u0026rsquo;s Vertex AI service provides terse, generic errors with insufficient information to understand what the problem is. I found that there were often no server-side log messages to aid my debugging either. \u0026#x1f622;\nOverall: \u0026#x1f534;\nI was hoping for better compatability in Google\u0026rsquo;s services, given the popularity of the OpenAI APIs in all major model serving tools. The layer of Vertex AI appears to be creating more problems than it is solving here.\nVertex AI # The Vertex AI client library is the Google-published SDK for communicating with Google\u0026rsquo;s hosted Gemini models, and user-deployed models that are hosted on the Vertex AI platform.\nAs discussed in this prior article about Gemini, Vertex AI client libraries actually have 2 different pieces - I\u0026rsquo;ll be referring to them this way:\naiplatform: the google.cloud.aiplatform python package. This auto-generated library uses a resource-based approach to call the underlying API. vertexai: the vertexai python package, which is a handwritten SDK built on top of the aiplatform package, providing an improved developer experience but lacking some features. Vertex and Gemini # This is the flagship case for this library, and the one that vertexai was created for. The code is quite straightforward and requires minimal configuration.\nfrom vertexai.generative_models import GenerativeModel llm = GenerativeModel(\u0026#34;gemini-1.5-flash\u0026#34;) r = llm.generate_content(\u0026#34;tell me a fact about frogs\u0026#34;) print(r.text) Vertex and Vertex (Vertex Squared) # Here\u0026rsquo;s where things get awkward - the GenerativeModel classes that work with Gemini do not work for user-deployed models in Vertex AI. For these, we\u0026rsquo;ll need to use the aiplatform library.\nproject=\u0026#34;MY_PROJECT\u0026#34; location=\u0026#34;us-central1\u0026#34; # use `gcloud ai endpoints list --region us-central1` to see endpoint ids endpoint_id=\u0026#34;NNNNNNNN\u0026#34; import google.cloud.aiplatform_v1beta1 as aipb client = aipb.PredictionServiceClient(client_options={ \u0026#39;api_endpoint\u0026#39;: location + \u0026#34;-aiplatform.googlapis.com\u0026#34; }) endpoint = str.format(\u0026#34;projects/{project}/locations/{location}/endpoints/{endpoint}\u0026#34;, project=project, location=location, endpoint=endpoint_id) r = client.predict( endpoint=endpoint, instances=[{\u0026#39;inputs\u0026#39;: prompt}]) print(r.predictions) Certainly not as tidy as the Gemini version, but not too bad once you understand the use of Endpoint resources, and the PredictionClient.\nThe instances parameter is a bit tricky here, and varies based on how your model was deployed. The instances key ('inputs' above) must be set differently for models served by Huggingface\u0026rsquo;s TGI vs OpenAI-compatible model serving like vLLM. TGI uses \u0026lsquo;inputs\u0026rsquo;, but OpenAI-compatible servers will use \u0026lsquo;prompt\u0026rsquo;.\nThis choice is poorly exposed in the Model Garden, so you\u0026rsquo;ll likely need to inspect the Model (NOT the Endpoint, but the model behind it) using gcloud ai models describe $DEPLOYED_MODEL_NAME --region $REGION. The imageUri field in the output is the serving container your model is using.\nVertex and GKE # The Vertex AI client libraries are only useful for talking to Models hosted by Google\u0026rsquo;s Vertex AI service, so they\u0026rsquo;re not usable for models hosted in your own Kubernetes cluster. :sad:\nResults # Usability: \u0026#x1f7e0;\nvertexai deserves a \u0026#x1f7e2;, but the need to also understand aiplatform API surface downgrades this to orange.\nFlexibility: \u0026#x1f534;\nTwo different APIs for talking to google-hosted vs user-hosted models is awkward and would require a full rewrite to switch, or building one\u0026rsquo;s own abstraction layer.\nOverall: \u0026#x1f534;\nThese APIs appear to prioritize google\u0026rsquo;s own models, with little consideration for user-hosted use cases. If the vertexai experience worked for user-hosted models, this would be much better.\nLangchain # Langchain is a framework that lets the same client API be used to talk to multiple AI models on a collection of platforms. The goal of langchain is to allow for a similar developer experience across all leading AI models and platforms.\nFinding the correct model class object can be a bit tricky, but once you have that, the usage of those objects is consistent across the framework (as you\u0026rsquo;ll see below).\nLangchain and Gemini # from langchain_google_vertexai import ChatVertexAI llm = ChatVertexAI(model_name=\u0026#34;gemini-1.5-flash\u0026#34;) r = llm.invoke(\u0026#34;tell me a frog fact\u0026#34;) print(r.content) The above is an example of basic Gemini usage, and is a good place to start. More flexiblility can be achieved using Prompt templates and chains:\nfrom langchain_google_vertexai import ChatVertexAI from langchain_core.prompts import ChatPromptTemplate from langchain_core.messages import HumanMessage, AIMessage llm = ChatVertexAI(model_name=\u0026#34;gemini-1.5-flash\u0026#34;) pt = ChatPromptTemplate.from_messages([ (\u0026#34;system\u0026#34;, \u0026#34;you are a helpful assistant who likes amphibians\u0026#34;), (\u0026#34;human\u0026#34;, \u0026#34;{input}\u0026#34;) ]) chain = pt | llm r = chain.invoke(\u0026#34;tell me a frog fact\u0026#34;) print(r.content) This example shows the use of Chat Prompt Templates to add some system-level instruction for how the model should behave. The argument passed to invoke() is inserted in the Prompt Template placeholder {input} since there is only one. When using multiple placeholders, invoke() requires a mapping.\nPrompt templates can also be used to reformat input for Instruction-tuned models like Gemma.\nThe use of chains and prompt templates applies to all langchain examples, though for brevity I will only demonstrate it here.\nLangchain and Vertex AI # llm = VertexAIModelGarden(project=projectid, location = location, endpoint_id=endpointid, prompt_arg=\u0026#34;inputs\u0026#34;) r = llm.invoke(prompt) As with other Endpoint usage, the endpointid above is the integer identifier of the endpoint (not the name). IDs can be seen with the gcloud ai endpoints list command.\nChat Prompt template usage is indentical to the previous example.\nLangchain and GKE # The most challenging part of this was finding the proper LLM class to use for a \u0026ldquo;generic\u0026rdquo; LLM endpoint. Since I\u0026rsquo;m using TGI to serve my model, and I know that TGI is OpenAI-compatible, I used the OpenAI module with a custom base URL.\nThis example uses a localhost URL because I was using kubernetes port forwarding to access the service. Production use cases should use a different approach (like cluster-level DNS).\nfrom langchain_openai import ChatOpenAI from langchain_core.prompts import ChatPromptTemplate from langchain_core.messages import HumanMessage, AIMessage llm = ChatOpenAI( openai_api_key=\u0026#34;unused\u0026#34;, # this key is required. base_url=\u0026#34;http://localhost:8080/v1/\u0026#34; ) r = llm.invoke(\u0026#34;tell me a frog fact\u0026#34;) print(r.content) Results # Usability: \u0026#x1f7e0;\nThe advanced langchain concepts (e.g. Prompt Templates, chains) take some significant learning to understand. The errors from prompts and chains can be difficult to debug - things like \u0026ldquo;expected str\u0026rdquo;, but the stacktrace is deep in the langchain code, and its not clear how the user would fix it.\nFlexibility: \u0026#x1f7e2;\nLangchain delivers on the goal to keep the query experience pretty uniform across models and providers. There are still some bumps in the road around the exact shape of arguments to invoke(), especially with instruction-tuned models, but those are industry-wide issues, not specific to langchain.\nOverall: \u0026#x1f7e0;\nThe cryptic error stacktraces are the biggest contributor to an orange rating here. The cognitive load of learning about prompt templates and output chains are also a factor, though relatively minor.\nConclusions # As I went through these evaluations, I tried to separate commentary on the model hosting platform from the client library. Both aspects have an effect on the developer experience, and evaluating them separately was not always possible.\nAt this point, I would choose Langchain over other client libraries, as it provides the most insulation from the rapid change in the underlying technologies. The next time there is a major shift in AI technology, I would expect a fairly simple transition as a langchain user - other client libraries will likely have more work to do.\nIf I was planning to use providers that were comitted to OpenAI-compatability, the OpenAI client library would be a solid choice. Google\u0026rsquo;s compatability here is OK, but the debugging experience is pretty opaque.\nI hope this helps you pick the right client library for your AI-calling needs!\n","date":"15 August 2024","externalUrl":null,"permalink":"/2024/ai-client-libraries-and-their-eccentricities/","section":"Posts","summary":"\u003cp\u003eAs discussed earlier in this series, there are several factors that affect how\nclient libraries communicate with your AI Model service.\u003c/p\u003e\n\u003cp\u003eIn this article, I\u0026rsquo;ll be demonstrating using some of the popular client\nlibraries to connect to models served in 3 different ways, to demonstrate the\nusability and flexibility of these client libraries. For this comparison, I\u0026rsquo;ll\nbe using python since it is the most common choice in LLM user communities.\u003c/p\u003e","title":"AI Client libraries and their eccentricities","type":"posts"},{"content":"","date":"15 August 2024","externalUrl":null,"permalink":"/series/ai-productionization/","section":"Series","summary":"","title":"AI Productionization","type":"series"},{"content":"","date":"15 August 2024","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"As noted in the previous post about AI Model Serving layers, your serving layer determines which APIs are presented, and which client libraries you can use.\nIn this article, I\u0026rsquo;ll cover the most common AI APIs and where you are likely to encounter them.\nCommon APIs # These APIs are in widespread use for AI model serving. Most AI Model serving layers will support both of these APIs. Chat-style completions are increasingly popular, so if you are working with newer models, you should start there.\nOpenAI Chat Completions API # API Reference\nThis is the most common API, and is used by most recent models at the time of this writing.\nThis API uses a series of messages to model a chat. There are 3 common roles in the chat interface.\nuser: represents the messages sent by the human interacting with the AI ai: unsurprisingly, this role represents the AI\u0026rsquo;s response system: context and messages designed to steer the AI toward a particular type of response. These are intended for use as grounding and context for the later chat messages. As an example, consider the following set of messages:\n\u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant who likes frogs.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;tell me a fact about frogs\u0026#34; } ] This API neatly maps to the way AI is often used in a chat bot, and allows for multi-turn prompting and user followup.\nOpenAI Completions (Legacy) API # API Reference\nThis was OpenAI\u0026rsquo;s initial API, focused on taking a plain text prompt, and returning a set of possible responses.\nThis is considered Legacy by OpenAI, though many models may still provide this API. This API is good for one-shot prompts, but may be unwieldy for longer prompts or multi-turn prompting.\nHuggingFace\u0026rsquo;s Text Generation Inference (TGI) API is not fully compatible with Completions. Clients may require minor code adjustment when switching between these two interfaces.\nThe request body key for user input with TGI is inputs, while OpenAI\u0026rsquo;s interface uses prompt.\nRelated Complications # In addition to the APIs described above, there are a few additional factors to consider when deciding how to communicate with your model.\nGemma: Instruction Tuning # Docs\nGoogle\u0026rsquo;s Gemma models use a prompting syntax called \u0026lsquo;instruction tuning\u0026rsquo;, which uses angle-bracketed tags to denote the start and end of the \u0026ldquo;turn\u0026rdquo;. This is a method to indicate when the user is done and the model should respond. Visually, it has a similar look to the Chat interface describe above.\nThis style of prompt looks like this:\n\u0026lt;start_of_turn\u0026gt;user tell me facts about turtles.\u0026lt;end_of_turn\u0026gt; \u0026lt;start_of_turn\u0026gt;model So far, i\u0026rsquo;ve only seen this used in Google\u0026rsquo;s Gemma and Gemma2 models, usually with the explicit \u0026ldquo;-it\u0026rdquo; suffix when found on Huggingface\nInstruction Tuning is an expectation of the underling model, which still requires a Serving API. Instruction tuning is usually hosted behind a Completions API surface, where a single large prompt is expected.\nGoogle Vertex AI # Vertex AI can be used to host your own models, and provides an API that is consistent with the general \u0026ldquo;feel\u0026rdquo; of Google\u0026rsquo;s other APIs.\nA Vertex AI-hosted model still requires its own Model Serving Layer, so you can think of the Vertex AI API as an envelope, encapsulating requests for the underlying Model Serving API.\nIn fact, if you look closely at Vertex AI model objects, you can see the details of how they are passing requests to the Model Serving container underneath:\ngcloud ai models list --region us-central1 --format json The above command will show the containerSpec that describes how to run your chosen Model Serving Layer, as well as the url paths on that container to use for health checks (healthRoute) and content generation (predictRoute).\nRecap and Next Steps # The Chat and Completion APIs are used by the majority of AI model serving tools. Understanding these APIs will help you choose the right client for talking to your model.\nI\u0026rsquo;ll discuss some available clients and their configuration in the next installment of this series.\n","date":"6 August 2024","externalUrl":null,"permalink":"/2024/ai-model-apis/","section":"Posts","summary":"\u003cp\u003eAs noted in the previous post about \u003ca\n  href=\"https://www.marcdougherty.com/2024/ai-model-serving-layers/\"\n    target=\"_blank\"\n  \u003eAI Model Serving layers\u003c/a\u003e, your serving layer determines which APIs are\npresented, and which client libraries you can use.\u003c/p\u003e\n\u003cp\u003eIn this article, I\u0026rsquo;ll cover the most common AI APIs and where you are likely to\nencounter them.\u003c/p\u003e\n\n\u003ch2 class=\"relative group\"\u003eCommon APIs\n    \u003cdiv id=\"common-apis\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#common-apis\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eThese APIs are in widespread use for AI model serving. Most AI Model serving\nlayers will support both of these APIs. Chat-style completions are increasingly\npopular, so if you are working with newer models, you should start there.\u003c/p\u003e","title":"AI Model APIs","type":"posts"},{"content":"When it comes to AI, most of the discussion focuses on the model itself, but there\u0026rsquo;s an important decision that many organizations are overlooking - the choice of their AI model serving layer.\nYou can think of the model itself as a big bundle of data - many models are distributed as a compressed archive of files. To make the model useful, you need a way to query it - which is where model serving layers come in.\nThe following diagram illustrates some of the various parts involved in AI model serving:\nflowchart TD client[\"client\"] vertex[\"Vertex AI Endpoint\"] subgraph \"serving container\" subgraph \"serving layer (e.g. vllm/hexllm/TGI)\" api1[\"Chat API\"] api2[\"Completion API\"] end m[\"model\"] api1 -.-\u003e m api2 --\u003e m end client --\u003e vertex client -. (non-Vertex) .- api1 vertex --- api2 click api1 \"https://platform.openai.com/docs/api-reference/chat\" click api2 \"https://platform.openai.com/docs/api-reference/completions/create\" The dotted line request path, using the Chat API, is not currently available for models hosted on Google\u0026rsquo;s Vertex AI platform.\nWhile AI technology is still rapidly evolving, the APIs published by OpenAI are becoming broadly used by other model serving layers. Specifically the OpenAI completion and chat completion APIs - with chat completion being the preferred method.\nvLLM is a popular serving tookit that provides an OpenAI compatible server built into its vllm serve command, and as a Docker container.\n\u0026#x1f917; HuggingFace offers their Text Generation Inference API (TGI) as another competing API, with an open source API implementation that\u0026rsquo;s capable of serving most modern models. It supports chat-style interaction through its Messages API for OpenAI-compatible chat completion.\nThe rest of this article discusses the similarities and differences of vLLM and TGI. The topics discussed here would apply equally to any AI model serving layer.\nwhy is the model serving layer important? # The model serving layer determines how users can interact with your model, like what kinds of inputs are accepted and how they need to be presented. There are a bunch of different options for client libraries:\nOpenAI\u0026rsquo;s client libraries work on any OpenAI-compatible model server HuggingFace\u0026rsquo;s TGI client (hugginface-hub) work for any TGI-compatible server Some hosted models (like Google\u0026rsquo;s Gemini) have their own API and associated client library There are also frameworks like langchain that have support for many different LLM backends. Because this technology is evolving rapidly, I recommend choosing either the OpenAI client (which has emerged as industry standard) or one of the multi-API frameworks like langchain. This choice insulates you from changes in the underlying APIs, and allow you to move between models (and model hosting platforms!) with minimal updates to your codebase.\nvLLM has two different API server implementations. One is OpenAI compatible (vllm.entrypoints.openai.api_server, while the other is not (vllm.entrypoints.api_server).\nThe differences between the two are subtle so be careful!\nWhile the client interface aspect may seem straightforward, there are some other effects that are less apparent.\nTelemetry # Telemetry from the model serving layer determines how much visibility you have into the behavior of your model serving. Common telemetry signals include logs, monitoring metrics and distributed traces.\nBoth vLLM and TGI provide metrics and tracing with OpenTelemetry, which is the industry standard. The specific metrics and tracing data varies between the two.\nI recommend serving the same model with both and exploring available monitoring and trace data.\nLogging is less standardized than metrics and tracing, but usually falls into the broad categories of \u0026ldquo;structured\u0026rdquo; or \u0026ldquo;unstructured\u0026rdquo;. Structured logs are commonly JSON objects, which logging backends can parse to make logs highly searchable. Unstructured logs are treated as plain text, which may be more difficult to search for specific strings.\nWhen logging from a cloud provider, be mindful of how your hosting platform interprets your logs. As an example, Google\u0026rsquo;s Cloud Logging service attempts to parse structured logs, if they are in the correct format, to set metadata fields like log level (which it calls \u0026lsquo;severity\u0026rsquo;). Properly parsed log metadata makes it easier to find errors when your service is misbehaving.\nOperability and tuning # Most model serving layers provide comparable options for use of hardware GPU/TPUs, quantizers, LoRA adapters, and batching. While exact support for hardware and model serving may vary slightly between model servers, they are unlikely to be a factor in your model server choice.\nOne notable difference is the server-side protection configuration available in TGI. While most clients allow the user to set query parameters, TGI has the ability to set maximums for many of these parameters, like input tokens, top N, total input length, etc. These options are described in the TGI Options documentation\nThese options ensure consistent treatment of all clients, and can help avoid a \u0026ldquo;noisy neighbor\u0026rdquo; problem when a model service has many clients.\nMAX_CONCURRENT_REQUESTS is particularly useful, as it allows busy models to fail requests when overloaded - allowing the client to retry the request on another instance, rather than waiting a long time for their request to be processed by a busy instance.\nI hope to see similar options appear in other frameworks soon. In the meantime, I recommend choosing TGI as your model serving layer as it gives maximum control over how the model service behaves in production.\nAs an added bonus, TGI also includes a benchmark tool, so you can test out various settings before deploying them to production!\nRecap # Model serving is an important consideration, and can have strong effects on how your model behaves in production, and how well you can inspect the model\u0026rsquo;s behavior.\n","date":"1 August 2024","externalUrl":null,"permalink":"/2024/ai-model-serving-layers/","section":"Posts","summary":"\u003cp\u003eWhen it comes to AI, most of the discussion focuses on the model itself, but\nthere\u0026rsquo;s an important decision that many organizations are overlooking - the\nchoice of their AI model serving layer.\u003c/p\u003e\n\u003cp\u003eYou can think of the model itself as a big bundle of data - many models are\ndistributed as a compressed archive of files. To make the model useful, you need\na way to query it - which is where model serving layers come in.\u003c/p\u003e","title":"AI model serving layers","type":"posts"},{"content":"","date":"27 June 2024","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"27 June 2024","externalUrl":null,"permalink":"/tags/load-balancing/","section":"Tags","summary":"","title":"Load Balancing","type":"tags"},{"content":"To build a high-availability service in the cloud, we need to be able to serve from multiple independent failure domains. We can achieve this with multiple kubernetes clusters in different cloud regions, but we\u0026rsquo;ll need a load balancer that can route to all of our clusters.\nThis article will provide an overview of 3 possible options for load balancing between kubernetes clusters on Google Cloud. Because I\u0026rsquo;m looking at this through the lens of Platform Engineering, I\u0026rsquo;ll also be discussing the breakdown of responsibilities and controls between a Platform team and an Application team.\n\u0026ldquo;Plain\u0026rdquo; Load Balancing # The \u0026ldquo;plain\u0026rdquo; load balancing model is the most flexible, since it uses GCP Load Balancing primitives. You can use any of the (many!) available options on the load balancers, since you are creating them directly.\nThis strategy is essentially the same as my previous post called \u0026ldquo;Architecting for Drains\u0026rdquo;, where you create a Backend Service from the Network Endpoint Groups (NEGs) that Google Cloud creates for your Service in each of your clusters.\nI strongly recommend using an Infrastructure as Code tool like Terraform to manage your load balancers. This is especially helpful in this case, since finding the NEG information manually in the UI is tedious and prone to error.\nFirst, the Kubernetes Services in each cluster must be made publicly available. To achieve this, you create a Service and Ingress object like this:\napiVersion: v1 kind: Service metadata: name: my-service spec: type: LoadBalancer selector: app: my-service ports: - name: web port: 8080 targetPort: 8080 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-service spec: defaultService: kind: Service name: my-service Once these objects are created, Google Cloud will automatically create Regional Load balancers for each service in each cluster. Importantly, a Network Endpoint Group is created for the Service object, and the name of this NEG is stored in an annotation called cloud.google.com/neg-status. With this information about the NEG, we can now create our own load balancer, using the NEGs from each of our clusters, to form a Global service.\nCreating a Public, Global Application Load Balancer creates the following objects:\nA global forwarding rule - which directs external traffic to a Target Proxy. A Target Proxy - Target Proxies tie forwarding rules to URL Maps. Target Proxies come in different types by protocol: HTTP, HTTPS, GRPC, etc. A URL Map - uses various matching rules to direct incoming requests to the appropriate backend service. A Backend Service - contains one or more destinations that can serve incoming requests. The Backend Service describes how to connect and balance traffic for each destination (this is where our NEGs come in). A Health Check - ensures that each destination in your Backend Service is healthy and capable of serving requests. Destinations that fail health checks will not receive requests. The drain-demo repository includes an example of working terraform to create this global load balancer. Specifically, parsing the NEG information out of the annotations is covered here.\nAs for the division of responsibilities, a Platform team could own the Global load balancing through terraform, while each application team is responsible for creating the relevant Service and Ingress objects in their cluster.\nMulti-Cluster Ingress # Multi-Cluster Ingress (MCI) is Google\u0026rsquo;s first managed multi-cluster routing product. It uses kubernetes Custom Resource Definitions (CRDs) for configuration. Like Google\u0026rsquo;s other managed multi-cluster solution, MCI requires a GKE Enterprise subscription.\nThe two relevant CRDs for MCI are MultiClusterService and MultiClusterIngress. These resources must be deployed on the GKE-Enterprise config cluster, which acts as the source of truth\nMultiClusterService uses a label-based selector to find pods in your clusters:\napiVersion: networking.gke.io/v1 kind: MultiClusterService metadata: name: my-service￼ spec: template: spec: selector: app: my-app ports: - name: web protocol: TCP port: 8080￼ targetPort: 8080 Once you have at least one service defined, you can make a MultiClusterIngress to route requests to your service:\napiVersion: networking.gke.io/v1 kind: MultiClusterIngress metadata: name: NAME￼ namespace: NAMESPACE￼ spec: template: spec: backend: serviceName: my-service￼ servicePort: 8080￼ This example routes all requests to my-service, but MultiClusterIngress also supports various rules to route requests based on the Host header or URL path.\nBecause MCI is the \u0026ldquo;original\u0026rdquo; managed multi-cluster service, it supports only the \u0026ldquo;Legacy\u0026rdquo; type of Cloud Load Balancer.\nAs for the division of responsibility, MCI requires changes to kubernetes objects in the GKE-E Config Cluster. Because these objects form a global control plane, any changes to these objects will require involvement from the Platform / Infrastructure team.\nMulti-Cluster Gateway # Multi-Cluster Gateway (MCG) is Google Cloud\u0026rsquo;s implementation of the Kubernetes Gateway API - a vendor-agnostic collection of kubernetes objects that can be used to configure incoming traffic. While the Gateway API (and Google\u0026rsquo;s implementation) can be used to make per-cluster gateways, we\u0026rsquo;ll be focusing on the multi-cluster use case.\nThe Gateway API consists of the following components:\nGatewayClass decides how different classes of Gateway are created and managed. You can think of this as a way to declare which Controller is responsible for each Class of gateway. These are predefined by Google Cloud, so it is just a matter of picking the option that best suits your needs. Gateway represents the load balancer itself. Routing rules are attached to the Gateway with protocol-specific objects like HTTPRoute. kind: Gateway apiVersion: gateway.networking.k8s.io/v1beta1 metadata: name: external-http namespace: default spec: gatewayClassName: gke-l7-global-external-managed-mc listeners: - name: http protocol: HTTP port: 80 ServiceExport objects indicate that the named Service object should be made available to other clusters. The GKE Gateway controller will create the corresponding ServiceImport objects in all clusters, which are used in HTTPRoute objects. kind:ServiceExport apiVersion: net.gke.io/v1 metadata: name: my-service namespace: default HTTPRoute objects contain rules for directing HTTP traffic to a backend service. In our multi-cluster routing setup, these will be ServiceImport references, to indicate that this service is present in multiple clusters. kind: HTTPRoute apiVersion: gateway.networking.k8s.io/v1beta1 metadata: name: example-route namespace: default spec: parentRefs: - kind: Gateway namespace: default name: external-http hostnames: - \u0026#34;example.com\u0026#34; rules: - backendRefs: - group: net.gke.io kind: ServiceImport name: my-service port: 8080 The Google Cloud tutorial for a Multi-cluster, multi-region external gateway provides sample objects of all of these types, and is a good illustration of how to use them.\nThe Kubernetes Gateway group considered the separation of responsibility as a core part of their API, and it shows. One way to split this would be to have a Platform team that owns all Gateway and HTTPRoute objects, while application teams are responsible for their ServiceExport objects. The API is flexible enough to allow for some application teams to own their own HTTPRoutes, or even their own Gateways as desired.\nConclusions # Multi-cluster load balancing is a complex topic, and we\u0026rsquo;ve barely scratched the surface with an overview of 3 approaches.\n\u0026ldquo;Plain\u0026rdquo; load balancing (perhaps better called \u0026ldquo;composite load balancing\u0026rdquo;?) reuses per-cluster Network Endpoint Groups in the creation of a global load balancer. It provides flexibility but requires considerable management to set up. Multi-cluster Ingress, which is specific to Google Cloud, and has poor support for advanced load balancing features. Multi-cluster Gateway, which is an implementation of a Kubernetes API, and supports most available load balancing features. If you need an even greater degree of control over your traffic, you may also want to consider using a Service Mesh. Service Mesh provides a great deal more than just load balancing, but for environments that want a more robust set of controls on inter-service communication, it may be the right choice.\n","date":"27 June 2024","externalUrl":null,"permalink":"/2024/multi-cluster-load-balancing-with-google-cloud/","section":"Posts","summary":"\u003cp\u003eTo build a high-availability service in the cloud, we need to be able to serve from multiple independent failure domains. We can achieve this with multiple kubernetes clusters in different cloud regions, but we\u0026rsquo;ll need a load balancer that can route to all of our clusters.\u003c/p\u003e\n\u003cp\u003eThis article will provide an overview of 3 possible options for load balancing between kubernetes clusters on Google Cloud. Because I\u0026rsquo;m looking at this through the lens of Platform Engineering, I\u0026rsquo;ll also be discussing the breakdown of responsibilities and controls between a Platform team and an Application team.\u003c/p\u003e","title":"Multi-cluster load balancing with Google Cloud","type":"posts"},{"content":"","date":"27 June 2024","externalUrl":null,"permalink":"/series/platform-thoughts/","section":"Series","summary":"","title":"Platform Thoughts","type":"series"},{"content":"","date":"27 June 2024","externalUrl":null,"permalink":"/tags/reliability/","section":"Tags","summary":"","title":"Reliability","type":"tags"},{"content":"","date":"27 June 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"I have read a lot of articles about Platform Engineering recently, and many of them talk about platforms as a completely new way for your developers to work. In fact, many of those articles are trying to sell you their platform!\nIn this series, I\u0026rsquo;ll be exploring how you can create a platform starting from the tools and processes that your team is already using.\nI\u0026rsquo;ll be writing with a focus on software development teams working in a Cloud environment, but most of this applies to other environments and teams, though some interpretation may be necessary.\nThis article will discuss some of key features of a Platform that I believe are important, based on my years of experience as a Google SRE, and a contributor to the Reliable App Platforms repository (RAP) which illustrates what a minimum viable platform might look like.\nEach of these features has some key benefits that help your organization be more efficient, and align with the goals of Platform Engineering. These features can be implemented in many different ways, so I\u0026rsquo;ll be discussing them in abstract. Possible implementations are mentioned briefly in each section.\nLibrary of reusable components # Modern cloud infrastructure has an absolutely massive number of configuration options. While these options allow the product to serve a variety of use cases, it can be overwhelming for each developer team to understand how to achieve their specific goal.\nCreating a library of reusable components allows us to capture successful patterns and promote their use. These components should model a specific use case like \u0026ldquo;global http load balancer\u0026rdquo; or \u0026ldquo;replicated sql database\u0026rdquo;. For example, we might have a component that models a set of replicated multi-region databases, given nothing more than a name and a list of regions. This component creates an abstraction layer that encodes the configuration of a common pattern, which eases cognitive load for the developer teams and keeps the organization\u0026rsquo;s database configurations more consistent!\nReusable components also allow us to adapt to changes in cloud products and/or organization policy. For example, we may decide that any multi-region database should also have weekly backups. We can configure the backups once, in the component definition, and apply it to all the affected databases.\nIt is important to note some situations where I do not recommend a reusable component. Some teams have drastically different needs that are not common in the organization - for such teams it is unlikely that a component will be reused and is probably not worth the investment. I also recommend avoiding components that are too general, and can serve different needs (for example, a generic \u0026ldquo;database\u0026rdquo; component) - such components often require exposing so much of the underlying configuration that they do not lower the cognitive load on developer teams.\nImplementations In the RAP repo, we chose Terraform to model our components, and created components for \u0026ldquo;global http frontend to kubernetes services\u0026rdquo; and \u0026ldquo;CI/CD pipeline in CloudBuild using Github webhooks\u0026rdquo;. Our applications consume these components as Terraform modules, so any changes to the components will be reflected in the next application rollout.\nService Catalog # RAP\u0026rsquo;s use of Terraform allows applications to pick up changes made to our reusable components - but what if there\u0026rsquo;s an urgent change that our Platform team needs to push out? How would we find all affected services? That\u0026rsquo;s where a Service Catalog comes in.\nA service catalog keeps track of metadata about applications and services in your organization. This usually includes the location of the source code, and the teams responsible for the service. It may also include related links, like an on-call emergency contact, a service health dashboard, or a place to file bugs about the service.\nWith a Service Catalog, the platform team can \u0026ldquo;push\u0026rdquo; reusable component updates out to any service that uses that component. The details of discovering which services are using a component will depend on the implementation of both the service catalog and the component library.\nImplementations For small organizations, their Service Catalog may be searching their Github Organization, or an internal set of documentation. Larger organizations may opt for something more automation-friendly like a YAML file. Organizations with many services may wish to use Backstage.io, which provides an API to query their Service Catalog.\nInfrastructure Catalog and Configuration # While a Service Catalog keeps track of services, we need a similar mechanism for reusable shared infrastructure. This may be part of an organization\u0026rsquo;s Service Catalog or Component Library, depending on the chosen implementation, but I feel it is worth its own discussion since the needs are slightly different.\nMany organizations have shared resources that multiple applications rely upon. Resources like this may include shared Kubernetes clusters, the locations of critical data, or shared frontend load balancers. This mechanism can also be used to share the location of secrets that are stored in a secret manager.\nPublishing this information in a machine-readable format will help teams adapt as your organization grows. Perhaps a major database moves to a different region - any team that runs a data-intensive workload will likely want to move as well to avoid an expensive cross-region network bill.\nImplementations As noted above, this feature can be implemented as part of the Component Library or Service Catalog. It could be as simple as a structured data file in a known location, or as complex as a custom API.\nRecap # These are some features of a platform that can improve the lives of both your platform team, as well as your app development teams.\nReusable components guide app developers toward \u0026ldquo;known good patterns\u0026rdquo;, and provide a centralized point of control for organizational best practices A Service Catalog ensures that the platform team knows about all their users and can avoid unexpected surprises when updating platform components. An Infrastructure Catalog ensures that shared infrastructure is discoverable, and allows the platform team to add/change infrastructure in a way that is clear and visible to app teams. What other platform features do you find valuable, or wish you had?\n","date":"17 June 2024","externalUrl":null,"permalink":"/2024/building-blocks-of-a-developer-platform/","section":"Posts","summary":"\u003cp\u003eI have read a lot of articles about Platform Engineering recently, and many of\nthem talk about platforms as a completely new way for your developers to work.\nIn fact, many of those articles are trying to sell you their platform!\u003c/p\u003e\n\u003cp\u003eIn this series, I\u0026rsquo;ll be exploring how you can create a platform starting from\nthe tools and processes that your team is already using.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ll be writing with a focus on software development teams working in a Cloud\nenvironment, but most of this applies to other environments and teams, though\nsome interpretation may be necessary.\u003c/p\u003e","title":"Building blocks of a Developer Platform","type":"posts"},{"content":"","date":"17 June 2024","externalUrl":null,"permalink":"/series/platform-engineering-thoughts/","section":"Series","summary":"","title":"Platform Engineering Thoughts","type":"series"},{"content":"","date":"22 May 2024","externalUrl":null,"permalink":"/tags/platform/","section":"Tags","summary":"","title":"Platform","type":"tags"},{"content":"","date":"22 May 2024","externalUrl":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"I was writing a Terraform module to create a Google Cloud Load Balancer with an arbitrary set of GKE services as backends. To achieve this, I needed to learn about 3 different methods of iteration that are supported in Terraform, when to use each of them. If you\u0026rsquo;d like to better understand the many flavors of iteration available in Terraform, this article can help!\nA bit of background # My goal with this terraform module is to turn a list of kubernetes_service objects into a google_compute_backend_service. This requires a few intermediate objects, and some json parsing in the terraform. If you\u0026rsquo;re curious about the outcome of this work, you can check out the Reliable App Platforms github repository.\nWhile I did eventually find the information I needed in the Terraform docs (each of which is linked below), I felt there was little comparison between the different types and when to use each of them, so I put together this brief introduction.\nThese three flavors of iteration all have their uses - choosing the right one depends on the circumstances. I have used all three of these methods, sometimes in the same piece of Terraform. Let\u0026rsquo;s get started!\nResources: for_each # If you\u0026rsquo;ve done much with Terraform before, you\u0026rsquo;ve probably encountered the for_each style of iteration. For_each is supported by all resources, and is a useful way to create a resource for each item in a list.\nFor example, to create kubernetes_service data objects from a list of kubernetes service names, you can do this:\ndata \u0026#34;kubernetes_service\u0026#34; \u0026#34;services\u0026#34; { for_each = toset([\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;]) metadata { name = each.value } } # you can now refer to these services individually: # data.kubernetes_service.services[\u0026#34;three\u0026#34;] This type of iteration is great if you need to turn a list into a set of resources (for example, make a set of Cloud Storage buckets from a list of names). The terraform docs cover the details of for_each.\nIf you\u0026rsquo;re not creating resources, you should consider one of the other methods, like a for expression.\nLists: for expressions and splat expressions # For Expressions are commonly used to filter items from a list, or change the shape of your data.\nFor example, suppose you have a list of regional Cloud Storage Buckets like this:\nbuckets = [ { name = \u0026#34;bucket1\u0026#34;, region = \u0026#34;US-EAST4\u0026#34; }, { name = \u0026#34;bucket2\u0026#34;, region = \u0026#34;EUROPE-WEST3\u0026#34; }, ] You would rather have this as a map indexed by region, so your application uses the closest regional bucket. This can be done like so:\nbuckets_by_region = { for b in buckets: b.region =\u0026gt; b } # buckets_by_region[\u0026#34;US-EAST4\u0026#34;].name == \u0026#34;bucket1\u0026#34; Or perhaps you want to select only the buckets located in Europe:\neuropean_buckets = [ for b in buckets: b if startswith(b.region, \u0026#34;EUROPE\u0026#34;) ] If you want the list of regions in which there is a bucket, you could use a For expression:\nregions_with_buckets = [ for b in buckets : b.region ] Or use the alternative syntax for this, which is called a Splat Expression:\nregions_with_buckets = buckets[*].region \u0026lsquo;For\u0026rsquo; and \u0026lsquo;Splat\u0026rsquo; expressions are great for data manipulation like filtering, and creating maps from an unordered list. They are also great for getting data in the right \u0026ldquo;shape\u0026rdquo; for one of the other iteration flavors.\nRepeated blocks: dynamic blocks # The last type of iteration is useful for tricky scenarios where you need to repeat an inner block. I came across this with the backend block of a google_compute_backend_service. I wanted to produce a section like this:\nresource \u0026#34;google_compute_backend_service\u0026#34; \u0026#34;my_service\u0026#34; { # other fields omitted for brevity backend { group = \u0026#34;group1\u0026#34; } backend { group = \u0026#34;group2\u0026#34; } } Because the repeated block here is not the resource itself, we cannot use a for_each to repeat it. To repeat an inner block, we need a new type of iteration, which Terraform calls dynamic blocks. These are a variant of for_each, but instead of creating resources, they create blocks.\nresource \u0026#34;google_compute_backend_service\u0026#34; \u0026#34;my_service\u0026#34; { dynamic \u0026#34;backend\u0026#34; { for_each = [\u0026#34;group1\u0026#34;, \u0026#34;group2\u0026#34; ] iterator = \u0026#34;thing\u0026#34; content { group = thing.value } } } The above will create one backend block for each item in var.backends. The iterator argument can be used to name the temporary object in each iteration.\nThis method is really helpful when you need to repeat a block inside a resource (rather than the resource itself).\nBonus: iteration helpers with terraform_data objects # While I was doing all this iteration, I tried to use local variables in a Terraform locals block. Unfortunately, local variables do not allow for_each expressions, so I needed another solution.\nIn the past, I\u0026rsquo;ve used null_resource for this, but as of Terraform 1.4, the terraform_data resource type is preferred.\nBecause I\u0026rsquo;m only using this as an iteration-safe locals block, I avoided the use of triggers_replace, and just used the input argument to hold my per-loop local variables.\nresource \u0026#34;terraform_data\u0026#34; \u0026#34;iter-helpers\u0026#34; { for_each = var.backends input = { svc_port = tostring(each.value.service_obj.spec.0.port.0.port) } } To use the terraform_data objects, you can reference the output attribute:\nresource \u0026#34;some_resource\u0026#34; \u0026#34;list_of_things\u0026#34; { # iterate through our list of helpers for_each = terraform_data.iter-helpers # and select the svc_port attribute port = each.value.output.svc_port } This gives me a \u0026ldquo;fake\u0026rdquo; resource that I can iterate over with any of the above iteration flavors.\nConclusion # I hope this quick introduction helped you understand the differences between for_each, for/splat expressions, and dynamic blocks, so you can choose the right flavor of iteration next time you\u0026rsquo;re working with terraform.\nIf you\u0026rsquo;re interested in Platform Engineering (sometimes with Terraform), you can follow me on Medium, or check out the Google Cloud Community publication for a broader range of topics.\n","date":"22 May 2024","externalUrl":null,"permalink":"/2024/three-flavors-of-terraform-iteration/","section":"Posts","summary":"\u003cp\u003eI was writing a Terraform module to create a Google Cloud Load Balancer with an\narbitrary set of GKE services as backends. To achieve this, I needed to learn\nabout 3 different methods of iteration that are supported in Terraform, when to\nuse each of them. If you\u0026rsquo;d like to better understand the many flavors of\niteration available in Terraform, this article can help!\u003c/p\u003e\n\n\u003ch2 class=\"relative group\"\u003eA bit of background\n    \u003cdiv id=\"a-bit-of-background\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#a-bit-of-background\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eMy goal with this terraform module is to turn a list of\n\u003ca\n  href=\"https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/service\"\n    target=\"_blank\"\n  \u003e\u003ccode\u003ekubernetes_service\u003c/code\u003e\u003c/a\u003e\nobjects into a\n\u003ca\n  href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_backend_service\"\n    target=\"_blank\"\n  \u003e\u003ccode\u003egoogle_compute_backend_service\u003c/code\u003e\u003c/a\u003e.\nThis requires a few intermediate objects, and some json parsing in the\nterraform. If you\u0026rsquo;re curious about the outcome of this work, you can check out\nthe \u003ca\n  href=\"github.com/googlecloudplatform/reliable-app-platforms\"\u003eReliable App\nPlatforms\u003c/a\u003e github\nrepository.\u003c/p\u003e","title":"Three flavors of Terraform iteration","type":"posts"},{"content":"Terraform and Kubernetes are both declarative systems, but there can be some rough edges when these two systems interact. Kubernetes - specifically Google Kubernetes Engine (GKE) makes extensive use of annotations to store additional information about GKE resources.\nI encountered this when working on an article about traffic drains, and you can see it for yourself on Service objects. By default, GKE clusters contain several services that show this - for example, you can inspect the annotations on the built-in default-http-backend service:\nkubectl get service -n kube-system default-http-backend -o yaml\nUnder metadata you\u0026rsquo;ll see a block like this:\nannotations: cloud.google.com/neg: \u0026#39;{\u0026#34;ingress\u0026#34;:true}\u0026#39; components.gke.io/component-name: l7-lb-controller-combined components.gke.io/component-version: 1.23.5-gke.0 components.gke.io/layer: addon While not usually an issue for built-in resources, annotations for Terraform-managed resources can create a couple different types of issue. Continuous Delivery systems may delete the unexpected annotations, causing the GKE infrastructure to re-create them, resulting in a lot of busywork for the automation \u0026#x1f916;.\nAnother type of annotation-related problem occurs with GKE Autopilot, which uses annotations to store scaling and load information. Constantly deleting this information reduces Autopilot\u0026rsquo;s ability to assess your long-term needs, and may result in suboptimal scaling behavior.\nPart 1: Ignoring annotations / labels # Terraform does have a couple of ways to ignore these troublesome annotations: one that works per-object and uses the lifecycle stanza, and the second works for all objects managed by the Kubernetes provider. The Kubernetes provider docs cover both solutions, but this article only covers the provider-level approach.\nThe ignore_annotations and ignore_labels options allow for the global ignoring of any annotation or label that matches one of the regular expressions provided. For example, to ignore all of the GKE Autopilot annotations, you could declare your provider like this:\nprovider \u0026#34;kubernetes\u0026#34; { ignore_annotations = [ \u0026#34;^autopilot\\\\.gke\\\\.io\\\\/.*\u0026#34;, ] } This works great when writing new terraform, and don\u0026rsquo;t have an existing terraform state file. If you add this to existing Terraform, you\u0026rsquo;ll see that it still reports diffs \u0026#x1f926;. This configuration prevents these labels from being part of the stored terraform state - if you already have offending annotations in your terraform state, you\u0026rsquo;ll need to continue to part 2.\nPart 2: Selective editing of TF state # If you\u0026rsquo;ve already got annotations in your terraform state, you\u0026rsquo;ll need to \u0026ldquo;forget\u0026rdquo; those resources, and re-import them with the ignores in place. This solution was initially shared on a TF issue about these ignore options.\nBefore getting started, make sure you have a backup of your current TF state file. If anything goes wrong, you can use this backup to recover.\nEnsure there are no active diffs besides the annotations, with terraform plan. List known objects from the terraform state: terraform state list to find the relevant resource names for deletion. Delete each resource from the state file with terraform state rm ${TF_RESOURCE} Re-import the resource with terraform import ${TF_RESOURCE} ${K8S_OBJECT} Finally, re-run terraform plan to verify that the resource imported correctly and does not show diffs for the ignored annotations. That\u0026rsquo;s it! You\u0026rsquo;ve successfully removed those pesky annotations from your Terraform state file! \u0026#x1f389;\nRecommended ignores # In addition to the autopilot example above, I\u0026rsquo;ve also run into problems with a few other Google-specific annotations. I\u0026rsquo;ll be configuring my GKE Kubernetes providers with the following set of ignores from now on:\nignore_annotations = [ \u0026#34;cloud\\\\.google\\\\.com\\\\/neg\u0026#34;, \u0026#34;cloud\\\\.google\\\\.com\\\\/neg-status\u0026#34;, \u0026#34;^autopilot\\\\.gke\\\\.io\\\\/.*\u0026#34;, ] Hopefully this will prevent any poor interactions between GKE and Terraform. \u0026#x1f91e;\n","date":"9 April 2024","externalUrl":null,"permalink":"/2024/terraform-and-gke-annotations/","section":"Posts","summary":"\u003cp\u003eTerraform and Kubernetes are both declarative systems, but there can be\nsome rough edges when these two systems interact. Kubernetes - specifically\nGoogle Kubernetes Engine (GKE) makes extensive use of annotations to store\nadditional information about GKE resources.\u003c/p\u003e\n\u003cp\u003eI encountered this when working on an \u003ca\n  href=\"https://www.marcdougherty.com/2024/architecting-for-traffic-drains/\"\n    target=\"_blank\"\n  \u003earticle about traffic drains\u003c/a\u003e, and\nyou can see it for yourself on Service objects. By default, GKE clusters\ncontain several services that show this - for example, you can inspect the\nannotations on the built-in \u003ccode\u003edefault-http-backend\u003c/code\u003e service:\u003c/p\u003e","title":"Terraform and GKE Annotations","type":"posts"},{"content":"Distributed systems are capable of fast change and adaptation, and highly tolerant of constrained failures. This is often achieved by building systems that can exclude failing components from the larger system, but this capability is not automatic. Many large systems use load balancers to \u0026ldquo;route around a problem\u0026rdquo; by removing failed components. This process is often called \u0026ldquo;draining\u0026rdquo;.\nDrains are a generic mitigation, which means you can use them even if you don\u0026rsquo;t understand the cause of the problem (yet)!\nBut to take advantage of drains, your services must be architected to support them. The details will vary depending on the service, but common requirements include:\nServing locations in separate failure domains Often achieved by using multiple zones/regions from your cloud provider, this ensures outages in one location do not affect others. Requests may be served from any available location If a whole region is unavailable, the requests may go to a neighboring region. Any data needed to serve the request should be present in multiple regions. A frontend load balancer with configurable backends To perform drains, we need to change the available backends in the load balancer. Most load balancers support this, but some managed load balancers may not allow you to customize the set of backends. Example Architecture # There are many ways to achieve a drainable service, and this article will use the following architecture.\nflowchart TB lb(Global Load Balancer) subgraph RegionX [\"Region X\"] subgraph Cl-A [\"GKE Cluster A\"] direction TB svcA(\"k8s Service\") --\u003e depA(\"k8s Deployment\") --\u003e podA(\"k8s Pod\") end end subgraph RegionY [\"Region Y\"] subgraph Cl-B [\"GKE Cluster B\"] direction TB svcB(\"k8s Service\") --\u003e depB(\"k8s Deployment\") --\u003e podB(\"k8s Pod\") end end lb --\u003e Cl-A lb --\u003e Cl-B Components:\nGlobal frontend load balancer 2 Regional GKE clusters The whereami example service from GoogleCloudPlatform/kubernetes-engine-samples (using google\u0026rsquo;s publicly available container) This example is modeled in Terraform in the drain-demo github repository in three steps. If you\u0026rsquo;re not yet familiar with Terraform, you can take a look at some of the gcp terraform tutorials.\nPrepare your clusters # The first step is to create the 2 regional GKE clusters to host our backend service. This is done as a separate step to prevent terraform errors when using the kubernetes terraform provider on non-existant clusters.\nTo create the clusters, run the following commands from the 00-setup-clusters directory:\nterraform init terraform apply --var project=${your_project_id} This will create 2 GKE clusters, called drain-demo-1-a and drain-demo-1-b. These names are important in subsequent steps.\nDeploy your workload # Next, we deploy our backend service to both clusters. We\u0026rsquo;re using terraform for this step as well, so these commands will look familiar. This time from the 01-deploy-workload directory:\nterraform init terraform apply --var project=${your_project_id} These steps create a kubernetes Deployment of our whereami service, as well as associated Service and Ingress objects.\nAt this point, we have two separate, independent deployments of our whereami service, one in each cluster.\nTODO: identify if we need the ingress objects.\nCreate your load balancer # The last piece of the puzzle is to create our load balancer. Load balancers in Google Cloud are made up of several related objects - the most interesting for our case is the Backend Service (terraform calls this a google_compute_backend_service).\nFor this example, we populate the Backend Service with the Network Endpoint Groups that are automatically created from the kubernetes Service objects. This information is available as annotations on the kubernetes objects. You can view these annotations directly with kubectl get service whereami - they are encoded as a json object under the key cloud.google.com/neg-status.\nThe provided terraform parses out the relevant information from the Service objects in both clusters, and populates a single Backend Service using both Network Endpoint Groups.\nTo deploy the load balancer, run the following commands from the 02-loadbalancer directory:\nterraform init terraform apply --var project=${your_project_id} We now have a global load balancer pointing to both of our independent deployments. The loadbalancer address can be found in the terraform output: terraform output loadbalancer_url.\nTo verify the expected behavior, you can use a web browser to view your loadbalancer. Repeated requests should show some results serviced from each of our GKE clusters.\nIf you have the curl and jq tools installed, you could also run a command like the following to show which cluster served each request:\nwhile true; do curl --silent ${loadbalancer_url} | jq .cluster_name ; sleep 0.2 ; done Performing a traffic drain # Our shiny new global load balancer is working great! Until late one night, when we get paged because the site is serving errors! \u0026#x26a0;\u0026#xfe0f; \u0026#x1f4df;\nA quick look at our monitoring dashboards show errors are only coming from Cluster A. We could spend our time investigating exactly what makes Cluster A different, but with complex distributed systems that can take a lot of investigation - meanwhile our users are getting errors. To restore service as quickly as possible, we can drain Cluster A, go back to sleep, and debug in the morning once we\u0026rsquo;ve had coffee \u0026#x2615;.\nTo perform a drain, we\u0026rsquo;ll need to edit the backend stanzas in our load balancer\u0026rsquo;s Backend Service object\n// NOTE: zero is not a valid max_rate. You must remove the block to drain. backend { group = data.google_compute_network_endpoint_group.neg-A.self_link balancing_mode = \u0026#34;RATE\u0026#34; max_rate_per_endpoint = 100 } backend { group = data.google_compute_network_endpoint_group.neg-B.self_link balancing_mode = \u0026#34;RATE\u0026#34; max_rate_per_endpoint = 100 } With these stanzas, we can control the balance of traffic between our clusters, and even drain all traffic by removing (or commenting out) the backend stanza for that cluster.\nTry commenting out the first block, and re-applying the loadbalancer terraform. Once the terraform apply has completed, you will see that cluster B is serving all incoming requests! \u0026#x1f389;\nConclusions # This example illustrated how traffic drains can be used to eliminate the user impact of a problem, without needing to solve the problem first.\nWhen used in a production incident, drains can quickly eliminate the user-facing impact of an incident, while preserving the misbehaving service for further investigation.\nThis example uses the Network Endpoint Groups (NEGs) that Google Cloud creates automatically for GKE Service objects, to route traffic to the correct kubernetes pods.\nHappy Draining!\n","date":"5 April 2024","externalUrl":null,"permalink":"/2024/architecting-for-traffic-drains/","section":"Posts","summary":"\u003cp\u003eDistributed systems are capable of fast change and adaptation, and highly\ntolerant of constrained failures.  This is often achieved by building systems\nthat can exclude failing components from the larger system, but this capability\nis not automatic. Many large systems use load balancers to \u0026ldquo;route around a\nproblem\u0026rdquo; by removing failed components. This process is often called \u0026ldquo;draining\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eDrains are a \u003ca\n  href=\"https://www.oreilly.com/content/generic-mitigations/\"\n    target=\"_blank\"\n  \u003egeneric\nmitigation\u003c/a\u003e, which means\nyou can use them even if you don\u0026rsquo;t understand the cause of the problem (yet)!\u003c/p\u003e","title":"Architecting for Traffic Drains","type":"posts"},{"content":"I get a bit nervous whenever I touch renovate.json files, because I did not know how to test the effects of my changes. Well, this week I spent some quality time with Renovate and while the results are not perfect, they\u0026rsquo;re a lot better than what I had before!\nIf you\u0026rsquo;re not familiar with Renovate, it is a convenient way to keep your dependencies up to date. It understands dependencies in many language ecosystems, and supports a variety of ways to run and configure Renovate.\nTL;DR # Testing is still a bit tedious, because Renovate has a lot of functionality, and the output does not conveniently sum up what renovate would do. But the information is there, you just have to know where to look.\nFirst, the command I\u0026rsquo;m using:\nLOG_LEVEL=debug \\ RENOVATE_CONFIG_FILE=.github/renovate.json \\ npx renovate --platform=local \\ --require-config=ignored LOG_LEVEL=debug ensures we get plenty of output about decisions renovate is making. we need this flag to see what packages would be updated. RENOVATE_CONFIG_FILE=... - specifies a local path to a renovate configuration to use for this run. note: if you omit --require-config=ignored, this config will be merged with the one from this repository, which can cause unexpected results. npx renovate - uses npx to run renovate. will prompt you to install renovate if not already present. --platform=local - Renovate\u0026rsquo;s local platform works on the current directory only, and does not require a remote git repo --require-config=ignored - tells renovate to ignore any configuration found in the repository. How I test # In a git checkout of a repository, check out a point in time where I know I expect some renovate changes - for example, the commit just before your latest renovate PR was merged.\nFrom here, I modify the renovate.json file to reflect my changes.\nTo ensure the config is well-formed, you can run npx -p renovate renovate-config-validator, which does some syntactic validation.\nThen I run renovate, and interpret the results\nLOG_LEVEL=debug RENOVATE_CONFIG_FILE=.github/renovate.json npx renovate --platform=local --require-config=ignored Interpreting renovate\u0026rsquo;s debug output # The output from renovate\u0026rsquo;s debug logs is verbose. There are a few things I look for in the logs.\n\u0026ldquo;flattened updates found\u0026rdquo; # This line summarizes the packages renovate has found updates for. This helps you ensure your datasources are configure correctly, and that updated packages are found. As of this writing, the line looks like this:\nDEBUG: 5 flattened updates found: opentelemetry-sdk, opentelemetry-instrumentation-flask, opentelemetry-instrumentation-jinja2, opentelemetry-instrumentation-requests (repository=local) This tells me that my datasources are configured correctly, because these are the updates I am expecting. The specific issue this helped me identify is that the opentelemetry-instrumentation packages were considered \u0026lsquo;unstable\u0026rsquo; because they were versioned as 0.41b0, which is pre-release by pep440 versioning rules. Adding a renovate packageRule with \u0026quot;ignoreUnstable\u0026quot;: false made the instrumentation packages appear in this list.\n\u0026ldquo;packageFiles with updates\u0026rdquo; # Following this line is a large json object containing information about the files renovate would edit, and the updates that were found. This object is large, so I\u0026rsquo;ve omitted most of it, and only included the bits I look for:\n\u0026#34;config\u0026#34;: { \u0026#34;pip-compile\u0026#34;: [ { \u0026#34;deps\u0026#34;: [ { \u0026#34;depName\u0026#34;: \u0026#34;opentelemetry-sdk\u0026#34;, \u0026#34;datasource\u0026#34;: \u0026#34;pypi\u0026#34;, \u0026#34;currentVersion\u0026#34;: \u0026#34;1.18.0\u0026#34;, \u0026#34;updates\u0026#34;: [ { \u0026#34;bucket\u0026#34;: \u0026#34;non-major\u0026#34;, \u0026#34;newVersion\u0026#34;: \u0026#34;1.20.0\u0026#34;, \u0026#34;releaseTimestamp\u0026#34;: \u0026#34;2023-09-04T19:01:22.000Z\u0026#34;, \u0026#34;updateType\u0026#34;: \u0026#34;minor\u0026#34;, \u0026#34;branchName\u0026#34;: \u0026#34;renovate/opentelemetry\u0026#34; } ], // more fields omitted. }, // More dependencies omitted ], \u0026#34;packageFile\u0026#34;: \u0026#34;requirements.in\u0026#34; } ] } packageFile: the file in the repository that declares the dependencies. This is the file renovate would edit. The rest of these fields will appear for each depencency renovate finds:\ndepName: what renovate calls this package. currentVersion: the version currently listed in the packageFile updates[0].newVersion: the version renovate will upgrade you to. updates[0].branchName: the branch that this change will go into. This helps you identify if your package grouping rules are working as expected. This is not the most concise summary of renovate\u0026rsquo;s intended changes, but it has certainly helped me feel more confident in making renovate.json changes.\nFor a more complete test, it is recommended that you fork an existing repo, and let renovate run on that fork. For me, this would have required downgrading some packages so that renovate would have updates to do. That seemed a little tedious for my current use case, but maybe there will be a part 2 :grinning_face_with_sweat:.\n","date":"21 September 2023","externalUrl":null,"permalink":"/2023/testing-changes-to-renovate-configs/","section":"Posts","summary":"\u003cp\u003eI get a bit nervous whenever I touch \u003ccode\u003erenovate.json\u003c/code\u003e files, because I did not\nknow how to test the effects of my changes. Well, this week I spent some quality\ntime with Renovate and while the results are not perfect, they\u0026rsquo;re a lot better\nthan what I had before!\u003c/p\u003e\n\u003cp\u003eIf you\u0026rsquo;re not familiar with \u003ca\n  href=\"https://docs.renovatebot.com/\"\n    target=\"_blank\"\n  \u003eRenovate\u003c/a\u003e, it is a\nconvenient way to keep your dependencies up to date. It understands dependencies\nin many language ecosystems, and supports a variety of ways to run and configure\nRenovate.\u003c/p\u003e","title":"Testing changes to Renovate configs","type":"posts"},{"content":"I recently spent some time with external application loadbalancers in GCP, and I found the data model pretty difficult to work with. What follows is an attempt to better explain these concepts, practicing some of the advice from Docs for Developers, which I\u0026rsquo;ve been reading.\nOverview # External Application Loadbalancers are represented in the API by a series of related configuration objects. There is no single \u0026ldquo;Loadbalancer\u0026rdquo; object, so it is important to ensure the relevant objects all reference each other.\nDescribed below are some of the objects involved in a Global External Application Loadbalancer, as I encountered them. The GCP docs describe many other types of LB configuration that may better fit your use case.\nURL Map - (:light_bulb:, \u0026#x1f4d6;) responsible for routing incoming HTTP requests to the correct backend based on host and url. Backend Service - (\u0026#x1f4d6;): tells the loadbalancer how to connect to your backend, but does not contain the list of backends! Network Endpoint Group (NEG) (\u0026#x1f4d6;: contains the list of backends. Note that there are several types, for different types of backends (e.g. zonal VMs, serverless, etc) Backend Bucket (\u0026#x1f4d6;): similar to a NEG, but used when serving from a Cloud Storage bucket. URL Map # URL Maps map incoming HTTP urls to Backend Services, through hostRules, pathMatchers and pathRules. They also contain the default backend service, to which requests will be send if they do not match any rules (or if no rules exist).\nHost Rules match only on the Hostname of incoming HTTP requests. They control which Path Matcher the request is sent to next.\nPath Matchers contain Path Rules, which map url \u0026ldquo;globs\u0026rdquo;, to a specific backend service.\nbackend service # Backend services contain a bunch of configuration for how the Load Balancer should connect to the service that is actually serving the request.\nThe backend service object contains a list of backends. the group field of a backed refers to either a Compute Instance Group (not discussed here), or a Network Endpoint Group. These references use urls that start with https://googleapis.com - these URLs can be used directly with gcloud commands, so there\u0026rsquo;s no need to parse them for their individual path components.\nnetwork endpoint group (NEG) # Important object with a terrible name \u0026#x1f926;\nThese list your actual backends. There is support for several different types, from fully-managed Serverless NEGs, to Internet NEGs (which are just host:port or ip:port endpoints).\nThe format varies depending on what kind of NEG you need, so be sure to check the Backend docs.\nBackend Bucket # Sort of a special kind of NEG, this serves static content from a Cloud Storage bucket. The \u0026ldquo;backend Bucket\u0026rdquo; object, like the Backend Service, contains meta-information about how to serve your content, and contains only a link to the actual gcs bucket (in the bucketName field).\nRecap # This probably deserves a diagram (with links to the relevant docs):\nclassDiagram direction LR class urlmap[\"URL Map\"]{ pathMatcher.defaultService pathmatcher.pathRules[].service } class backendservice { backends[].group } class NEG { various types } urlmap --\u003e backendservice backendservice --\u003e NEG link urlmap \"https://cloud.google.com/compute/docs/reference/rest/v1/urlMaps\" link backendservice \"https://cloud.google.com/compute/docs/reference/rest/v1/backendServices\" link NEG \"https://cloud.google.com/load-balancing/docs/negs/\" I hope this little tour through the jungle of LB products has been helpful. I hope I find this article again the next time i have to touch one of these!\n","date":"7 August 2023","externalUrl":null,"permalink":"/2023/understanding-gcps-loadbalancer-models/","section":"Posts","summary":"\u003cp\u003eI recently spent some time with \u003ca\n  href=\"https://cloud.google.com/load-balancing/docs/https\"\n    target=\"_blank\"\n  \u003eexternal application\nloadbalancers\u003c/a\u003e in GCP, and I\nfound the data model pretty difficult to work with. What follows is an attempt\nto better explain these concepts, practicing some of the advice from \u003ca\n  href=\"http://docsfordevelopers.com\"\n    target=\"_blank\"\n  \u003eDocs for\nDevelopers\u003c/a\u003e, which I\u0026rsquo;ve been reading.\u003c/p\u003e\n\n\u003ch1 class=\"relative group\"\u003eOverview\n    \u003cdiv id=\"overview\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#overview\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h1\u003e\n\u003cp\u003eExternal Application Loadbalancers are represented in the API by a series of\nrelated configuration objects. There is no single \u0026ldquo;Loadbalancer\u0026rdquo; object, so it\nis important to ensure the relevant objects all reference each other.\u003c/p\u003e","title":"Understanding GCP's Loadbalancer models","type":"posts"},{"content":"This post assumes you\u0026rsquo;re already familiar with OpenTelemetry, and are already collecting some observability data.\nWhether you\u0026rsquo;ve chosen automatic instrumentation, or manual, you\u0026rsquo;re now collecting telemetry data from your code. Congratulations \u0026#x1f389;\nBut what about all the other code you\u0026rsquo;re using? When your service makes a database query, or fetches weather data, you\u0026rsquo;re using someone else\u0026rsquo;s code. These other services may have their own production problems - can you separate issues in your code from issues in a dependency with your current observability signals?\nIf you\u0026rsquo;re not sure, read on! We\u0026rsquo;ll cover creating metrics and traces around your existing calls to other services over HTTP or GRPC. The samples below are in Go, but similar tactics should work in most languages.\nBefore you start instrumenting these calls yourself, consider searching the OTel Registry for existing instrumentation libraries. For example, Postgres database users could adopt the pgotel library, which will auto-magically provide instrumentation for existing go-pg code.\nWrapping HTTP clients # Most of the APIs you\u0026rsquo;re calling are likely HTTP-based. Some of these services may provide a client library, some users may choose to create their own client library, and still others will choose to use a simple HTTP client. No matter which category you\u0026rsquo;re in, this approach can help you get better telemetry (provided your language supports interfaces or something equivalent).\nLet\u0026rsquo;s assume you\u0026rsquo;re using a client library to fetch pictures of cats, called a CatClient. You can create an instrumented version of this library using Go\u0026rsquo;s embedding.\nTo begin, we\u0026rsquo;ll define a type for our OTelCatClient:\ntype OTelCatClient struct { CatClient } Now we\u0026rsquo;ll need to \u0026ldquo;wrap\u0026rdquo; the CatClient method calls to include our instrumentation. For a method like CatClient.GetRandomCat, we can add a trace span as described in the OTel guide to Manual Instrumentation:\nfunc (c *OTelCatClient) GetRandomCat(c context.Context) Cat { ctx, span := c.tracer.Start(c, \u0026#34;get-random-cat\u0026#34;) defer span.End() return c.CatClient.GetRandomCat(ctx) } The same can be done to add Metrics as desired, to track the number of calls, or errors.\nWe can now use OTelCatClient the same way we would use a regular CatClient, and the instrumented client will produce a trace span for any calls to GetRandomCat.\nIf you produce your own client libraries, you can add instrumentation directly to your libraries with the OpenTelemetry API. By default, OpenTelemetry libraries use a no-op implementation which has a minimal effect on performance and does not record any data. When the OpenTelemetry SDK is configured by the consumer of your client libraries, all your beautiful telemetry will be available, sent to the destination of their choosing.\nGRPC Interceptors # For calls made over GRPC (which includes most of Google\u0026rsquo;s Client Libraries), you can get telemetry by using one of the GRPC Interceptors provided by the otelgrpc instrumentation library.\nGRPC Interceptors provide \u0026ldquo;hooks\u0026rdquo; in the GRPC handling process, as a way to implement logging, authorization, and other types of \u0026ldquo;middleware\u0026rdquo; tasks. The Interceptor concept is present in all supported GRPC languages, though I find it is not well described. This guide to gRPC and Interceptors is a nice summary of the concept.\nTo make use of the interceptor, it must be plumbed down into the GRPC Dial() call as an option. If you\u0026rsquo;re creating GRPC connections yourself, this is straightforward. For Google API Clients, it looks a bit like this:\nimport ( iam \u0026#34;cloud.google.com/go/iam/apiv2\u0026#34; \u0026#34;go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc\u0026#34; \u0026#34;google.golang.org/api/option\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; ) policyclient, _ := iam.NewPoliciesClient(r.Context(), option.WithGRPCDialOption( grpc.WithUnaryInterceptor( otelgrpc.UnaryClientInterceptor()))) Note that for Streaming APIs, there\u0026rsquo;s also an otelgrpc.StreamingClientInterceptor.\nThis Policy Client will now record OpenTelemetry spans for each of its GRPC calls, and report them to whichever backend OTel has been configured to use. These spans include labels such as the method it called, and what the returned status code. With this telemetry at your fingertips, it becomes easier to identify when your dependent services are experiencing latency or instability.\nRecap # We\u0026rsquo;ve discussed a few ways to add instrumentation when calling another service via custom clients, or GRPC.\nIf you create your own libraries, you can add native instrumentation (so your customers get better telemetry!) using the OTel guide to Instrumenting libraries.\nHappy Instrumenting! \u0026#x1f52d;\n","date":"31 March 2023","externalUrl":null,"permalink":"/2023/day-2-observability-calls-to-other-services/","section":"Posts","summary":"\u003cp\u003eThis post assumes you\u0026rsquo;re already familiar with\n\u003ca\n  href=\"http://opentelemetry.io\"\n    target=\"_blank\"\n  \u003eOpenTelemetry\u003c/a\u003e, and are already collecting some\nobservability data.\u003c/p\u003e\n\u003cp\u003eWhether you\u0026rsquo;ve chosen automatic instrumentation, or manual, you\u0026rsquo;re now\ncollecting telemetry data from your code. Congratulations \u0026#x1f389;\u003c/p\u003e\n\u003cp\u003eBut what about all the \u003cem\u003eother\u003c/em\u003e code you\u0026rsquo;re using? When your service makes a\ndatabase query, or fetches weather data, you\u0026rsquo;re using someone else\u0026rsquo;s code. These\nother services may have their own production problems - can you separate issues\nin your code from issues in a dependency with your current observability\nsignals?\u003c/p\u003e","title":"Day 2 Observability - calls to other services","type":"posts"},{"content":"","date":"31 March 2023","externalUrl":null,"permalink":"/tags/o11y/","section":"Tags","summary":"","title":"O11y","type":"tags"},{"content":"","date":"31 March 2023","externalUrl":null,"permalink":"/tags/observability/","section":"Tags","summary":"","title":"Observability","type":"tags"},{"content":"","date":"24 February 2023","externalUrl":null,"permalink":"/tags/cli/","section":"Tags","summary":"","title":"Cli","type":"tags"},{"content":"","date":"24 February 2023","externalUrl":null,"permalink":"/tags/gh/","section":"Tags","summary":"","title":"Gh","type":"tags"},{"content":"Last year, I started working in a number of public Github repositories, and learned to use the gh Github CLI. I wrote an article about using the github cli with multiple repos, but given how much my workflow has changed, I think its time for an update.\nPicking a random teammate # Sometimes, I need to pick a human to be responsible for something (usually a PR review). When there is no obvious choice (for example, someone who already knows the context of the PR), I caught myself relying on the same team members repeatedly. While this is not a big issue, I wanted a more \u0026ldquo;fair\u0026rdquo; way to pick a random assignee.\nTo do that, I first needed a quick way to list the members in a github team. Teams are typically referred to with a @my-org-name/my-team-name syntax - but the API call to list team members needs the individual parts, rather than the whole string. Bash string manipulation to the rescue! The following alias uses the %% and ## operators to remove the substring from the back or front of a string, respectively. This allows us to get either the org name or team name from the full string. We also use jq to print just the login of each member.\nThe alias should be added to the aliases section of your gh config file. On linux machines, that is ~/.config/gh/config.yml\nmembers: \u0026gt; ! gh api orgs/${1%%/*}/teams/${1##*/}/members | jq -r \u0026#34;.[].login\u0026#34; With this alias, I can run gh members my-org/my-team | shuf -n 1 to pick a random member of this team.\nCoping with long-running status checks # Some PRs run exhaustive tests as PR Checks, and these can take a while. As either an author or reviewer, I want to know when the Checks are done, so I can properly review the change.\nTo achieve this, I use the following gh alias:\n# pop up a notification when the checks are complete for a given PR. lmk: \u0026gt; ! ( gh pr checks $1 --watch \u0026gt; /dev/null ; notify-send \u0026#34;GH PR Checks done\u0026#34; \\ $(gh pr view $1 --json url --jq \u0026#34;.url\u0026#34;) ) \u0026amp; For the Mac users, you\u0026rsquo;ll need to replace the notify-send command with something like the following, after installing terminal-notifier:\nterminal-notifier -title \u0026#34;PR Ready: $1\u0026#34; -message \u0026#34;PR checks done\u0026#34; \\ -contentImage https://github.githubassets.com/images/modules/logos_page/Octocat.png \\ -open $(gh pr view $1 --json url --jq \u0026#34;.url\u0026#34;) In either case, this will produce a little desktop notification when the PR checks are completed. To use, just give it a PR number, branch, or full URL (just like with gh pr view).\nMaintainer / Reviewer SLOs # One of the responsibilities of a repository maintainer (or reviewer) is ensuring contributors get timely follow-up on their contributions. I have a few different reviewer roles, and each of them has different review expectations. To help me keep them straight, I\u0026rsquo;ve written a small gh extension:\nmuncus/gh-slocheck Go 0 0 In brief, the extension allows me to search open PRs, and sorts them with the oldest ones first, highlighting any that are older than a specified age. Most of this can be done without an extension command, but I wanted to also include status indicators for Status Checks, Review status, and Mergeability. Github provides this information through the API, and the tool just wraps them up in convenient output.\nWith this tool, I can now define aliases for reviews assigned to me, and also for each of my reviewer and maintainer roles.\nReviews I\u0026rsquo;m actively involved in: gh slocheck -s \u0026quot;involves:@me is:open review:required\u0026quot; --limit 20 Reviews where I\u0026rsquo;m explicitly requested: gh slocheck -s \u0026quot;user-review-requested:@me is:open\u0026quot; Reviews that are looking for a Golang-samples reviewer: gh slocheck -s \u0026quot;team-review-requested:googlecloudplatform/go-samples-reviewers is:open draft:false status:success\u0026quot; --limit 20 -w 36h This explicitly checks that the PR is ready for review, with passing status checks, and non-draft status. It also has a shorter warning period than the rest, since this repo has more strict review expectations. I like to prefix all these aliases with slo:, so I can see everything that needs my input with a few short commands.\nBenefits # This gives me a full view of my github review responsibilities with a few short commands, and helps me stay on top of PRs that need my attention.\nMoving to a gh-extension simplifies my aliases considerably, and gives me more flexibility in the output format and sort order.\n","date":"24 February 2023","externalUrl":null,"permalink":"/2023/more-github-cli-tips/","section":"Posts","summary":"\u003cp\u003eLast year, I started working in a number of public Github repositories, and\nlearned to use the \u003ca\n  href=\"//cli.github.com\"\u003e\u003ccode\u003egh\u003c/code\u003e Github CLI\u003c/a\u003e. I wrote an article about\n\u003ca\n  href=\"https://dev.to/muncus/using-the-github-cli-with-multiple-repos-38k\"\n    target=\"_blank\"\n  \u003eusing the github cli with multiple\nrepos\u003c/a\u003e, but\ngiven how much my workflow has changed, I think its time for an update.\u003c/p\u003e\n\n\u003ch3 class=\"relative group\"\u003ePicking a random teammate\n    \u003cdiv id=\"picking-a-random-teammate\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#picking-a-random-teammate\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eSometimes, I need to pick a human to be responsible for something (usually\na PR review). When there is no obvious choice (for example, someone who already\nknows the context of the PR), I caught myself relying on the same team members\nrepeatedly. While this is not a big issue, I wanted a more \u0026ldquo;fair\u0026rdquo; way to pick a\nrandom assignee.\u003c/p\u003e","title":"More Github CLI Tips","type":"posts"},{"content":"(This content was originally published at https://dev.to/muncus/handy-yaml-tricks-415p)\nIn the past few years, YAML (http://yaml.org) has become an essential part of software, particularly for infrastructure-as-code tools. Yaml at the heart of kubernetes configuration, kubernetes-inspired APIs like Google\u0026rsquo;s config connector, and a number of workflow systems like Google Cloud Workflows and Github Actions.\nIn its simplest forms, Yaml is quite human-readable, but over time many of these configurations become more complex, and the documentation of these formats is not always as complete or searchable as we might like. Included below are some small tips that may help you make the most of your Yaml configurations.\nIDE assistance with JSON Schemas! # Do you struggle to remember the names of fields in your Yaml objects? I sure do!\nMost modern editors and IDEs support the Language Server Protocol, which powers the code completion, validation, and tooltip features. Combined with a Yaml Language Server, we can get rich completion for Yaml files!\nInstallation varies depending on the editor, but I found installation with VSCode to be pretty straightforward.\nFor many common yaml files, the correct schema can be inferred from the file name, and looked up automatically with SchemaStore. SchemaStore hosts a wide collection of JSON Schemas, which can be used to validate Yaml. SchemaStore is backed by the SchemaStore Github Repository, and contributions of additional JSON Schemas are welcome!\nSchema detection can also be configured manually in your editor if auto-detection is inaccurate.\nSetting up the Yaml Language Server takes a bit of work, but if you regularly work with complex Yaml objects, you\u0026rsquo;ll be glad you took the time to set it up!\nBetter Multiline Strings with \u0026gt; and | # Sometimes, with workflow configuration like cloudbuild.yaml and Github Actions, we write long commandlines that are not very readable in an IDE.\nThe Yaml spec has what they call literal style (|) and folded style (\u0026gt;) to help with this.\nLiteral Style preserves newlines in the string, so it can be used to run multiple commands as a single step. For example, installing python dependencies and running the relevant tests can be done like this:\n- name: Run tests run: | pip install -r requirements.txt pytest -v . Note: In some situations, this may mask failures in the earlier commands if the last command exits successfully.\nFolding Style \u0026ldquo;condenses\u0026rdquo; whitespace in the string, replacing spaces and/or newlines with a single space as describe in the spec section on line folding. In short, it lets us insert newlines in a string to make it more readable.\nConsider this very long line from a github actions workflow that calls the github API with curl:\n- name: Get current repo settings run: curl -o repository.json https://api.github.com/repos/${{ github.repository}} -H \u0026#34;Authorization: Bearer ${{ github.token }}\u0026#34; Rewritten with Folding Style, it looks like this instead:\n- name: Get current repo settings run: \u0026gt; curl -o repository.json https://api.github.com/repos/${{ github.repository}} -H \u0026#34;Authorization: Bearer ${{ github.token }}\u0026#34; For long lines, this can make them easier to read, and easier to edit and review.\nThese tips have helped me write more maintainable Yaml files, and I hope they help you, too!\n","date":"8 February 2023","externalUrl":null,"permalink":"/2023/handy-yaml-tricks/","section":"Posts","summary":"\u003cp\u003e(This content was originally published at \u003ca\n  href=\"https://dev.to/muncus/handy-yaml-tricks-415p\"\n    target=\"_blank\"\n  \u003ehttps://dev.to/muncus/handy-yaml-tricks-415p\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eIn the past few years, YAML (\u003ca\n  href=\"http://yaml.org\"\n    target=\"_blank\"\n  \u003ehttp://yaml.org\u003c/a\u003e) has become an essential part of software, particularly for infrastructure-as-code tools. Yaml at the heart of \u003ca\n  href=\"http://kubernetes.io\"\n    target=\"_blank\"\n  \u003ekubernetes\u003c/a\u003e configuration, kubernetes-inspired APIs like \u003ca\n  href=\"https://cloud.google.com/config-connector/docs/concepts/resources\"\n    target=\"_blank\"\n  \u003eGoogle\u0026rsquo;s config connector\u003c/a\u003e, and a number of workflow systems like \u003ca\n  href=\"https://cloud.google.com/workflows\"\n    target=\"_blank\"\n  \u003eGoogle Cloud Workflows\u003c/a\u003e and \u003ca\n  href=\"http://github.com/features/actions\"\n    target=\"_blank\"\n  \u003eGithub Actions\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn its simplest forms, Yaml is quite human-readable, but over time many of these configurations become more complex, and the documentation of these formats is not always as complete or searchable as we might like. Included below are some small tips that may help you make the most of your Yaml configurations.\u003c/p\u003e","title":"Handy Yaml Tricks!","type":"posts"},{"content":"","date":"8 February 2023","externalUrl":null,"permalink":"/tags/yaml/","section":"Tags","summary":"","title":"Yaml","type":"tags"},{"content":"","date":"17 July 2018","externalUrl":null,"permalink":"/archive/","section":"Archives","summary":"","title":"Archives","type":"archive"},{"content":"I\u0026rsquo;ve been a debian/ubuntu user for years, but had always been intimidated by the packaging process (because i tried to read the New Maintainer\u0026rsquo;s Guide). I decided to give it a shot recently, because I was building experimental software for the raspberry pi, and grew tired for scp and rsync.\nNote: The advice below does not abide by the debian packaging policy.\nVincent Bernat\u0026rsquo;s Pragmatic Debian Packaging is a great place to get started. Unfortunately, I only found it near the end of my efforts.\nSo\u0026hellip; why? # I had been tinkering with several different raspberry-pi-based projects, and I found copying around whole directories from my computer (where I do most of the coding) to the pi was becoming tedious. I was also looking for a clean and simple way to cleanly remove a project if I later decided it was no longer needed.\nI had looked into a few other \u0026lsquo;installation methods\u0026rsquo;, like shell scripts, and tarballs, etc. But none of them seemed to be quite simple enough to set up.\n\u0026ldquo;Simple?\u0026rdquo; you say, \u0026ldquo;you picked debian packages because you wanted simple?!\u0026rdquo;\nWell, no. It was not simple, but it does teach me more about debian packages, which is sure to be beneficial down the line.\nBefore we start # There is a great deal of tooling built around debian packaging, but if we start with the robots, there will be cleanup work to do later.\nset the environment variables DEBEMAIL to your email address, and DEBFULLNAME to your full (first + last) name. These are used in the autogenerated steps to come, and its easier to set them now than to fix the generated output.\n(Optional) put the following in ~/.devscripts, which is read by many debian tools:\n# Do not require directories to conform to packagename-version standard. DEVSCRIPTS_CHECK_DIRNAME_LEVEL=0 # dont rename the directory when the version number changes. DEBCHANGE_PRESERVE=no # Dont sign packages. I just build them for me, so signing is pointless. DEBUILD_DPKG_BUILDPACKAGE_OPTS=\u0026#34;-uc -us\u0026#34; Generate boilerplate: dh_make # I started by creating a basic debian package with dh_make --native. This gives me a \u0026ldquo;native\u0026rdquo; debian package, meaning that there is no separate source tarball to worry about. For my use case, i felt this was best, as it avoids the intermediate step of tarring up my files. This approach can be found in an appendix of the debian maintainer docs\ndh_make --native --packagename=$pkgname_$version\nWhere $pkgname is the name of the package you\u0026rsquo;re building, and $version is some arbitrary version number.\nSee docs on source/format for more information about native vs non-native packages.\nThis command will generate a debian/ directory, with lots of files in it. Anything that ends in .ex is an example, and can safely be deleted if you wish.\nCustomize the boilerplate for your needs # The most important two files generated are debian/control and debian/rules.\ndebian/control # The control file is metadata about your package. there are several placeholders in there, like Description and Section which should be filled in.\nBuild-Deps is a bit harder, and should include packages needed to build the package. Depends should list packages required for your package to function. I often guess at the right packages, and test by running a package build.\ndebian/rules # This is actually a Makefile, with rules for building your package. Thanks to debhelper (dh and friends), this file is typically short in my experience. There\u0026rsquo;s a variety of helper utils for specific purposes (for example, dh-golang helps build packages written in Go, and is discussed below).\nTrials and Errors: debuild # debuild is the main tool for building packages. now that we\u0026rsquo;ve customized some of the debian build files, its time to give it a shot. It probably wont work, but it will tell us what needs to be fixed. The lintian tool makes helpful suggestions that are probably worth fixing, and points out placeholders that need to be filled in.\nThe build process assumes that there\u0026rsquo;s a toplevel Makefile, though if you are packaging a simple collection of files, this is not necessary. See the section below on Install and Configuration.\nEventually, you should end up with a functional debian package in the parent directory of your package. If that\u0026rsquo;s all you needed, you\u0026rsquo;re done!\nHelpers, and other considerations # Install and Conffile # For packages that do not require compilation, or are platform-agnostic (e.g. python scripts), a Makefile feels a bit overwhelming. There\u0026rsquo;s a simpler approach: debian/install. this file describes how files from the package build directory should be installed by the package.\nFor example, a directory with files a, b, and c might choose to install into different directories like this:\na /usr/bin/ b /usr/sbin/ c /etc/ That file c looks like a configuration file, and we dont want to overwrite with the package version when we upgrade. debian/conffiles provides a mechanism for this. Just include the path to the installed configuration file (i.e. etc/c) in here, and dpkg will treat it as a configuration file.\nHelper: dh-golang # The debian go team has a golang packaging guide for full details, but here\u0026rsquo;s the short version:\ninstall dh-golang, which is a deb helper for making go packages. in debian/rules set DH_GOPKG to the name of your go package (what you\u0026rsquo;d usually go get) in debian/rules, add the following options to the dh invocation: --buildsystem=golang --with=golang run debuild again, and hopefully get a go package! For binary-only packages, set DH_GOLANG_BUILDPKG in rules, pointing to only the packages that are binaries (which are all under cmd, right?)\nHelper: dh-systemd # If you are creating services to run at startup, say on a raspberry pi, you\u0026rsquo;ll likely want to include some systemd configuration. There are some docs for dh-systemd.\nFor debhelper versions \u0026gt;= 10, systemd is activated by default, according to the above.\nadd a systemd file as debian/packagename.systemd, and it will be included in the deb package.\n","date":"17 July 2018","externalUrl":null,"permalink":"/archive/debian-packaging/","section":"Archives","summary":"\u003cp\u003eI\u0026rsquo;ve been a debian/ubuntu user for \u003cstrong\u003eyears\u003c/strong\u003e, but had always been intimidated by\nthe packaging process (because i tried to read the \u003ca\n  href=\"https://www.debian.org/doc/manuals/maint-guide/\"\n    target=\"_blank\"\n  \u003eNew Maintainer\u0026rsquo;s\nGuide\u003c/a\u003e).\nI decided to give it a shot recently, because I was building experimental\nsoftware for the raspberry pi, and grew tired for \u003ccode\u003escp\u003c/code\u003e and \u003ccode\u003ersync\u003c/code\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: The advice below does not abide by the debian packaging policy.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eVincent Bernat\u0026rsquo;s \u003ca\n  href=\"https://vincent.bernat.im/en/blog/2016-pragmatic-debian-packaging\"\n    target=\"_blank\"\n  \u003ePragmatic Debian\nPackaging\u003c/a\u003e\nis a great place to get started. Unfortunately, I only found it near the end of\nmy efforts.\u003c/p\u003e","title":"Building Debian packages for my own use.","type":"archive"},{"content":" TL;DR - I made a vim compiler plugin for arduino that parses errors into the quickfix list, and learned a bunch about the quickfix list and compiler plugins.\nI\u0026rsquo;ve been working on several small arduino projects lately (mostly the series of bluetooth projects posted earlier). While I find the arduino toolchain easy to use, I\u0026rsquo;ve never enjoyed using the editor, or having to use the mouse to click the Verify and Upload buttons.\nExisting vim-arduino integration # A bit of googling brought to my attention that the arduino tool can also be used as a commandline tool as of version 1.15, and has a man page. Much of the desired vim integration exists already, in the github/stevearc/vim-arduino plugin, but the one thing I was still missing was the quickfix list of errors.\nA small discussion of vim variables # This sent me down the rabbithole of reading vim\u0026rsquo;s help about quickfix, and several other related topics (setqflist, QuickfixCmdPost, errorformat, efm-entries, etc). The first lesson is one of maintainability, as there are many errorformat strings on github, but most of them are just there as one long string, with commas, like the default one in this vim session:\nerrorformat=%*[^\u0026#34;]\u0026#34;%f\u0026#34;%*\\D%l: %m,\u0026#34;%f\u0026#34;%*\\D%l: %m,%-G%f:%l: (Each undeclared identifier is reported only once,%-G%f:%l: for each function it appears in.),%-GIn file included from %f:%l:%c:,%-GIn file included from %f:%l:%c\\,,%-GIn file included from %f:%l:%c,%-GIn file included from %f:%l,%-G%*[ ]from %f:%l:%c,%-G%*[ ]from %f:%l:,%-G%*[ ]from %f:%l\\,,%-G%*[ ]from %f:%l,%f:%l:%c:%m,%f(%l):%m,%f:%l:%m,\u0026#34;%f\u0026#34;\\, line %l%*\\D%c%*[^ ]%m,%D%*\\a[%*\\d]: Entering directory%*[`\u0026#39;]%f\u0026#39;,%X%*\\a[%*\\d]: Leaving directory %*[`\u0026#39;]%f\u0026#39;,%D%*\\a: Entering directory%*[`\u0026#39;]%f\u0026#39;,%X%*\\a: Leaving directory %*[`\u0026#39;]%f\u0026#39;,%DMaking %*\\a in %f,%f|%l| %m\u0026#39; The first little vim trick is that the option errorformat (and all other options, actually), can be set as variables by prepending \u0026amp;. The following two lines do the same thing:\nfiletype=markdown let \u0026amp;filetype=markdown Why would we choose the second syntax? I learned from the vim help for efm-entries that there are multiple ways to change the value of long strings like errorformat. We can build them up from smaller strings by appending, by using .= instead of =. According to efm-entries, += and -= can be used to add and remove entries in errorformat.\nSetting errorformat # The format of errors is tricky here because the arduino toolchain actually pre-processes the arduino sketch files into a standard C++ file before building, so the resulting error messages do not always contain a filename. Sample output:\nPicked up JAVA_TOOL_OPTIONS: Loading configuration... Initializing packages... Preparing boards... Verifying... sketch_file:32: error: \u0026#39;asdf\u0026#39; does not name a type asdf ^ /path/to/sketch_file.ino: In function \u0026#39;void setupCandleService()\u0026#39;: sketch_file:59: error: \u0026#39;candleService\u0026#39; was not declared in this scope candleService.begin(); ^ exit status 1 Ignoring the \u0026ldquo;standard\u0026rdquo; output lines is easy enough with the errorformat specifier %-G, which ignores the lines that match it. (e.g. %-GInitializing packages...).\nThe next tricky bit was that the two errors above only show one full file path, and it is for the second error. Here I considered fetching the existing quickfix list in a QuickfixCmdPost, and \u0026ldquo;fixing\u0026rdquo; the files to be opened, but after spending some time on this solution, it occurred to me that the most likely outcome was that the error was in the current buffer.\nPutting it all together # To get the right compiler options set, I created my own vim compiler plugin, for arduino. Since the existing plugin from stevearc/vim-arduino sets up make, I used that. The only real settings here are for the errorformat parsing (the top bit is all boilerplate for vim compiler plugins, and copied from the help docs). Here is the custom arduino compiler plugin, and to make it work automatically, I set the compiler in the filetype plugin for arduino.\nNow when I open an arduino sketch file in vim, I can verify the build, and get decent quickfix integration as well!\n","date":"16 January 2018","externalUrl":null,"permalink":"/archive/vim-arduino/","section":"Archives","summary":"\u003cblockquote\u003e\n\u003cp\u003eTL;DR - I made a vim \u003ca\n  href=\"https://github.com/muncus/dotfiles/blob/master/.vim/after/compiler/arduino.vim\"\n    target=\"_blank\"\n  \u003ecompiler plugin for arduino\u003c/a\u003e\nthat parses errors into the quickfix list, and learned a bunch about the\nquickfix list and compiler plugins.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eI\u0026rsquo;ve been working on several small arduino projects lately (mostly the series of\nbluetooth projects posted earlier). While I find the arduino toolchain easy to\nuse, I\u0026rsquo;ve never enjoyed using the editor, or having to use the mouse to click\nthe \u003ccode\u003eVerify\u003c/code\u003e and \u003ccode\u003eUpload\u003c/code\u003e buttons.\u003c/p\u003e","title":"Vim and Arduino","type":"archive"},{"content":"I\u0026rsquo;ve recently become interested in bluetooth, specifically web-bluetooth, which is newly enabled in most chrome/firefox/opera browsers. It allows a web app (javascript) to interact with bluetooth devices near the web browser. Combine this with eddystone beacons and the bluetooth device is advertising a URL that links a user directly to an app to interact with the device.\nHardware and software # To play with bluetooth, the first thing I needed was a bluetooth radio. Some simple beacon stuff can be done with the beacon toy app, I wanted to use a microcontroller, suitable for embedding into projects, as this was my eventual goal. There are plenty of good options on this front:\nmicro:bit Adafruit nRF51 Adafruit nRF52 Sparkfun ESP32 Thing many more. I went for the Adafruit nRF52, only because that was the first one that caught my eye. the micro:bit has several other onboard sensors, and is cheaper, so I may switch to that platform eventually.\nTo program the nRF52, i\u0026rsquo;m using the arduino toolchain, but since i\u0026rsquo;m a vim user, i\u0026rsquo;m editing primarily with vim, and only using the arduino IDE parts for the compilation. (I later switched to using the vim-arduino plugin, but i\u0026rsquo;ll talk about that setup separately).\nThere are also some mobile apps that are helpful here, notably nRF Connect, which is produced by Nordic Semiconductor, a major manufacturer of bluetooth chips. It has a variety of modes, and companion apps that i found indispensible for debugging and testing these examples.\nFirst steps: Eddystone Beacon # My first goal was to just get an Eddystone beacon broadcasting, to direct an interested user to a web page. The code is nearly verbatim from the adafruit nRF52 example. it is important to note that the Eddystone protocol only allocates 17 bytes for the encoded URL. No error message is emitted when using a url that is too long. The best practice for avoiding problems here is to use a url shortener like goo.gl. This has the added benefit of letting you change the beacon\u0026rsquo;s destination without having to update the beacon device.\nNext: Playbulb candle emulation # At this point, i\u0026rsquo;m more familiar with arduino/C++ than with javascript, so I opted to create a device that an existing webapp could interact with. I stumbled on the playbulb candle codelab, which interacts with a fairly simple custom bluetooth device.\nBased on the javascript code, I discovered the protocol used to set the name, color and light effect on the playbulb candle devices. These are handled by individual Characteristics inside of a Bluetooth GATT service. Armed with this knowledge, i created a sketch that implemented enough of the service to interact with the app.\nThe webapp has some rough edges, notably that if web-bluetooth is not enabled, there is no error message displayed (though there is one printed to the javascript console if you open chrome\u0026rsquo;s developer tools). The same is true if you access the page over http, rather than https. (https is required for all web-bluetooth functionality, in accordance with the spec).\nThe big takeaway from this example is that i\u0026rsquo;ve now implemented entirely custom BLE services on the device, with read-only and read-write characteristics. These can serve as the basis for any custom services I build later.\nNordic Uart Service: not-exactly-standard, but close enough. # Chronologically, this project was the second one i built, not the third. but logically it makes more sense here.\nIn my research about bluetooth LE services, i kept seeing references to the Nordic Uart Service (NUS). This is a service common to many of the chips from Nordic Semiconductor, that emulates a standard bluetooth UART connection over BLE.\nWhile the nRF52 i\u0026rsquo;m using is made by Nordic, it does not have built-in support for this service, so I decided to build a simple Echo service on the NUS protocol. A later addition can interpret commands delivered over this link, to perform actions. See the terminal echo sketch for the details here. I was now able to use nRF Connect mobile app to connect to the device over NUS, and send/receive text.\nWondering how to apply this to a web-bluetooth app, i came across a web bluetooth terminal app that had a similar behavior, but was built using only a single service and characteristic, where NUS uses two characteristics in the same service (one for Transmit, and one for Receive). With some refactoring of the main javascript file, I was eventually able to produce a terminal app that\u0026rsquo;s compatible with NUS, and could use it to connect and interact with my bluetooth echo device, which would dutifully reply with whatever it was sent. (as long as it was in chunks smaller than 20 bytes, which is the max write size for a ble characteristic). The original terminal had some buffering to work around this, but with my limited javascript experience, i decided to remove it, as it complicated the parsing of ble packets.\nWhat\u0026rsquo;s next? # Well, now that i\u0026rsquo;ve mastered beacons, created custom services and characteristics, and exchanged simple text commands over BLE, its time to build something bigger! Maybe Zork over ble, or multi-player bluetooth hungry-hungry-hippos. I\u0026rsquo;m not sure exactly what\u0026rsquo;s next, but stay tuned to find out!\n","date":"25 November 2017","externalUrl":null,"permalink":"/archive/bluetooth-pt1/","section":"Archives","summary":"\u003cp\u003eI\u0026rsquo;ve recently become interested in bluetooth, specifically web-bluetooth, which\nis newly enabled in most chrome/firefox/opera browsers. It allows a web app\n(javascript) to interact with bluetooth devices near the web browser. Combine\nthis with \u003ca\n  href=\"\"\u003eeddystone beacons\u003c/a\u003e and the bluetooth device is advertising a URL\nthat links a user directly to an app to interact with the device.\u003c/p\u003e\n\n\u003ch2 class=\"relative group\"\u003eHardware and software\n    \u003cdiv id=\"hardware-and-software\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#hardware-and-software\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eTo play with bluetooth, the first thing I needed was a bluetooth radio. Some\nsimple beacon stuff can be done with the \u003ca\n  href=\"https://play.google.com/store/apps/details?id=com.uriio\"\n    target=\"_blank\"\n  \u003ebeacon toy\napp\u003c/a\u003e, I wanted to use a\nmicrocontroller, suitable for embedding into projects, as this was my eventual\ngoal. There are plenty of good options on this front:\u003c/p\u003e","title":"Bluetooth Part 1","type":"archive"},{"content":"I did not really need a thermal receipt printer, but i bought one anyway. Then i tried to make it useful.\nCode on github\nThe Basics # First, i followed the excellent tutorial from Adafruit(where i bought the printer). It covers all the initial setup stuff very well, and got me to a working printer that outputs simple text, with some basic text styling features (and other stuff like barcodes i did not expect to use).\nPrinter Drivers # A printer is fun, but I don\u0026rsquo;t typically have a monitor connected to my raspberry pi, so printing things was infrequent, as I had to ssh into the pi, and run a script.\nI found another excellent adafruit tutorial that described the setup process for a network-connected thermal printer.\nThis let me sent more complex content to the printer, and print from other computers. This setup is great for printing out a grocery list before heading off to the grocery store. It uses the thermal printer just like any other printer.\nI ran into some interesting paper sizing troubles here, where pages were printed in the aspect ratio of a 8.5x11in page, but only 2in wide.\nPrint the list, on demand. # Being able to print the grocery list, or my todo list, is good, but still involves using a regular computer to print out the list. The next idea was to set up physical buttons that would cause the pi itself to fetch the list, and print it.\nI keep a few different lists on todoist, which has a decent api, and a python client library. Python is perfect, since the Pi also has a python-based library for reading the GPIO pins (where we can attach the buttons). With some small refactoring of the gpio_listener.py script from the above example, it will listen for multiple buttons, and launch the todoist script to print any number of pre-configured lists.\nBe aware that the search strings \u0026ldquo;supported\u0026rdquo; by the todoist api are not the same as the searches done through the search box. Many search features are for paying customers only. There is a todoist help article on Filters that explains available filters, but only some of them appear to work correctly for me (not a paying customer). Notably, the no date, today, and overdue filters dont appear to work. \u0026#x1f626;\nI was still able to build a reasonable set priority searches that work well enough for me.\nAside: packaging # My process so far has been to develop on my laptop, and copy files over to the Pi for testing. This workflow gets a little tedious, and means i\u0026rsquo;m constantly overwriting files with new copies, from a machine which cannot run most of this code (the pi\u0026rsquo;s GPIO libraries are not available for my laptop).\nTo ease this tedium, and allow for easier rollbacks, I decided to put the todo-printer files into a quick-and-dirty debian package, so i\u0026rsquo;d only need to copy a single file over to the pi, and could install/uninstall as needed. My process for doing that will be covered in a separate post about quick-and-dirty debian packaging.\nEnclosure # The last step of the project is to build a suitably attractive case for the pi and printer. I settled on a cube ~4.5\u0026quot; in all dimensions. This size allows plenty of room for the printer, and a set of 4 small pushbuttons on the top surface.\n","date":"30 June 2017","externalUrl":null,"permalink":"/archive/thermal-printer/","section":"Archives","summary":"\u003cp\u003eI did not really \u003cem\u003eneed\u003c/em\u003e a thermal receipt printer, but i bought one anyway.\nThen i tried to make it useful.\u003c/p\u003e\n\u003cp\u003e\u003ca\n  href=\"https://github.com/muncus/todo-printer\"\n    target=\"_blank\"\n  \u003eCode on github\u003c/a\u003e\u003c/p\u003e\n\n\u003ch3 class=\"relative group\"\u003eThe Basics\n    \u003cdiv id=\"the-basics\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#the-basics\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003eFirst, i followed the excellent \u003ca\n  href=\"https://learn.adafruit.com/mini-thermal-receipt-printer/overview\"\n    target=\"_blank\"\n  \u003etutorial from\nAdafruit\u003c/a\u003e(where\ni bought the printer). It covers all the initial setup stuff very well, and got\nme to a working printer that outputs simple text, with some basic text styling\nfeatures (and other stuff like barcodes i did not expect to use).\u003c/p\u003e","title":"Thermal Receipt printer experiments","type":"archive"},{"content":"The Internet can be a very distracting place sometimes. There are so many sites (particularly social media) that demand our attention, and let us mindlessly scroll through photos and status updates, when we really want to be doing something else.\nIn an effort to aid my own concentration, I build a proof-of-concept Internet Volume Control that helps filter out some of the distraction.\nAll my work is up on github.com/muncus/internet-volume and the background / origin story is below.\nInspiration and Design # While i was thinking about this, i considered several possible approaches to filtering out \u0026ldquo;noisy\u0026rdquo; internet content. A \u0026ldquo;captive portal\u0026rdquo; like public wifi, that would have specific rules. A \u0026ldquo;transparent proxy\u0026rdquo; that would rewrite content (sort of like an ad-blocker). These all had different drawbacks, particularly when it comes to HTTPS content.\nThen I realized that there\u0026rsquo;s a service under the rest of the internet\u0026rsquo;s noisy content: DNS! Before an app or a web browser can do anything, it needs to talk to DNS to find the \u0026ldquo;address\u0026rdquo; for the noisy service (e.g. facebook.com). By intercepting these calls, it is relatively easy to prevent users from accessing these sites. It is not foolproof (anyone willing to edit their DHCP-received nameservers can get around it). But it seemed good enough for my purposes.\nSo, using some handy Ruby libraries, I put together a simple DNS server that checks the \u0026ldquo;internet volume\u0026rdquo; (more on that later) and either returns the upstream response (from Google\u0026rsquo;s public DNS server), or returns an error of NXDOMAIN, which indicates the name cannot be resolved.\nInternet Volume # Now that i\u0026rsquo;d built the guts of the volume control, I needed a way to set the volume. Obviously, this was going to be a big dial of some kind, but how would it connect to the rest of the system?\nI\u0026rsquo;d built a few standalone internet-connected things before, and this is just one more. The wifi-connected dial (well, potentiometer plus wifi-connected microcontroller) reports the current \u0026ldquo;position\u0026rdquo; as a number from 0 - 10. This number is sent to adafruit.io, but could use any other IoT pubsub service.\nSome updates to the DNS server to read (and cache!) this value, and i\u0026rsquo;m ready for distraction-free wifi browsing!\nFuture work # I\u0026rsquo;d like to add a proper captive portal \u0026ldquo;login\u0026rdquo; page which describes the project, so users are not caught by surprise when i dial down the volume.\n","date":"22 June 2017","externalUrl":null,"permalink":"/archive/internet-volume/","section":"Archives","summary":"\u003cp\u003eThe Internet can be a very distracting place sometimes. There are so many sites\n(particularly social media) that demand our attention, and let us mindlessly\nscroll through photos and status updates, when we really want to be doing\nsomething else.\u003c/p\u003e\n\u003cp\u003eIn an effort to aid my own concentration, I build a proof-of-concept Internet\nVolume Control that helps filter out some of the distraction.\u003c/p\u003e\n\u003cp\u003eAll my work is up on\n\u003ca\n  href=\"http://github.com/muncus/internet-volume\"\n    target=\"_blank\"\n  \u003egithub.com/muncus/internet-volume\u003c/a\u003e\nand the background / origin story is below.\u003c/p\u003e","title":"Internet Volume Control","type":"archive"},{"content":"Inspired by the Amazon Dash-style esp8266 IoT button, I built a button my distant family could use to request a refill of my home-made jam (though it could be used for anything). To keep with the theme, i built the whole thing to fit in a standard 8oz jam jar.\nUnlike the inspiration, this button should be able to live for months in the pantry without power, so it needed a power switch. After designing a P-channel mosfet circuit to handle this, i came across the adafruit powerswitch which is pretty much exactly what i needed. Throw a micro-lipo battery charge circuit in there and BOOM - button that turns on the ESP chip, and allow it to turn itself off.\nThe powerswitch means that there is no longer a \u0026ldquo;button press\u0026rdquo; from the ESP perspective, but the IFTTT event should fire as soon as it gets connected to the wifi network. Minor code changes were needed to make that happen, and then assert the pin connected to the powerswitch\u0026rsquo;s p-fet gate (to power off the device). All code is available on github/muncus/jambutton.\nAfter configuring a couple of IFTTT recipes, a press of the button results in ~20 seconds of powered-on time for the device, and a pushbullet notification being sent to my phone.\nI\u0026rsquo;m currently playing around with a small thermal printer to print out a physical receipt for the jam order, so they are harder to ignore. :)\n","date":"26 December 2015","externalUrl":null,"permalink":"/archive/jamomatic/","section":"Archives","summary":"\u003cp\u003eInspired by the Amazon Dash-style \u003ca\n  href=\"http://github.com/garthvh/esp8266button\"\n    target=\"_blank\"\n  \u003eesp8266 IoT\nbutton\u003c/a\u003e, I built a button my distant\nfamily could use to request a refill of my home-made jam (though it could be\nused for anything). To keep with the theme, i built the whole thing to fit in a\nstandard 8oz jam jar.\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"photo of finished button\"\n    src=\"/images/jamomatic.jpg\"\n    \u003e\u003c/figure\u003e\n\u003c/p\u003e\n\u003cp\u003eUnlike the inspiration, this button should be able to live for months in the\npantry without power, so it needed a power switch. After designing a P-channel\nmosfet circuit to handle this, i came across the \u003ca\n  href=\"https://www.adafruit.com/products/1400\"\n    target=\"_blank\"\n  \u003eadafruit\npowerswitch\u003c/a\u003e which is pretty much\nexactly what i needed. Throw a micro-lipo battery charge circuit in there and\n\u003cem\u003eBOOM\u003c/em\u003e - button that turns on the ESP chip, and allow it to turn itself off.\u003c/p\u003e","title":"Jam-O-Matic","type":"archive"},{"content":"San Francisco\u0026rsquo;s Muni trains are not well known for their ability to be on time. Recent sfmta data shows a ~60% on-time rate (where \u0026ldquo;on-time\u0026rdquo; includes arriving anywhere between 1 minute before to 4 minutes after the intended time). Fortunately, they provide real-time arrival estimates with the Next Bus service. While the QuickMuni app does a great job of displaying this on my phone, I wanted to build a more \u0026ldquo;ambient\u0026rdquo; display, to help me decide when to leave in the morning.\nRevision 1 # My first attempt at a muni display was an Arduino with 3 big LEDs, laid out as a stoplight. A python script parsed train arrival times, and sent them to the arduino, which then lit the corresponding lights: Green for \u0026gt;10 minutes, Yellow for 7-10, Red for 6-7. (These times were chosen based on how long it took for me to walk to the train).\nThe most obvious drawback here was the need for a computer to run the python script. I was eventually able to get the script running on a wireless access point with DD-WRT, but the package was still rather awkward.\nThe first working prototype Around this time, Quick Muni came out, so I had a quick phone-based way to check trains, and I lost interest for a while.\nRevision 2 # Some time later, I bought a Particle Photon (formerly known as the Spark Core). Small, and wifi-enabled, it was perfect for a tiny train display.\nInstead of a stoplight, I found a small servo, and got the Photon using it very quickly thanks to their builtin Servo library. Original Proof-of-concept pushed the train times to the device through the Particle Cloud api, but I wanted the device to be a bit more self-sufficient.\nWiring photo of muni 2.0 To that end, I put together a web server with some configuration that maps the Particle device name (which the device can fetch from the Particle cloud api) to a set of nextbus query parameters. The device requests /times/scrapple_ferret (for instance), and the server returns the number of minutes until the next estimated arrival at the stop at which scrapple_ferret is configured.\nThe servo, however, makes a slight noise when it moves, and I found that having it run all night was not ideal. I added a button to activate the device for ~30m, and a pair of LED headlights that turn on when the device activates, and gradually dim as the device turns off.\nAll that remained was to paint up a display face, vaguely resembling the from of an SF Muni train, and hang it on the wall!\nFinished enclosure, on wall The code for this project is available from github.com/muncus/muni-display.\n","date":"24 August 2015","externalUrl":null,"permalink":"/archive/muni-displays/","section":"Archives","summary":"\u003cp\u003eSan Francisco\u0026rsquo;s \u003ca\n  href=\"http://sfmta.com\"\n    target=\"_blank\"\n  \u003eMuni\u003c/a\u003e trains are not well known for their ability to be on time. Recent \u003ca\n  href=\"https://www.sfmta.com/about-sfmta/reports/performance-metrics/percentage-time-performance\"\n    target=\"_blank\"\n  \u003esfmta data\u003c/a\u003e shows a ~60% on-time rate (where \u0026ldquo;on-time\u0026rdquo; includes arriving anywhere between 1 minute before to 4 minutes after the intended time). Fortunately, they provide real-time arrival estimates with the \u003ca\n  href=\"http://nextbus.com\"\n    target=\"_blank\"\n  \u003eNext Bus\u003c/a\u003e service. While the \u003ca\n  href=\"https://play.google.com/store/apps/details?id=com.worldofbilly.quickmuni\"\n    target=\"_blank\"\n  \u003eQuickMuni\u003c/a\u003e app does a great job of displaying this on my phone, I wanted to build a more \u0026ldquo;ambient\u0026rdquo; display, to help me decide when to leave in the morning.\u003c/p\u003e","title":"Muni Displays","type":"archive"},{"content":"Emoji keyboards on mobile devices have been in widespread use for a while, but I sometimes find myself using a computer, and have a hard time remembering the unicode identifiers for a nice cup of tea (1f375 - 🍵 ) or a suitable warning character (2620 - ☠ ). So I decided to make a supplemental Unicode keyboard.\nCode # The code can be found on Github: http://github.com/muncus/unicode-keyboard. User-servicable parts are in config.h.\nSince the key sequence for entering a unicode character varies by operating system, the current implementation only supports Linux, though there are notes in the code for how support for other OSes could be implemented.\nBuild # I had a Teensy 2.0 handy, for which there are several good examples of emulating a USB keyboard. Any old button will do, but I wanted that good old keyboard feel, so i bought a Cherry MX switch sampler from WASD keyboards and some spare switches, just in case.\nWired one side of the switches all to ground, the other side to the first 6 data pins on the Teensy.\nOnce that was done, all that was left was to add the symbols to the keys. Knowing i\u0026rsquo;d probably want to change these frequently, i chose to print out the symbols, and just tape them to the keycaps with clear scotch tape. Done!\n","date":"21 April 2015","externalUrl":null,"permalink":"/archive/unicode-keyboard/","section":"Archives","summary":"\u003cp\u003eEmoji keyboards on mobile devices have been in widespread use for a while, but\nI sometimes find myself using a computer, and have a hard time remembering the\nunicode identifiers for a nice cup of tea (1f375 - 🍵 ) or a suitable warning\ncharacter (2620 - ☠ ). So I decided to make a supplemental Unicode keyboard.\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\u003cimg\n    class=\"my-0 rounded-md\"\n    loading=\"lazy\"\n    decoding=\"async\"\n    fetchpriority=\"low\"\n    alt=\"unicode keyboard\"\n    src=\"/images/unicodekeyboard.jpg\"\n    \u003e\u003c/figure\u003e\n\u003c/p\u003e\n\n\u003ch4 class=\"relative group\"\u003eCode\n    \u003cdiv id=\"code\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#code\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h4\u003e\n\u003cp\u003eThe code can be found on Github: \u003ca\n  href=\"http://github.com/muncus/unicode-keyboard\"\n    target=\"_blank\"\n  \u003ehttp://github.com/muncus/unicode-keyboard\u003c/a\u003e. User-servicable parts are in\n\u003ccode\u003econfig.h\u003c/code\u003e.\u003c/p\u003e","title":"Unicode (emoji) keyboard","type":"archive"},{"content":"I was working on a particularly challenging service turndown, which involved handholding some user migrations. During the waiting periods, it occurred to me that turning off the service by simply typing the right command lacked a certain gravity.\nSo I grabbed a Bluefruit EZ-key (12-key programmable bluetooth keyboard), a big red arcade button, and a 9-volt battery. Using the adafruit intro docs, it was simple to get the button working. But it still wasnt quite right.\nIt wasn\u0026rsquo;t until i found a fancy cardboard box at the local variety store that things really came together. A few tasteful stamps invited the pushing of the big red button.\nI also wrote a quick script for pairing, since the pairing instructions for linux were a little cumbersome. (unfortunately, there\u0026rsquo;s a lot of sudo in there\u0026hellip;).\nNow i\u0026rsquo;m fully prepared for the next dramatic launch (or turndown)!\n","date":"1 January 2015","externalUrl":null,"permalink":"/archive/big-red-button/","section":"Archives","summary":"\u003cp\u003eI was working on a particularly challenging service turndown, which\ninvolved handholding some user migrations. During the waiting periods, it\noccurred to me that turning off the service by simply typing the right command\nlacked a certain \u003cem\u003egravity\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eSo I grabbed a \u003ca\n  href=\"http://www.adafruit.com/products/1535\"\n    target=\"_blank\"\n  \u003eBluefruit EZ-key\u003c/a\u003e\n(12-key programmable bluetooth keyboard), a big red arcade button, and a 9-volt\nbattery. Using the \u003ca\n  href=\"https://learn.adafruit.com/introducing-bluefruit-ez-key-diy-bluetooth-hid-keyboard\"\n    target=\"_blank\"\n  \u003eadafruit intro\ndocs\u003c/a\u003e,\nit was simple to get the button working. But it still wasnt \u003cem\u003equite\u003c/em\u003e right.\u003c/p\u003e","title":"Big Red Button","type":"archive"},{"content":"First Post! Just playing around with jekyll and github pages.\n","date":"11 December 2014","externalUrl":null,"permalink":"/archive/first-post/","section":"Archives","summary":"\u003cp\u003eFirst Post! Just playing around with jekyll and github pages.\u003c/p\u003e","title":"First Post!","type":"archive"},{"content":"","date":"11 December 2014","externalUrl":null,"permalink":"/tags/jekyll/","section":"Tags","summary":"","title":"Jekyll","type":"tags"},{"content":"","date":"11 December 2014","externalUrl":null,"permalink":"/tags/update/","section":"Tags","summary":"","title":"Update","type":"tags"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"}]